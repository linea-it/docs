{"config":{"indexing":"full","lang":["en","pt","es"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Bem-vindo(a) \u00e0 p\u00e1gina de Documenta\u00e7\u00e3o para Usu\u00e1rios do LIneA. Este \u00e9 o local central para encontrar guias de usu\u00e1rio e documenta\u00e7\u00e3o sobre todos os servi\u00e7os e ferramentas fornecidos pelo LIneA para a comunidade astron\u00f4mica e p\u00fablico em geral. Aqui voc\u00ea tamb\u00e9m encontrar\u00e1 links para documenta\u00e7\u00e3o externa relevante. O LIneA - Laborat\u00f3rio Interinstitucional de e-Astronomia - \u00e9 um laborat\u00f3rio multiusu\u00e1rio, operado por uma organiza\u00e7\u00e3o sem fins lucrativos (Associa\u00e7\u00e3o LIneA) com apoio financeiro predominantemente proveniente do Minist\u00e9rio da Ci\u00eancia, Tecnologia e Inova\u00e7\u00e3o do Brasil. Nossa miss\u00e3o \u00e9 trabalhar para apoiar a comunidade astron\u00f4mica brasileira com infraestrutura computacional e expertise em an\u00e1lise de big data para fornecer condi\u00e7\u00f5es t\u00e9cnicas para participa\u00e7\u00e3o em grandes levantamentos astron\u00f4micos, como Sloan Digital Sky Survey (SDSS) , Dark Energy Survey (DES) e Legacy Survey of Space and Time (LSST) . Para saber mais, confira nossos v\u00eddeos institucionais abaixo, nosso canal no YouTube ou navegue no nosso Website institucional . Coment\u00e1rios, d\u00favidas, sugest\u00f5es? Se voc\u00ea encontrar algo faltando nesta documenta\u00e7\u00e3o, fique \u00e0 vontade para abrir um issue no reposit\u00f3rio de documenta\u00e7\u00e3o do LIneA no GitHub .","title":"In\u00edcio"},{"location":"cursos.html","text":"O LIneA oferecemos cursos de treinamento nas principais ferramentas utilizadas pelos projetos. As grava\u00e7\u00f5es e o material did\u00e1tico de todos os cursos j\u00e1 oferecidos se encontram dispon\u00edveis no Google Classroom. Acesse a p\u00e1gina de cursos no site do LIneA para mais detalhes.","title":"Cursos e Tutoriais"},{"location":"faq.html","text":"Como criar par de chaves SSH \u00b6 Para acessar nosso ambiente via ssh \u00e9 preciso gerar um par de chaves seguindo os passos abaixos: Linux \u00b6 a) Para gerar o par de chaves utilize o comando abaixo em seu terminal. ssh-keygen -t rsa -b 4096 Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): [pressione ENTER] Created directory '/root/.ssh'. Enter passphrase (empty for no passphrase): [digite uma senha e para confirmar pressione ENTER] Enter same passphrase again: [repita a senha e pressione ENTER] b) Ap\u00f3s voc\u00ea receber a mensagem que a chave foi gerada. Voc\u00ea pode ver os dois arquivos criados listando o conte\u00fado do diret\u00f3rio ls $HOME/.ssh id_rsa id_rsa.pub c) Ap\u00f3s as chaves geradas enviar a chave .pub para a equipe de TI via email helpdesk@linea.org.br . A equipe de TI do LIneA ir\u00e1 configurar a chave no servidor e retornar com instru\u00e7\u00f5es para login no cluster Apollo. Aguarde o retorno de ok . Windows \u00b6 Para gerar par de chaves no sistema operacional Windows a) Baixar o aplicativo Putty e instalar. b) Acessar a pasta onde o programa foi instalado (essa instala\u00e7\u00e3o foi no Windows 10) C:\\Program File\\PuTTY (caminho pode ser diferente devido ao sistema operacional), Abra o programa Puttygen. c) Clicar em Generate (o tipo de chave mant\u00e9m como RSA ). OBS : Movimentar o ponteiro do mouse ajuda a gerar a chave mais rapidamente, pois gera bits aleat\u00f3rios . d) Par de chaves geradas com sucesso. Copiar a chave publicar para ser salva no servidor (detalhe na imagem em amarelo); Colocar uma senha na chave p\u00fablica (detalhe na imagem em azul). Ap\u00f3s copiar salve as chaves public e private no computador (detalhe na imagem em vermelho) envie a chave .pub para a equipe de TI via email helpdesk@linea.org.br . A equipe de TI do LIneA ir\u00e1 configurar a chave no servidor. Aguarde o retorno de ok . e) Ap\u00f3s o receber o email de confirma\u00e7\u00e3o que a chave .pub foi cadastrada no servidor de acesso, fazer as configura\u00e7\u00f5es no programa Putty . Crie um atalho na \u00e1rea de trabalho, abra o PuTTY ; Coloque o Hostname login.linea.org.br. f) Ao lado esquerdo ir na op\u00e7\u00e3o SSH > Auth (detalhe em azul) > aperte em Browse (detalhe em amarelo) e escolha a chave a ser usada com exten\u00e7\u00e3o .ppk . h) Caso precise utilizar algum t\u00fanel fa\u00e7a a seguinte configura\u00e7\u00e3o. OBS: os tunnels s\u00e3o configurado conforme o que o usu\u00e1rio vai acessar Ir na op\u00e7\u00e3o Tunnels (lado esquerdo); Em Source port coloque a porta; Destination > coloque o endere\u00e7o de destino > Add. Volte ao lado esquerdo e v\u00e1 na primeira op\u00e7\u00e3o menu Session (em vermelho) coloque o nome da sessions (em amarelo) e aperte em Save (em azul) , para acessar aperte em Open .","title":"Faq"},{"location":"faq.html#como-criar-par-de-chaves-ssh","text":"Para acessar nosso ambiente via ssh \u00e9 preciso gerar um par de chaves seguindo os passos abaixos:","title":"Como criar par de chaves SSH"},{"location":"faq.html#linux","text":"a) Para gerar o par de chaves utilize o comando abaixo em seu terminal. ssh-keygen -t rsa -b 4096 Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): [pressione ENTER] Created directory '/root/.ssh'. Enter passphrase (empty for no passphrase): [digite uma senha e para confirmar pressione ENTER] Enter same passphrase again: [repita a senha e pressione ENTER] b) Ap\u00f3s voc\u00ea receber a mensagem que a chave foi gerada. Voc\u00ea pode ver os dois arquivos criados listando o conte\u00fado do diret\u00f3rio ls $HOME/.ssh id_rsa id_rsa.pub c) Ap\u00f3s as chaves geradas enviar a chave .pub para a equipe de TI via email helpdesk@linea.org.br . A equipe de TI do LIneA ir\u00e1 configurar a chave no servidor e retornar com instru\u00e7\u00f5es para login no cluster Apollo. Aguarde o retorno de ok .","title":"Linux"},{"location":"faq.html#windows","text":"Para gerar par de chaves no sistema operacional Windows a) Baixar o aplicativo Putty e instalar. b) Acessar a pasta onde o programa foi instalado (essa instala\u00e7\u00e3o foi no Windows 10) C:\\Program File\\PuTTY (caminho pode ser diferente devido ao sistema operacional), Abra o programa Puttygen. c) Clicar em Generate (o tipo de chave mant\u00e9m como RSA ). OBS : Movimentar o ponteiro do mouse ajuda a gerar a chave mais rapidamente, pois gera bits aleat\u00f3rios . d) Par de chaves geradas com sucesso. Copiar a chave publicar para ser salva no servidor (detalhe na imagem em amarelo); Colocar uma senha na chave p\u00fablica (detalhe na imagem em azul). Ap\u00f3s copiar salve as chaves public e private no computador (detalhe na imagem em vermelho) envie a chave .pub para a equipe de TI via email helpdesk@linea.org.br . A equipe de TI do LIneA ir\u00e1 configurar a chave no servidor. Aguarde o retorno de ok . e) Ap\u00f3s o receber o email de confirma\u00e7\u00e3o que a chave .pub foi cadastrada no servidor de acesso, fazer as configura\u00e7\u00f5es no programa Putty . Crie um atalho na \u00e1rea de trabalho, abra o PuTTY ; Coloque o Hostname login.linea.org.br. f) Ao lado esquerdo ir na op\u00e7\u00e3o SSH > Auth (detalhe em azul) > aperte em Browse (detalhe em amarelo) e escolha a chave a ser usada com exten\u00e7\u00e3o .ppk . h) Caso precise utilizar algum t\u00fanel fa\u00e7a a seguinte configura\u00e7\u00e3o. OBS: os tunnels s\u00e3o configurado conforme o que o usu\u00e1rio vai acessar Ir na op\u00e7\u00e3o Tunnels (lado esquerdo); Em Source port coloque a porta; Destination > coloque o endere\u00e7o de destino > Add. Volte ao lado esquerdo e v\u00e1 na primeira op\u00e7\u00e3o menu Session (em vermelho) coloque o nome da sessions (em amarelo) e aperte em Save (em azul) , para acessar aperte em Open .","title":"Windows"},{"location":"glossario.html","text":"Acesse aqui a lista de termos t\u00e9cnicos e acr\u00f4nimos dispon\u00edvel no nosso site.","title":"Gloss\u00e1rio"},{"location":"monitoramento.html","text":"TODO","title":"Monitoramento"},{"location":"politicas.html","text":"Pol\u00edtica de Seguran\u00e7a da Informa\u00e7\u00e3o (PSI) \u00b6 A Pol\u00edtica de Seguran\u00e7a da Informa\u00e7\u00e3o (PSI) do LIneA define diretrizes estrat\u00e9gicas sobre como a Seguran\u00e7a da Informa\u00e7\u00e3o ser\u00e1 encarada no ambiente da associa\u00e7\u00e3o. Ela foi elaborada pelo Comit\u00ea Gestor de Seguran\u00e7a da Informa\u00e7\u00e3o (CGSI/LIneA) com o apoio do Centro de Atendimento a Incidentes de Seguran\u00e7a da RNP (CAIS/RNP), e cobre diversos aspectos da seguran\u00e7a da informa\u00e7\u00e3o, definindo papeis e responsabilidades tanto para a Associa\u00e7\u00e3o LIneA, quanto para os seus colaboradores e usu\u00e1rios. Para acessar a PSI, clique aqui Pol\u00edtica de Uso do Ambiente de HPC \u00b6 Em conformidade com a Pol\u00edtica de Seguran\u00e7a da Informa\u00e7\u00e3o (PSI), o LIneA tamb\u00e9m estabelece diretrizes para o uso do ambiente de computa\u00e7\u00e3o de alto desempenho. A Pol\u00edtica de Uso do Ambiente de HPC visa informar aos colaboradores e usu\u00e1rios acerca da utiliza\u00e7\u00e3o da infraestrutura de forma respons\u00e1vel, eficiente e alinhada \u00e0 prop\u00f3sitos cient\u00edficos. Para acessar a Pol\u00edtica de Uso do Ambiente de HPC, clique aqui Reconhecimento de uso dos recursos computacionais do LIneA \u00b6 Por favor, comprometa-se em reconhecer o LIneA em suas publica\u00e7\u00f5es, utilizando uma cita\u00e7\u00e3o como: Em ingl\u00eas: \u201cThis research was supported by resources supplied by the Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\u201d OU Em portugu\u00eas: \u201cEsta pesquisa tornou-se poss\u00edvel gra\u00e7as aos recursos computacionais disponibilizados pelo Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\u201d Incidentes de Seguran\u00e7a \u00b6 Se voc\u00ea desconfia que ocorreu algum incidente de seguran\u00e7a com sua conta, servi\u00e7o ou aplica\u00e7\u00e3o que voc\u00ea esteja utilizando voc\u00ea deve contatar o LIneA imediatamente em helpdesk@linea.org.br . Recomendamos que salve qualquer evid\u00eancia referente ao incidente (logs, mensagens, screenshots etc) e inclua quantos detalhes for poss\u00edvel no seu email para n\u00f3s. Pol\u00edtica de Privacidade \u00b6 Em breve Pol\u00edtica de Tratamento de Dados \u00b6 Em breve","title":"Pol\u00edticas"},{"location":"politicas.html#politica-de-seguranca-da-informacao-psi","text":"A Pol\u00edtica de Seguran\u00e7a da Informa\u00e7\u00e3o (PSI) do LIneA define diretrizes estrat\u00e9gicas sobre como a Seguran\u00e7a da Informa\u00e7\u00e3o ser\u00e1 encarada no ambiente da associa\u00e7\u00e3o. Ela foi elaborada pelo Comit\u00ea Gestor de Seguran\u00e7a da Informa\u00e7\u00e3o (CGSI/LIneA) com o apoio do Centro de Atendimento a Incidentes de Seguran\u00e7a da RNP (CAIS/RNP), e cobre diversos aspectos da seguran\u00e7a da informa\u00e7\u00e3o, definindo papeis e responsabilidades tanto para a Associa\u00e7\u00e3o LIneA, quanto para os seus colaboradores e usu\u00e1rios. Para acessar a PSI, clique aqui","title":"Pol\u00edtica de Seguran\u00e7a da Informa\u00e7\u00e3o (PSI)"},{"location":"politicas.html#politica-de-uso-do-ambiente-de-hpc","text":"Em conformidade com a Pol\u00edtica de Seguran\u00e7a da Informa\u00e7\u00e3o (PSI), o LIneA tamb\u00e9m estabelece diretrizes para o uso do ambiente de computa\u00e7\u00e3o de alto desempenho. A Pol\u00edtica de Uso do Ambiente de HPC visa informar aos colaboradores e usu\u00e1rios acerca da utiliza\u00e7\u00e3o da infraestrutura de forma respons\u00e1vel, eficiente e alinhada \u00e0 prop\u00f3sitos cient\u00edficos. Para acessar a Pol\u00edtica de Uso do Ambiente de HPC, clique aqui","title":"Pol\u00edtica de Uso do Ambiente de HPC"},{"location":"politicas.html#reconhecimento-de-uso-dos-recursos-computacionais-do-linea","text":"Por favor, comprometa-se em reconhecer o LIneA em suas publica\u00e7\u00f5es, utilizando uma cita\u00e7\u00e3o como: Em ingl\u00eas: \u201cThis research was supported by resources supplied by the Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\u201d OU Em portugu\u00eas: \u201cEsta pesquisa tornou-se poss\u00edvel gra\u00e7as aos recursos computacionais disponibilizados pelo Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\u201d","title":"Reconhecimento de uso dos recursos computacionais do LIneA"},{"location":"politicas.html#incidentes-de-seguranca","text":"Se voc\u00ea desconfia que ocorreu algum incidente de seguran\u00e7a com sua conta, servi\u00e7o ou aplica\u00e7\u00e3o que voc\u00ea esteja utilizando voc\u00ea deve contatar o LIneA imediatamente em helpdesk@linea.org.br . Recomendamos que salve qualquer evid\u00eancia referente ao incidente (logs, mensagens, screenshots etc) e inclua quantos detalhes for poss\u00edvel no seu email para n\u00f3s.","title":"Incidentes de Seguran\u00e7a"},{"location":"politicas.html#politica-de-privacidade","text":"Em breve","title":"Pol\u00edtica de Privacidade"},{"location":"politicas.html#politica-de-tratamento-de-dados","text":"Em breve","title":"Pol\u00edtica de Tratamento de Dados"},{"location":"primeiros_passos.html","text":"Para ter acesso \u00e0s plataformas e servi\u00e7os disponibilizados pelo LIneA, voc\u00ea precisa ter um registro ativo em nosso ambiente e efetuar login com uma das seguintes op\u00e7\u00f5es: Login com Credenciais Institucionais - Indicado para quem tem v\u00ednculo institucional (graduandos, p\u00f3s-graduandos, pesquisadores, etc). O login \u00e9 feito via CILogon. Login via Google - Indicado para astr\u00f4nomos amadores, estudantes do Ensino M\u00e9dio, entusiastas da astronomia, etc. Acess\u00edvel a qualquer pessoa, basta ter um Gmail. Membros de Colabora\u00e7\u00f5es Cient\u00edficas O login com credencial institucional \u00e9 obrigat\u00f3rio para acessar servi\u00e7os que s\u00e3o exclusivos para membros das colabora\u00e7\u00f5es DES, DESI, LSST, SDSS e TON. N\u00e3o possui registro? \u00b6 Para efetuar o registro de usu\u00e1rio no LIneA, voc\u00ea deve ficar atento as instru\u00e7\u00f5es abaixo: Registro para P\u00fablico Geral \u00b6 P\u00fablico com v\u00ednculo institucional: graduandos , p\u00f3s-graduandos , pesquisadores , etc, vinculados a uma institui\u00e7\u00e3o registre-se aqui . P\u00fablico sem v\u00ednculo institucional: astr\u00f4nomos amadores , estudantes do Ensino M\u00e9dio , entusiastas da astronomia , etc, registre-se aqui com login via Gmail. ATEN\u00c7\u00c3O: P\u00fablico Sem V\u00ednculo Institucional \u00c9 necess\u00e1rio buscar e selecionar o Google no campo \"Select an Identity Provider\", na p\u00e1gina inicial do CILogon. Registro para Membros de Colabora\u00e7\u00e3o \u00b6 Os membros das colabora\u00e7\u00f5es DES, DESI, LSST, SDSS e TON, devem passar pelo formul\u00e1rio de triagem abaixo, preenchendo todos os campos necess\u00e1rios e enviando o formul\u00e1rio ao final. Solicita\u00e7\u00e3o de registro para membros de Colabora\u00e7\u00e3o. Ap\u00f3s a avalia\u00e7\u00e3o dos dados, entraremos em contato por email. Qualquer d\u00favida ou problema, entre em contato com o nosso Service Desk .","title":"Primeiros passos"},{"location":"primeiros_passos.html#nao-possui-registro","text":"Para efetuar o registro de usu\u00e1rio no LIneA, voc\u00ea deve ficar atento as instru\u00e7\u00f5es abaixo:","title":"N\u00e3o possui registro?"},{"location":"suporte.html","text":"Para suporte t\u00e9cnico, entre em contato com o nosso Service Desk atrav\u00e9s do e-mail helpdesk@linea.org.br . Caso seja necess\u00e1rio, nossa equipe poder\u00e1 agendar um atendimento por v\u00eddeo-chamada. Se voc\u00ea quiser fazer parte da comunidade LIneA e conversar com outros usu\u00e1rios, clique aqui para receber um convite para o Workspace LIneA Users no Slack.","title":"Suporte"},{"location":"armazenamento/index.html","text":"LustreFS (HPC) \u00b6 O ambiente do cluster Apollo conta com sistema de arquivos de alta performance Lustre com dois n\u00edveis ( tiers ) de armazenamento, um em SSD com ~70 TB ( T0 ) e outro em HDD com ~500 TB ( T1 ), ambos conectados a uma rede infiniband EDR de 100 Gb/s. Os dois n\u00edveis de armazenamento est\u00e3o dispon\u00edveis em /scratch e /data . \u00c1rea de scratch e quota \u00b6 Os usu\u00e1rios poder\u00e3o acessar seu diret\u00f3rio de scratch atrav\u00e9s da vari\u00e1vel de ambiente, ou acessando o diret\u00f3rio com o caminho completo. cd $SCRATCH Ou cd /scratch/users/<username> ATEN\u00c7\u00c3O Essa \u00e1rea N\u00c3O sofrer\u00e1 backup! Os arquivos que n\u00e3o foram modificados nos \u00faltimos 60 dias ser\u00e3o automaticamente removidos, o que torna tempor\u00e1rio o armazenamento de arquivos nessa \u00e1rea. Recomenda-se que os usu\u00e1rios realizem a transferencia dos arquivos importantes do $SCRATCH para o seu homedir . Warning O script de limpeza \u00e9 executado uma vez por semana, sempre nos fins de semana. A quota padr\u00e3o do /scratch disponibilizada para usu\u00e1rios com direito ao uso do Cluster \u00e9: area bsoft bhard isoft ihard grace period /scratch 100 GB 120 GB 100000 120000 7 days Boas pr\u00e1ticas \u00b6 Sistemas de arquivos distribu\u00eddos como o Lustre s\u00e3o ideais para ambientes HPC e HTC. Nesses ambientes, a carga de trabalho t\u00edpica consiste em arquivos grandes que precisam ser acessados \u200b\u200ba partir de muitos n\u00f3s de computa\u00e7\u00e3o com largura de banda muito alta e/ou baixa lat\u00eancia. Portanto, esses sistemas de arquivos s\u00e3o muito diferentes dos sistemas de arquivos usados \u200b\u200bem computadores desktop ou servidores isolados. Embora sejam excelentes no manuseio de arquivos grandes, eles tamb\u00e9m apresentam fortes limita\u00e7\u00f5es ao lidar com arquivos pequenos e padr\u00f5es de acesso mais comumente encontrados em ambientes corporativos e de desktop. As opera\u00e7\u00f5es que podem ser extremamente r\u00e1pidas em um disco local de esta\u00e7\u00e3o de trabalho podem ser dolorosamente lentas e caras em um sistema de arquivos Lustre, afetando tanto os usu\u00e1rios que executam essas opera\u00e7\u00f5es quanto, eventualmente, todos os outros usu\u00e1rios. Estas melhores pr\u00e1ticas e recomenda\u00e7\u00f5es t\u00eam como objetivo permitir um uso tranquilo do Lustre, minimizando ou evitando opera\u00e7\u00f5es desnecess\u00e1rias ou muito caras do sistema de arquivos. Evite acessar atributos de arquivos e diret\u00f3rios Acessar informa\u00e7\u00f5es de metadados, como atributos de arquivo (por exemplo, tipo, propriedade, prote\u00e7\u00e3o, tamanho, datas, etc.) no Lustre consome muitos recursos e pode degradar o desempenho do sistema de arquivos, especialmente quando realizado com frequ\u00eancia ou em diret\u00f3rios com grande quantidade de arquivos. Minimize o uso de chamadas de sistema que acessam ou modificam esses atributos, como stat() , statx() , open() , openat() , etc. O mesmo se aplica a comandos como ls -l (para todo o diret\u00f3rio) ou ls --color que fazem uso das chamadas mencionadas acima. Em vez disso, use um simples ls ou ls -l filename . Evite usar comandos que acessam metadados massivamente Evite usar comandos como ls -R , find , locate , du , df e similares. Esses comandos percorrem o sistema de arquivos recursivamente e/ou executam opera\u00e7\u00f5es pesadas de metadados. Eles s\u00e3o muito intensivos no acesso aos metadados do sistema de arquivos e podem degradar gravemente o desempenho geral do sistema de arquivos. Se for absolutamente necess\u00e1rio percorrer o sistema de arquivos recursivamente, use o comando fornecido com o Lustre lfs find em vez de find , por exemplo. Use o comando Lustre lfs Para minimizar o n\u00famero de chamadas Lustre RPC, sempre que poss\u00edvel use os comandos lfs em vez dos comandos fornecidos pelo sistema: lfs df => em vez de df lfs find => em vez de find Evite usar curingas Expandir os curingas exige muitos recursos. A execu\u00e7\u00e3o de comandos com curingas em um grande n\u00famero de arquivos pode levar muito tempo e afetar gravemente o desempenho do sistema de arquivos. Em vez de usar curingas, crie uma lista dos arquivos de destino e aplique o comando a cada um desses arquivos. Acesso somente leitura Sempre que poss\u00edvel, abra os arquivos como somente leitura usando O_RDONLY , al\u00e9m disso, se voc\u00ea n\u00e3o precisar atualizar o tempo de acesso ao arquivo, abra os arquivos como O_RDONLY | O_NOATIME . Se as informa\u00e7\u00f5es de tempo de acesso forem necess\u00e1rias durante a execu\u00e7\u00e3o de E/S paralela, deixe o processo pai abrir os arquivos como O_RDONLY e todas as outras classifica\u00e7\u00f5es abrirem os mesmos arquivos como O_RDONLY|O_NOATIME . Evite ter um grande n\u00famero de arquivos em um \u00fanico diret\u00f3rio Quando um arquivo \u00e9 acessado, o Lustre bloqueia o diret\u00f3rio pai. Quando muitos arquivos no mesmo diret\u00f3rio devem ser abertos, isso cria conten\u00e7\u00e3o. Gravar milhares de arquivos em um \u00fanico diret\u00f3rio produz uma carga massiva nos servidores de metadados Lustre, geralmente resultando na desativa\u00e7\u00e3o dos sistemas de arquivos. Acessar um \u00fanico diret\u00f3rio contendo milhares de arquivos pode causar grande conten\u00e7\u00e3o de recursos, degradando o desempenho do sistema de arquivos. A alternativa \u00e9 organizar os dados em v\u00e1rios subdiret\u00f3rios e dividir os arquivos entre eles. Uma abordagem comum \u00e9 usar a raiz quadrada do n\u00famero de arquivos, por exemplo, para 90.000 arquivos a raiz quadrada seria 300, portanto devem ser criados 300 diret\u00f3rios contendo 300 arquivos cada. Evite arquivos pequenos Acessar arquivos pequenos no sistema de arquivos Lustre \u00e9 muito ineficiente. O tamanho de arquivo recomendado \u00e9 superior a 1 GB. Reorganize os dados em arquivos grandes ou use formatos de arquivo como HDF5 . Alternativamente, se o tamanho total dos arquivos for pequeno, como alguns gigabytes, copie os arquivos pequenos para /tmp ou para um diret\u00f3rio tempor\u00e1rio local para cada n\u00f3 de computa\u00e7\u00e3o no in\u00edcio do trabalho (n\u00e3o se esque\u00e7a de transferir e/ou excluir os arquivos no fim). Essa abordagem pode ser combinada com o uso de ferramentas de arquivamento, como tar e armazenar pequenos arquivos em um ou mais tarballs grandes podem ser mantidos no Lustre de maneira mais eficiente. Ao ler ou gravar arquivos, o Lustre tem um desempenho muito melhor com tamanhos de buffer grandes (>= 1 MB). \u00c9 altamente recomend\u00e1vel agregar pequenas opera\u00e7\u00f5es de leitura e grava\u00e7\u00e3o em opera\u00e7\u00f5es maiores. O buffer coletivo MPI-IO permite E/S agregada. Evite pequenas opera\u00e7\u00f5es repetitivas de arquivos Evite realizar pequenas opera\u00e7\u00f5es de E/S repetitivas, como abrir arquivos frequentemente no modo de acr\u00e9scimo, gravar pequenas quantidades de dados e fechar o arquivo. Em vez disso, abra o arquivo uma vez, execute todas as opera\u00e7\u00f5es de E/S e feche. Evite v\u00e1rios processos abrindo os mesmos arquivos ao mesmo tempo V\u00e1rios processos abrindo os mesmos arquivos ao mesmo tempo podem criar conten\u00e7\u00e3o e erros de abertura de arquivos. Em vez disso, execute a abertura a partir de um \u00fanico processo (pai), ou abra o arquivo somente leitura para evitar bloqueio, ou implemente a abertura com uma abordagem de tentativa e erro com suspens\u00e3o em caso de erro. Evite acessar a mesma regi\u00e3o de arquivo de muitos processos Se v\u00e1rios processos acessarem a mesma regi\u00e3o de arquivo ao mesmo tempo, o gerenciador de bloqueio distribu\u00eddo Lustre refor\u00e7ar\u00e1 a coer\u00eancia para que todos os clientes vejam resultados consistentes. Ter muitos processos tentando acessar a mesma regi\u00e3o de arquivo simultaneamente pode causar degrada\u00e7\u00e3o no desempenho. Neste caso, pode ser prefer\u00edvel: replicar o arquivo, dividir o arquivo, executar as opera\u00e7\u00f5es de E/S a partir de uma \u00fanica classifica\u00e7\u00e3o de processo ou garantir que o acesso simult\u00e2neo n\u00e3o ocorrer\u00e1. Em qualquer caso, \u00e9 recomendado manter a quantidade de opera\u00e7\u00f5es de abertura e bloqueio de arquivos em paralelo t\u00e3o pequena quanto poss\u00edvel para reduzir a conten\u00e7\u00e3o. Se v\u00e1rios processos tentarem anexar ao mesmo arquivo, isso acionar\u00e1 o bloqueio e poder\u00e1 causar grande conten\u00e7\u00e3o. Idealmente, apenas um processo deve anexar cada arquivo. Opera\u00e7\u00f5es de arquivo atrav\u00e9s de processo pai Ao acessar pequenos arquivos compartilhados em uma tarefa paralela, muitas vezes \u00e9 mais eficiente executar todas as opera\u00e7\u00f5es necess\u00e1rias atrav\u00e9s do processo pai e, se necess\u00e1rio, transmitir os dados para outras classifica\u00e7\u00f5es, em vez de acessar os mesmos arquivos de todas as classifica\u00e7\u00f5es. Da mesma forma, se m\u00faltiplas classifica\u00e7\u00f5es de um trabalho paralelo requerem informa\u00e7\u00f5es sobre um determinado arquivo, a abordagem mais eficiente \u00e9 fazer com que o processo pai execute as chamadas necess\u00e1rias (por exemplo stat() , fstat() , etc) e ent\u00e3o transmita as informa\u00e7\u00f5es para as outras classifica\u00e7\u00f5es. Distribui\u00e7\u00e3o de arquivos (striping) No Lustre, arquivos grandes podem ser divididos em segmentos que, por sua vez, podem ser distribu\u00eddos automaticamente por v\u00e1rios dispositivos de armazenamento. A distribui\u00e7\u00e3o de arquivos \u00e9 \u00fatil para E/S paralela em arquivos grandes. Para que isso funcione, o ponto de montagem em quest\u00e3o aponta para v\u00e1rios dispositivos de armazenamento (OSTs). O comando lfs df pode ser usado para verificar se um determinado ponto de montagem aponta para v\u00e1rios OSTs. Para obter informa\u00e7\u00f5es de distribui\u00e7\u00e3o de arquivos para um determinado arquivo, use: lfs getstripe filename A distribui\u00e7\u00e3o do arquivo pode ser definida usando o comando lfs setstripe . Se o comando for aplicado a um diret\u00f3rio, ele definir\u00e1 as configura\u00e7\u00f5es de distribui\u00e7\u00e3o padr\u00e3o para arquivos criados nesse diret\u00f3rio. Um subdiret\u00f3rio herda todas as configura\u00e7\u00f5es de distribui\u00e7\u00e3o de seu diret\u00f3rio pai. Se o comando for aplicado a um arquivo, ele distribuir\u00e1 esse arquivo pelos OSTs de acordo com as configura\u00e7\u00f5es especificadas. lfs setstripe -s 128m -c 8 filename => divide o arquivo em segmentos de 128 MB e os distribui em 8 OSTs Se um arquivo grande for compartilhado em paralelo por v\u00e1rios processos, com cada processo trabalhando em sua pr\u00f3pria parte do arquivo, ent\u00e3o pode ser \u00fatil dividir o arquivo em um n\u00famero de segmentos igual ao n\u00famero de processos, ou um m\u00faltiplo do n\u00famero de processos. Para obter o m\u00e1ximo desempenho, as solicita\u00e7\u00f5es de E/S devem ser alinhadas \u00e0s faixas, o que significa que os processos que acessam o arquivo devem faz\u00ea-lo em deslocamentos que correspondam aos limites das faixas. Isto minimiza as chances de um processo ter que acessar mais de um segmento (e mais de um OST) para obter os dados necess\u00e1rios. Para arquivos pequenos, a distribui\u00e7\u00e3o (striping) deve ser desabilitada, isso pode ser conseguido definindo uma contagem de distribui\u00e7\u00e3o de 1. O mesmo se aplica se um arquivo grande for acessado por um \u00fanico processo. lfs setstripe -s 1m -c 1 meudiretorio/arquivospequenos/ Evite instalar software no Lustre Um software geralmente \u00e9 composto de muitos arquivos pequenos e, como mencionado anteriormente, acessar muitos arquivos pequenos no Lustre pode sobrecarregar os servidores de metadados. As compila\u00e7\u00f5es de software em particular podem ser melhor executadas localmente copiando ou descompactando o software para /tmp/$USER/ o para o seu homedir . Al\u00e9m disso, sob alta carga, o acesso de E/S aos sistemas de arquivos Lustre pode ser bloqueado. Se os execut\u00e1veis \u200b\u200bforem armazenados no Lustre e o acesso ao sistema de arquivos falhar, os execut\u00e1veis \u200b\u200bpoder\u00e3o travar. Portanto, sempre que poss\u00edvel, \u00e9 melhor copiar os execut\u00e1veis \u200b\u200bpara o /tmp dos n\u00f3s do cluster. \u00c1rea de scripts \u00b6 Os usu\u00e1rios poder\u00e3o acessar seu diret\u00f3rio de scripts atrav\u00e9s da vari\u00e1vel de ambiente, ou acessando o diret\u00f3rio com o caminho completo. cd $SCRIPTS Ou cd /scripts/<username> Essa \u00e1rea \u00e9 destinada ao armazenamento de scripts de submiss\u00e3o de jobs ao cluster e outros. Recomenda-se tamb\u00e9m utilizar esse caminho para a cria\u00e7\u00e3o de ambientes (envs) pynton e kernels. A quota padr\u00e3o do /scripts disponibilizada para usu\u00e1rios \u00e9: area bsoft bhard isoft ihard grace period /scripts 10 GB 12 GB 100k 120k 7 days Observa\u00e7\u00e3o: O diret\u00f3rio /scripts n\u00e3o \u00e9 afetado pelo processo de limpeza autom\u00e1tica. Homedir \u00b6 O diret\u00f3rio home \u00e9 uma \u00e1rea para os usu\u00e1rios armazenarem seus arquivos pessoais e \u00e9 acess\u00edvel atrav\u00e9s dos n\u00f3s de login do cluster e tamb\u00e9m na plataforma jupyter . A quota padr\u00e3o do homedir de cada usu\u00e1rio, segundo o seu perfil, \u00e9 apresentada abaixo: profile bsoft bhard isoft ihard grace period p\u00fablico geral 5 GB 7 GB 7000 10000 7 days p\u00fablico institucional 25 GB 30 GB 40000 50000 7 days colabora\u00e7\u00e3o 100 GB 120 GB 1000000 1200000 7 days Tip Para verificar os valores de quota configurado basta utilizar o comando: quota -s -u <username> /home . Observa\u00e7\u00e3o: O diret\u00f3rio /home do usu\u00e1rio n\u00e3o \u00e9 afetado pelo processo de limpeza autom\u00e1tica. Comandos \u00fateis \u00b6 a) Como verificar minha quota dispon\u00edvel? show_quota b) Como consultar os meus arquivos criados h\u00e1 mais de 60 dias? lfs find $SCRATCH --uid $UID -mtime +60 --print c) Como consultar os meus arquivos criados h\u00e1 menos de 60 dias? lfs find $SCRATCH --uid $UID -mtime -60 --print d) Como listar os OSTs do Lustre? lfs osts $SCRATCH e) Como listar os arquivos armazenados h\u00e1 mais de 60 dias em um determinado OST do Lustre? lfs find $SCRATCH -mtime +60 --print --obd t0-OST0002_UUID f) Como configurar o striping em diret\u00f3rio de modo a \"quebrar\" os arquivos e distribuir esses \"peda\u00e7os\" em 10 OSTs? lfs setstripe -c 10 $SCRATCH/meus_arquivos_grandes g) Como consultar o striping de arquivos/diret\u00f3rios? lfs setstripe -c $SCRATCH/meus_arquivos_grandes Tip O Lustre do LIneA foi projetado para trabalhar a 100Gbps, para alcan\u00e7ar o m\u00e1ximo de performance fa\u00e7a uso do striping e sempre com arquivos grandes (+1GB). NAS (NFS) \u00b6 Os sistemas de armazenamento NAS s\u00e3o utilizados para armazenamento de longo prazo e n\u00e3o est\u00e3o acess\u00edveis atrav\u00e9s dos n\u00f3s de processamento (HPC). Caracter\u00edsticas atuais: Fabricante Modelo Capacidade Instalado em Disponibilidade SGI IS5500 [1] 540TB Dez-2011 Fora de servi\u00e7o SGI IS5600 240TB Jul-2014 Em uso HPE APOLO 4510 1.2 PB Apr-2025 Em uso [1] este equipamento foi desativado em Jun/2023 devido a problemas f\u00edsicos. Backup \u00b6 \u00e1reas frequ\u00eancia tipo reten\u00e7\u00e3o /home di\u00e1rio incremental 30 dias /home semanal diferencial 30 dias /home mensal completo 90 dias /data - - - /scratch - - - /scripts - - - Refer\u00eancias \u00b6 Estas melhores pr\u00e1ticas foram compiladas a partir da experi\u00eancia do time do LIneA e das seguintes fontes: https://www.nas.nasa.gov/hecc/support/kb/lustre-best-practices_226.html https://hpcf.umbc.edu/general-productivity/lustre-best-practices/ https://wiki.gsi.de/foswiki/bin/view/Linux/LustreFs https://doc.lustre.org/lustre_manual.pdf","title":"Armazenamento"},{"location":"armazenamento/index.html#lustrefs-hpc","text":"O ambiente do cluster Apollo conta com sistema de arquivos de alta performance Lustre com dois n\u00edveis ( tiers ) de armazenamento, um em SSD com ~70 TB ( T0 ) e outro em HDD com ~500 TB ( T1 ), ambos conectados a uma rede infiniband EDR de 100 Gb/s. Os dois n\u00edveis de armazenamento est\u00e3o dispon\u00edveis em /scratch e /data .","title":"LustreFS (HPC)"},{"location":"armazenamento/index.html#area-de-scratch-e-quota","text":"Os usu\u00e1rios poder\u00e3o acessar seu diret\u00f3rio de scratch atrav\u00e9s da vari\u00e1vel de ambiente, ou acessando o diret\u00f3rio com o caminho completo. cd $SCRATCH Ou cd /scratch/users/<username> ATEN\u00c7\u00c3O Essa \u00e1rea N\u00c3O sofrer\u00e1 backup! Os arquivos que n\u00e3o foram modificados nos \u00faltimos 60 dias ser\u00e3o automaticamente removidos, o que torna tempor\u00e1rio o armazenamento de arquivos nessa \u00e1rea. Recomenda-se que os usu\u00e1rios realizem a transferencia dos arquivos importantes do $SCRATCH para o seu homedir . Warning O script de limpeza \u00e9 executado uma vez por semana, sempre nos fins de semana. A quota padr\u00e3o do /scratch disponibilizada para usu\u00e1rios com direito ao uso do Cluster \u00e9: area bsoft bhard isoft ihard grace period /scratch 100 GB 120 GB 100000 120000 7 days","title":"\u00c1rea de scratch e quota"},{"location":"armazenamento/index.html#boas-praticas","text":"Sistemas de arquivos distribu\u00eddos como o Lustre s\u00e3o ideais para ambientes HPC e HTC. Nesses ambientes, a carga de trabalho t\u00edpica consiste em arquivos grandes que precisam ser acessados \u200b\u200ba partir de muitos n\u00f3s de computa\u00e7\u00e3o com largura de banda muito alta e/ou baixa lat\u00eancia. Portanto, esses sistemas de arquivos s\u00e3o muito diferentes dos sistemas de arquivos usados \u200b\u200bem computadores desktop ou servidores isolados. Embora sejam excelentes no manuseio de arquivos grandes, eles tamb\u00e9m apresentam fortes limita\u00e7\u00f5es ao lidar com arquivos pequenos e padr\u00f5es de acesso mais comumente encontrados em ambientes corporativos e de desktop. As opera\u00e7\u00f5es que podem ser extremamente r\u00e1pidas em um disco local de esta\u00e7\u00e3o de trabalho podem ser dolorosamente lentas e caras em um sistema de arquivos Lustre, afetando tanto os usu\u00e1rios que executam essas opera\u00e7\u00f5es quanto, eventualmente, todos os outros usu\u00e1rios. Estas melhores pr\u00e1ticas e recomenda\u00e7\u00f5es t\u00eam como objetivo permitir um uso tranquilo do Lustre, minimizando ou evitando opera\u00e7\u00f5es desnecess\u00e1rias ou muito caras do sistema de arquivos. Evite acessar atributos de arquivos e diret\u00f3rios Acessar informa\u00e7\u00f5es de metadados, como atributos de arquivo (por exemplo, tipo, propriedade, prote\u00e7\u00e3o, tamanho, datas, etc.) no Lustre consome muitos recursos e pode degradar o desempenho do sistema de arquivos, especialmente quando realizado com frequ\u00eancia ou em diret\u00f3rios com grande quantidade de arquivos. Minimize o uso de chamadas de sistema que acessam ou modificam esses atributos, como stat() , statx() , open() , openat() , etc. O mesmo se aplica a comandos como ls -l (para todo o diret\u00f3rio) ou ls --color que fazem uso das chamadas mencionadas acima. Em vez disso, use um simples ls ou ls -l filename . Evite usar comandos que acessam metadados massivamente Evite usar comandos como ls -R , find , locate , du , df e similares. Esses comandos percorrem o sistema de arquivos recursivamente e/ou executam opera\u00e7\u00f5es pesadas de metadados. Eles s\u00e3o muito intensivos no acesso aos metadados do sistema de arquivos e podem degradar gravemente o desempenho geral do sistema de arquivos. Se for absolutamente necess\u00e1rio percorrer o sistema de arquivos recursivamente, use o comando fornecido com o Lustre lfs find em vez de find , por exemplo. Use o comando Lustre lfs Para minimizar o n\u00famero de chamadas Lustre RPC, sempre que poss\u00edvel use os comandos lfs em vez dos comandos fornecidos pelo sistema: lfs df => em vez de df lfs find => em vez de find Evite usar curingas Expandir os curingas exige muitos recursos. A execu\u00e7\u00e3o de comandos com curingas em um grande n\u00famero de arquivos pode levar muito tempo e afetar gravemente o desempenho do sistema de arquivos. Em vez de usar curingas, crie uma lista dos arquivos de destino e aplique o comando a cada um desses arquivos. Acesso somente leitura Sempre que poss\u00edvel, abra os arquivos como somente leitura usando O_RDONLY , al\u00e9m disso, se voc\u00ea n\u00e3o precisar atualizar o tempo de acesso ao arquivo, abra os arquivos como O_RDONLY | O_NOATIME . Se as informa\u00e7\u00f5es de tempo de acesso forem necess\u00e1rias durante a execu\u00e7\u00e3o de E/S paralela, deixe o processo pai abrir os arquivos como O_RDONLY e todas as outras classifica\u00e7\u00f5es abrirem os mesmos arquivos como O_RDONLY|O_NOATIME . Evite ter um grande n\u00famero de arquivos em um \u00fanico diret\u00f3rio Quando um arquivo \u00e9 acessado, o Lustre bloqueia o diret\u00f3rio pai. Quando muitos arquivos no mesmo diret\u00f3rio devem ser abertos, isso cria conten\u00e7\u00e3o. Gravar milhares de arquivos em um \u00fanico diret\u00f3rio produz uma carga massiva nos servidores de metadados Lustre, geralmente resultando na desativa\u00e7\u00e3o dos sistemas de arquivos. Acessar um \u00fanico diret\u00f3rio contendo milhares de arquivos pode causar grande conten\u00e7\u00e3o de recursos, degradando o desempenho do sistema de arquivos. A alternativa \u00e9 organizar os dados em v\u00e1rios subdiret\u00f3rios e dividir os arquivos entre eles. Uma abordagem comum \u00e9 usar a raiz quadrada do n\u00famero de arquivos, por exemplo, para 90.000 arquivos a raiz quadrada seria 300, portanto devem ser criados 300 diret\u00f3rios contendo 300 arquivos cada. Evite arquivos pequenos Acessar arquivos pequenos no sistema de arquivos Lustre \u00e9 muito ineficiente. O tamanho de arquivo recomendado \u00e9 superior a 1 GB. Reorganize os dados em arquivos grandes ou use formatos de arquivo como HDF5 . Alternativamente, se o tamanho total dos arquivos for pequeno, como alguns gigabytes, copie os arquivos pequenos para /tmp ou para um diret\u00f3rio tempor\u00e1rio local para cada n\u00f3 de computa\u00e7\u00e3o no in\u00edcio do trabalho (n\u00e3o se esque\u00e7a de transferir e/ou excluir os arquivos no fim). Essa abordagem pode ser combinada com o uso de ferramentas de arquivamento, como tar e armazenar pequenos arquivos em um ou mais tarballs grandes podem ser mantidos no Lustre de maneira mais eficiente. Ao ler ou gravar arquivos, o Lustre tem um desempenho muito melhor com tamanhos de buffer grandes (>= 1 MB). \u00c9 altamente recomend\u00e1vel agregar pequenas opera\u00e7\u00f5es de leitura e grava\u00e7\u00e3o em opera\u00e7\u00f5es maiores. O buffer coletivo MPI-IO permite E/S agregada. Evite pequenas opera\u00e7\u00f5es repetitivas de arquivos Evite realizar pequenas opera\u00e7\u00f5es de E/S repetitivas, como abrir arquivos frequentemente no modo de acr\u00e9scimo, gravar pequenas quantidades de dados e fechar o arquivo. Em vez disso, abra o arquivo uma vez, execute todas as opera\u00e7\u00f5es de E/S e feche. Evite v\u00e1rios processos abrindo os mesmos arquivos ao mesmo tempo V\u00e1rios processos abrindo os mesmos arquivos ao mesmo tempo podem criar conten\u00e7\u00e3o e erros de abertura de arquivos. Em vez disso, execute a abertura a partir de um \u00fanico processo (pai), ou abra o arquivo somente leitura para evitar bloqueio, ou implemente a abertura com uma abordagem de tentativa e erro com suspens\u00e3o em caso de erro. Evite acessar a mesma regi\u00e3o de arquivo de muitos processos Se v\u00e1rios processos acessarem a mesma regi\u00e3o de arquivo ao mesmo tempo, o gerenciador de bloqueio distribu\u00eddo Lustre refor\u00e7ar\u00e1 a coer\u00eancia para que todos os clientes vejam resultados consistentes. Ter muitos processos tentando acessar a mesma regi\u00e3o de arquivo simultaneamente pode causar degrada\u00e7\u00e3o no desempenho. Neste caso, pode ser prefer\u00edvel: replicar o arquivo, dividir o arquivo, executar as opera\u00e7\u00f5es de E/S a partir de uma \u00fanica classifica\u00e7\u00e3o de processo ou garantir que o acesso simult\u00e2neo n\u00e3o ocorrer\u00e1. Em qualquer caso, \u00e9 recomendado manter a quantidade de opera\u00e7\u00f5es de abertura e bloqueio de arquivos em paralelo t\u00e3o pequena quanto poss\u00edvel para reduzir a conten\u00e7\u00e3o. Se v\u00e1rios processos tentarem anexar ao mesmo arquivo, isso acionar\u00e1 o bloqueio e poder\u00e1 causar grande conten\u00e7\u00e3o. Idealmente, apenas um processo deve anexar cada arquivo. Opera\u00e7\u00f5es de arquivo atrav\u00e9s de processo pai Ao acessar pequenos arquivos compartilhados em uma tarefa paralela, muitas vezes \u00e9 mais eficiente executar todas as opera\u00e7\u00f5es necess\u00e1rias atrav\u00e9s do processo pai e, se necess\u00e1rio, transmitir os dados para outras classifica\u00e7\u00f5es, em vez de acessar os mesmos arquivos de todas as classifica\u00e7\u00f5es. Da mesma forma, se m\u00faltiplas classifica\u00e7\u00f5es de um trabalho paralelo requerem informa\u00e7\u00f5es sobre um determinado arquivo, a abordagem mais eficiente \u00e9 fazer com que o processo pai execute as chamadas necess\u00e1rias (por exemplo stat() , fstat() , etc) e ent\u00e3o transmita as informa\u00e7\u00f5es para as outras classifica\u00e7\u00f5es. Distribui\u00e7\u00e3o de arquivos (striping) No Lustre, arquivos grandes podem ser divididos em segmentos que, por sua vez, podem ser distribu\u00eddos automaticamente por v\u00e1rios dispositivos de armazenamento. A distribui\u00e7\u00e3o de arquivos \u00e9 \u00fatil para E/S paralela em arquivos grandes. Para que isso funcione, o ponto de montagem em quest\u00e3o aponta para v\u00e1rios dispositivos de armazenamento (OSTs). O comando lfs df pode ser usado para verificar se um determinado ponto de montagem aponta para v\u00e1rios OSTs. Para obter informa\u00e7\u00f5es de distribui\u00e7\u00e3o de arquivos para um determinado arquivo, use: lfs getstripe filename A distribui\u00e7\u00e3o do arquivo pode ser definida usando o comando lfs setstripe . Se o comando for aplicado a um diret\u00f3rio, ele definir\u00e1 as configura\u00e7\u00f5es de distribui\u00e7\u00e3o padr\u00e3o para arquivos criados nesse diret\u00f3rio. Um subdiret\u00f3rio herda todas as configura\u00e7\u00f5es de distribui\u00e7\u00e3o de seu diret\u00f3rio pai. Se o comando for aplicado a um arquivo, ele distribuir\u00e1 esse arquivo pelos OSTs de acordo com as configura\u00e7\u00f5es especificadas. lfs setstripe -s 128m -c 8 filename => divide o arquivo em segmentos de 128 MB e os distribui em 8 OSTs Se um arquivo grande for compartilhado em paralelo por v\u00e1rios processos, com cada processo trabalhando em sua pr\u00f3pria parte do arquivo, ent\u00e3o pode ser \u00fatil dividir o arquivo em um n\u00famero de segmentos igual ao n\u00famero de processos, ou um m\u00faltiplo do n\u00famero de processos. Para obter o m\u00e1ximo desempenho, as solicita\u00e7\u00f5es de E/S devem ser alinhadas \u00e0s faixas, o que significa que os processos que acessam o arquivo devem faz\u00ea-lo em deslocamentos que correspondam aos limites das faixas. Isto minimiza as chances de um processo ter que acessar mais de um segmento (e mais de um OST) para obter os dados necess\u00e1rios. Para arquivos pequenos, a distribui\u00e7\u00e3o (striping) deve ser desabilitada, isso pode ser conseguido definindo uma contagem de distribui\u00e7\u00e3o de 1. O mesmo se aplica se um arquivo grande for acessado por um \u00fanico processo. lfs setstripe -s 1m -c 1 meudiretorio/arquivospequenos/ Evite instalar software no Lustre Um software geralmente \u00e9 composto de muitos arquivos pequenos e, como mencionado anteriormente, acessar muitos arquivos pequenos no Lustre pode sobrecarregar os servidores de metadados. As compila\u00e7\u00f5es de software em particular podem ser melhor executadas localmente copiando ou descompactando o software para /tmp/$USER/ o para o seu homedir . Al\u00e9m disso, sob alta carga, o acesso de E/S aos sistemas de arquivos Lustre pode ser bloqueado. Se os execut\u00e1veis \u200b\u200bforem armazenados no Lustre e o acesso ao sistema de arquivos falhar, os execut\u00e1veis \u200b\u200bpoder\u00e3o travar. Portanto, sempre que poss\u00edvel, \u00e9 melhor copiar os execut\u00e1veis \u200b\u200bpara o /tmp dos n\u00f3s do cluster.","title":"Boas pr\u00e1ticas"},{"location":"armazenamento/index.html#area-de-scripts","text":"Os usu\u00e1rios poder\u00e3o acessar seu diret\u00f3rio de scripts atrav\u00e9s da vari\u00e1vel de ambiente, ou acessando o diret\u00f3rio com o caminho completo. cd $SCRIPTS Ou cd /scripts/<username> Essa \u00e1rea \u00e9 destinada ao armazenamento de scripts de submiss\u00e3o de jobs ao cluster e outros. Recomenda-se tamb\u00e9m utilizar esse caminho para a cria\u00e7\u00e3o de ambientes (envs) pynton e kernels. A quota padr\u00e3o do /scripts disponibilizada para usu\u00e1rios \u00e9: area bsoft bhard isoft ihard grace period /scripts 10 GB 12 GB 100k 120k 7 days Observa\u00e7\u00e3o: O diret\u00f3rio /scripts n\u00e3o \u00e9 afetado pelo processo de limpeza autom\u00e1tica.","title":"\u00c1rea de scripts"},{"location":"armazenamento/index.html#homedir","text":"O diret\u00f3rio home \u00e9 uma \u00e1rea para os usu\u00e1rios armazenarem seus arquivos pessoais e \u00e9 acess\u00edvel atrav\u00e9s dos n\u00f3s de login do cluster e tamb\u00e9m na plataforma jupyter . A quota padr\u00e3o do homedir de cada usu\u00e1rio, segundo o seu perfil, \u00e9 apresentada abaixo: profile bsoft bhard isoft ihard grace period p\u00fablico geral 5 GB 7 GB 7000 10000 7 days p\u00fablico institucional 25 GB 30 GB 40000 50000 7 days colabora\u00e7\u00e3o 100 GB 120 GB 1000000 1200000 7 days Tip Para verificar os valores de quota configurado basta utilizar o comando: quota -s -u <username> /home . Observa\u00e7\u00e3o: O diret\u00f3rio /home do usu\u00e1rio n\u00e3o \u00e9 afetado pelo processo de limpeza autom\u00e1tica.","title":"Homedir"},{"location":"armazenamento/index.html#comandos-uteis","text":"a) Como verificar minha quota dispon\u00edvel? show_quota b) Como consultar os meus arquivos criados h\u00e1 mais de 60 dias? lfs find $SCRATCH --uid $UID -mtime +60 --print c) Como consultar os meus arquivos criados h\u00e1 menos de 60 dias? lfs find $SCRATCH --uid $UID -mtime -60 --print d) Como listar os OSTs do Lustre? lfs osts $SCRATCH e) Como listar os arquivos armazenados h\u00e1 mais de 60 dias em um determinado OST do Lustre? lfs find $SCRATCH -mtime +60 --print --obd t0-OST0002_UUID f) Como configurar o striping em diret\u00f3rio de modo a \"quebrar\" os arquivos e distribuir esses \"peda\u00e7os\" em 10 OSTs? lfs setstripe -c 10 $SCRATCH/meus_arquivos_grandes g) Como consultar o striping de arquivos/diret\u00f3rios? lfs setstripe -c $SCRATCH/meus_arquivos_grandes Tip O Lustre do LIneA foi projetado para trabalhar a 100Gbps, para alcan\u00e7ar o m\u00e1ximo de performance fa\u00e7a uso do striping e sempre com arquivos grandes (+1GB).","title":"Comandos \u00fateis"},{"location":"armazenamento/index.html#nas-nfs","text":"Os sistemas de armazenamento NAS s\u00e3o utilizados para armazenamento de longo prazo e n\u00e3o est\u00e3o acess\u00edveis atrav\u00e9s dos n\u00f3s de processamento (HPC). Caracter\u00edsticas atuais: Fabricante Modelo Capacidade Instalado em Disponibilidade SGI IS5500 [1] 540TB Dez-2011 Fora de servi\u00e7o SGI IS5600 240TB Jul-2014 Em uso HPE APOLO 4510 1.2 PB Apr-2025 Em uso [1] este equipamento foi desativado em Jun/2023 devido a problemas f\u00edsicos.","title":"NAS (NFS)"},{"location":"armazenamento/index.html#backup","text":"\u00e1reas frequ\u00eancia tipo reten\u00e7\u00e3o /home di\u00e1rio incremental 30 dias /home semanal diferencial 30 dias /home mensal completo 90 dias /data - - - /scratch - - - /scripts - - -","title":"Backup"},{"location":"armazenamento/index.html#referencias","text":"Estas melhores pr\u00e1ticas foram compiladas a partir da experi\u00eancia do time do LIneA e das seguintes fontes: https://www.nas.nasa.gov/hecc/support/kb/lustre-best-practices_226.html https://hpcf.umbc.edu/general-productivity/lustre-best-practices/ https://wiki.gsi.de/foswiki/bin/view/Linux/LustreFs https://doc.lustre.org/lustre_manual.pdf","title":"Refer\u00eancias"},{"location":"data/index.html","text":"Acervo de Dados \u00b6 Cat\u00e1logos \u00b6 As informa\u00e7\u00f5es relevantes sobre cat\u00e1logos astron\u00f4micos p\u00fablicos hospedados no LIneA est\u00e3o dispon\u00edveis nas p\u00e1ginas de documenta\u00e7\u00e3o do servi\u00e7o User Query . Para acess\u00e1-las, clique no menu \"Database Tables\" e selecione o conjunto de dados desejado. Imagens \u00b6 As imagens p\u00fablicas hospedadas no LIneA e dispon\u00edveis nos servi\u00e7os de visualiza\u00e7\u00e3o pertencem ao release DES DR2. Para mais informa\u00e7\u00f5es, acesse a publica\u00e7\u00e3o The Dark Energy Survey Data Release 2 . Produtos de Photo-z \u00b6 Produtos P\u00fablicos de Photo-z \u00b6 (em breve) Produtos Privados Photo-z do LSST \u00b6 Documenta\u00e7\u00e3o dos produtos privados do LSST Photo-z Server","title":"Dados"},{"location":"data/index.html#acervo-de-dados","text":"","title":"Acervo de Dados"},{"location":"data/index.html#catalogos","text":"As informa\u00e7\u00f5es relevantes sobre cat\u00e1logos astron\u00f4micos p\u00fablicos hospedados no LIneA est\u00e3o dispon\u00edveis nas p\u00e1ginas de documenta\u00e7\u00e3o do servi\u00e7o User Query . Para acess\u00e1-las, clique no menu \"Database Tables\" e selecione o conjunto de dados desejado.","title":"Cat\u00e1logos"},{"location":"data/index.html#imagens","text":"As imagens p\u00fablicas hospedadas no LIneA e dispon\u00edveis nos servi\u00e7os de visualiza\u00e7\u00e3o pertencem ao release DES DR2. Para mais informa\u00e7\u00f5es, acesse a publica\u00e7\u00e3o The Dark Energy Survey Data Release 2 .","title":"Imagens"},{"location":"data/index.html#produtos-de-photo-z","text":"","title":"Produtos de Photo-z"},{"location":"data/index.html#produtos-publicos-de-photo-z","text":"(em breve)","title":"Produtos P\u00fablicos de Photo-z"},{"location":"data/index.html#produtos-privados-photo-z-do-lsst","text":"Documenta\u00e7\u00e3o dos produtos privados do LSST Photo-z Server","title":"Produtos Privados Photo-z do LSST"},{"location":"data/des.html","text":"DES \u00b6 Warning Esta p\u00e1gina est\u00e1 incompleta, pois est\u00e1 sendo constru\u00edda DES datasets available at LIneA Levantamento inicial de informa\u00e7\u00f5es sobre os datasets do DES dispon\u00edveis no ambiente Archive \u00b6 DR2 \u00b6 There is no DR2 on Archive Paper or reference: https://ui.adsabs.harvard.edu/abs/2021ApJS..255...20A/abstract Description (paper's abstract): We present the second public data release of the Dark Energy Survey, DES DR2, based on optical/near-infrared imaging by the Dark Energy Camera mounted on the 4 m Blanco telescope at Cerro Tololo Inter-American Observatory in Chile. DES DR2 consists of reduced single-epoch and coadded images, a source catalog derived from coadded images, and associated data products assembled from 6 yr of DES science operations. This release includes data from the DES wide-area survey covering ~5000 deg2 of the southern Galactic cap in five broad photometric bands, grizY. DES DR2 has a median delivered point-spread function FWHM of g = 1.11 arcsec, r = 0.95 arcsec, i = 0.88 arcsec, z = 0.83 arcsec, and Y = 0.90 arcsec, photometric uniformity with a standard deviation of <3 mmag with respect to Gaia DR2 G band, a photometric accuracy of ~11 mmag, and a median internal astrometric precision of ~27 mas. The median coadded catalog depth for a 1.95 arcsec diameter aperture at signal-to-noise ratio = 10 is g = 24.7, r = 24.4, i = 23.8, z = 23.1, and Y = 21.7 mag. DES DR2 includes ~691 million distinct astronomical objects detected in 10,169 coadded image tiles of size 0.534 deg2 produced from 76,217 single-epoch images. After a basic quality selection, benchmark galaxy and stellar samples contain 543 million and 145 million objects, respectively. These data are accessible through several interfaces, including interactive image visualization tools, web-based query clients, image cutout servers, and Jupyter notebooks. DES DR2 constitutes the largest photometric data set to date at the achieved depth and photometric precision. Data access (catalogs and images): easyaccess (internal collaboration) https://desportal.cosmology.illinois.edu/ (internal collaboration) https://des.ncsa.illinois.edu/desaccess/ (public) https://desportal2.cosmology.illinois.edu/ (public) https://datalab.noirlab.edu (public) Y6A1_COADD and Y6A1_GOLD (and SMALL versions) \u00b6 tree -L 2 Y6A1_COADD/ Y6A1_COADD/ \u251c\u2500\u2500 Y6A1_COADD \u2502 \u251c\u2500\u2500 cats \u2502 \u251c\u2500\u2500 depth_maps \u2502 \u2514\u2500\u2500 masks \u251c\u2500\u2500 Y6A1_COADD_SMALL \u2502 \u251c\u2500\u2500 healpix \u2502 \u251c\u2500\u2500 masks \u2502 \u2514\u2500\u2500 masks_ -> ../Y6A1_COADD/masks \u251c\u2500\u2500 Y6A1_GOLD \u2502 \u251c\u2500\u2500 cats \u2502 \u2514\u2500\u2500 masks \u2514\u2500\u2500 Y6A1_GOLD_SMALL \u251c\u2500\u2500 healpix \u2514\u2500\u2500 masks 14 directories, 0 files du -khs Y6A1_COADD/*/* 3.6T Y6A1_COADD/Y6A1_COADD/cats 2.4G Y6A1_COADD/Y6A1_COADD/depth_maps 1.2T Y6A1_COADD/Y6A1_COADD/masks 46G Y6A1_COADD/Y6A1_COADD_SMALL/healpix 16K Y6A1_COADD/Y6A1_COADD_SMALL/masks 0 Y6A1_COADD/Y6A1_COADD_SMALL/masks_ 2.3T Y6A1_COADD/Y6A1_GOLD/cats 368K Y6A1_COADD/Y6A1_GOLD/masks 3.0G Y6A1_COADD/Y6A1_GOLD_SMALL/healpix 4.0K Y6A1_COADD/Y6A1_GOLD_SMALL/masks - Paper (or reference): https://opensource.ncsa.illinois.edu/confluence/pages/viewpage.action?spaceKey=DESDM&title=Y6A1+Release+Notes https://cdcvs.fnal.gov/redmine/projects/des-y6/wiki/Y6_Gold_release - Description: This is not a public catalog, only available for the collaboration. Y6A1 is the first version of DES-Y6 coadd catalog and image, covering all five bands. Y6A1_COADD here is the table Y6A1_COADD_OBJECT_SUMMARY available for the collaboration (also available on easyaccess), and the catalog here is the full catalog for Y6A1 coadd, with 186 columns and 691498505 objects. The catalog is available into two sets: balanced and healpix with only nside 32 available. There is a version of Y6A1_GOLD catalog, which is a catalog based on Y6A1_COADD, but with a few measurements added. The catalog here has 125 columns of the version Y6_GOLD_1_1 (in easyaccess the versions available are Y6_GOLD_1_1, Y6_GOLD_2_0, and Y6_GOLD_2_1), with 334 columns. The catalog Y6_GOLD_1_1 in easyaccess has 690153156 objects. The catalog is available into two sets: balanced and healpix with only nside 32 available. There is a minimal documentation about the depth maps (where they came from, etc). The masks are the complete list of files (*.pol, *.area, *.count, *.fits, *.maglims, *.red, *.time, *.weight) needed to run the systematic maps. Files are organized per tile and band (101689 tiles and five bands). - Data access (catalogs and images): easyaccess (internal collaboration) Y6A2_COADD, Y6A2_GOLD, Y6A2_SOF (and SMALL versions) \u00b6 tree -L 2 Y6A2_COADD/ Y6A2_COADD/ \u251c\u2500\u2500 masks \u251c\u2500\u2500 Y6A2_COADD \u2502 \u251c\u2500\u2500 cats \u2502 \u251c\u2500\u2500 cats_y6a2 \u2502 \u251c\u2500\u2500 fits \u2502 \u251c\u2500\u2500 healpix \u2502 \u251c\u2500\u2500 maps \u2502 \u251c\u2500\u2500 masks \u2502 \u251c\u2500\u2500 pngs \u2502 \u2514\u2500\u2500 stilts_converter_Y6A2-coadd_to_Y6A2-small.sh \u251c\u2500\u2500 Y6A2_COADD_SMALL \u2502 \u2514\u2500\u2500 healpix \u251c\u2500\u2500 Y6A2_GOLD \u2502 \u251c\u2500\u2500 cats \u2502 \u251c\u2500\u2500 install \u2502 \u251c\u2500\u2500 Y6A2_GOLD_LIMIT.27000000_OFFSET.0.fits \u2502 \u2514\u2500\u2500 Y6_Gold_2_0.csv \u251c\u2500\u2500 Y6A2_GOLD_SMALL \u2502 \u251c\u2500\u2500 cats \u2502 \u251c\u2500\u2500 healpix \u2502 \u2514\u2500\u2500 install \u251c\u2500\u2500 Y6A2_SOF_V2 \u2502 \u251c\u2500\u2500 cats \u2502 \u251c\u2500\u2500 healpix \u2502 \u251c\u2500\u2500 install \u2502 \u2514\u2500\u2500 Y6A2_SOF_V2.csv \u2514\u2500\u2500 Y6_Gold_2_0 \u2514\u2500\u2500 Y6_Gold_2_0 du -khs Y6A2_COADD/*/* 274M Y6A2_COADD/Y6A2_COADD/cats 0 Y6A2_COADD/Y6A2_COADD/cats_y6a2 0 Y6A2_COADD/Y6A2_COADD/fits 898G Y6A2_COADD/Y6A2_COADD/healpix 0 Y6A2_COADD/Y6A2_COADD/maps 9.1G Y6A2_COADD/Y6A2_COADD/masks 0 Y6A2_COADD/Y6A2_COADD/pngs 4.0K Y6A2_COADD/Y6A2_COADD_SMALL/healpix 4.0K Y6A2_COADD/Y6A2_COADD/stilts_converter_Y6A2-coadd_to_Y6A2-small.sh 1.7T Y6A2_COADD/Y6A2_GOLD/cats 36K Y6A2_COADD/Y6A2_GOLD/install 0 Y6A2_COADD/Y6A2_GOLD_SMALL/cats 3.2G Y6A2_COADD/Y6A2_GOLD_SMALL/healpix 196K Y6A2_COADD/Y6A2_GOLD_SMALL/install 15G Y6A2_COADD/Y6A2_GOLD/Y6A2_GOLD_LIMIT.27000000_OFFSET.0.fits 132K Y6A2_COADD/Y6A2_GOLD/Y6_Gold_2_0.csv 904G Y6A2_COADD/Y6A2_SOF_V2/cats 0 Y6A2_COADD/Y6A2_SOF_V2/healpix 14M Y6A2_COADD/Y6A2_SOF_V2/install 132K Y6A2_COADD/Y6A2_SOF_V2/Y6A2_SOF_V2.csv 0 Y6A2_COADD/Y6_Gold_2_0/Y6_Gold_2_0 - Paper (or reference): https://opensource.ncsa.illinois.edu/confluence/pages/viewpage.action?spaceKey=DESDM&title=Y6A1+Release+Notes - Description: This is not a public catalog, only available for the collaboration. Y6A2 is the reprocessing of only 73 tiles after detect an issue in align DECam images (see the Y6A1 description for reference). See the list of tiles in https://desar2.cosmology.illinois.edu/DESFiles/desarchive/OPS/multiepoch/Y6A2/r5137/ . Files in Y6A2_COADD/Y6A2_COADD/healpix/32/ .fits here is the table Y6A2_COADD_OBJECT_SUMMARY available for the collaboration, with 186 columns and 691483608 objects. The catalog is available into two sets: balanced and healpix with only nside 32 available. The masks are the set list of files ( .pol, *.area, *.count, *.fits, *.maglims, *.red, *.time, *.weight) needed to run the systematic maps for only the tiles that changed in Y6A1 to Y6A2. Y6A2_GOLD is the catalog Y6_GOLD_2_0 version available on easyaccess. The files are split by tile in folder Y6A2_GOLD/cats. The files copied here have the same amount of 333 columns and 691483608 objects. Data access (catalogs and images): easyaccess (internal collaboration) T1 tree -L 2 . . \u251c\u2500\u2500 desar2.cosmology.illinois.edu \u2502 \u2514\u2500\u2500 DESFiles \u251c\u2500\u2500 dr2 \u2502 \u251c\u2500\u2500 fits \u2502 \u2514\u2500\u2500 ptifs \u251c\u2500\u2500 images \u2502 \u2514\u2500\u2500 y6a1_hips \u251c\u2500\u2500 process \u2502 \u251c\u2500\u2500 production \u2502 \u251c\u2500\u2500 testing \u2502 \u2514\u2500\u2500 testnagios \u251c\u2500\u2500 Y6A2_COADD \u2502 \u251c\u2500\u2500 Y6A2_GOLD \u2502 \u2514\u2500\u2500 Y6A2_GOLD_SMALL \u251c\u2500\u2500 y6a2_gold \u2502 \u2514\u2500\u2500 cats \u251c\u2500\u2500 Y6A2_GOLD \u2502 \u2514\u2500\u2500 cats \u251c\u2500\u2500 Y6A2_GOLD_FITS \u2502 \u2514\u2500\u2500 BALANCED \u2514\u2500\u2500 Y6_SUBSET_HIPS \u251c\u2500\u2500 AladinBeta.jar \u251c\u2500\u2500 Hipsgen-cat.jar \u251c\u2500\u2500 imagens \u2514\u2500\u2500 outputs 23 directories, 2 files Getting information about datasets volume size","title":"DES"},{"location":"data/des.html#des","text":"Warning Esta p\u00e1gina est\u00e1 incompleta, pois est\u00e1 sendo constru\u00edda DES datasets available at LIneA Levantamento inicial de informa\u00e7\u00f5es sobre os datasets do DES dispon\u00edveis no ambiente","title":"DES"},{"location":"data/des.html#archive","text":"","title":"Archive"},{"location":"data/lsst.html","text":"LSST \u00b6 Warning Esta p\u00e1gina est\u00e1 incompleta, pois est\u00e1 sendo constru\u00edda LSST datasets available at LIneA Archive \u00b6 \u251c\u2500\u2500 calexp \u251c\u2500\u2500 cosmo_dc2 \u251c\u2500\u2500 dc2 \u251c\u2500\u2500 dp0 \u2514\u2500\u2500 images 5 directories, 0 files du -khs * 17G calexp 103G cosmo_dc2 5.5T dc2 345M dp0 36G images T1 \u00b6 tree -L 2 . . \u251c\u2500\u2500 cosmo_dc2 \u2502 \u2514\u2500\u2500 EXTRAGALACTIC \u251c\u2500\u2500 dp0 \u2502 \u2514\u2500\u2500 lsst_dp0 \u251c\u2500\u2500 dp0.2 \u2502 \u251c\u2500\u2500 log_dp0.txt \u2502 \u251c\u2500\u2500 md5sum \u2502 \u251c\u2500\u2500 objectTable_tract_2897_DC2_2_2i_runs_DP0_2_v23_0_1_PREOPS-905_step3_1_20220317T233937Z.parq | \u251c\u2500\u2500 [...] 157 arquivos .parq ocultados \u2502 \u251c\u2500\u2500 objectTable_tract_5074_DC2_2_2i_runs_DP0_2_v23_0_1_PREOPS-905_step3_31_20220314T212509Z.parq \u2502 \u2514\u2500\u2500 objectTable_tract.txt \u251c\u2500\u2500 dp0_skinny \u2502 \u2514\u2500\u2500 DP0 \u251c\u2500\u2500 dr1 \u251c\u2500\u2500 dr2 \u251c\u2500\u2500 gawa_project \u2502 \u251c\u2500\u2500 adriano.pieres \u2502 \u251c\u2500\u2500 input_data \u2502 \u251c\u2500\u2500 outputs \u2502 \u251c\u2500\u2500 raslan.oliveira \u2502 \u2514\u2500\u2500 singulani \u251c\u2500\u2500 tmp \u2502 \u2514\u2500\u2500 henrique.almeida \u2514\u2500\u2500 wazp_project \u251c\u2500\u2500 carlos \u251c\u2500\u2500 datasets \u251c\u2500\u2500 tiles_slices \u251c\u2500\u2500 wazp \u251c\u2500\u2500 wazp-errors.txt \u251c\u2500\u2500 wazp.log \u251c\u2500\u2500 wazp-outfile.txt \u251c\u2500\u2500 wazp_sdumont \u2514\u2500\u2500 wazp_tiles 24 directories, 163 files du -khs */* 354G cosmo_dc2/EXTRAGALACTIC 12G dp0.2/log_dp0.txt 24K dp0.2/md5sum 6.6G dp0.2/objectTable_tract_2897_DC2_2_2i_runs_DP0_2_v23_0_1_PREOPS-905_step3_1_20220317T233937Z.parq [...] 157 arquivos .parq ocultados 5.9G dp0.2/objectTable_tract_5074_DC2_2_2i_runs_DP0_2_v23_0_1_PREOPS-905_step3_31_20220314T212509Z.parq 32K dp0.2/objectTable_tract.txt 2.5T dp0/lsst_dp0 69G dp0_skinny/DP0","title":"LSST"},{"location":"data/lsst.html#lsst","text":"Warning Esta p\u00e1gina est\u00e1 incompleta, pois est\u00e1 sendo constru\u00edda LSST datasets available at LIneA","title":"LSST"},{"location":"data/lsst.html#archive","text":"\u251c\u2500\u2500 calexp \u251c\u2500\u2500 cosmo_dc2 \u251c\u2500\u2500 dc2 \u251c\u2500\u2500 dp0 \u2514\u2500\u2500 images 5 directories, 0 files du -khs * 17G calexp 103G cosmo_dc2 5.5T dc2 345M dp0 36G images","title":"Archive"},{"location":"data/lsst.html#t1","text":"tree -L 2 . . \u251c\u2500\u2500 cosmo_dc2 \u2502 \u2514\u2500\u2500 EXTRAGALACTIC \u251c\u2500\u2500 dp0 \u2502 \u2514\u2500\u2500 lsst_dp0 \u251c\u2500\u2500 dp0.2 \u2502 \u251c\u2500\u2500 log_dp0.txt \u2502 \u251c\u2500\u2500 md5sum \u2502 \u251c\u2500\u2500 objectTable_tract_2897_DC2_2_2i_runs_DP0_2_v23_0_1_PREOPS-905_step3_1_20220317T233937Z.parq | \u251c\u2500\u2500 [...] 157 arquivos .parq ocultados \u2502 \u251c\u2500\u2500 objectTable_tract_5074_DC2_2_2i_runs_DP0_2_v23_0_1_PREOPS-905_step3_31_20220314T212509Z.parq \u2502 \u2514\u2500\u2500 objectTable_tract.txt \u251c\u2500\u2500 dp0_skinny \u2502 \u2514\u2500\u2500 DP0 \u251c\u2500\u2500 dr1 \u251c\u2500\u2500 dr2 \u251c\u2500\u2500 gawa_project \u2502 \u251c\u2500\u2500 adriano.pieres \u2502 \u251c\u2500\u2500 input_data \u2502 \u251c\u2500\u2500 outputs \u2502 \u251c\u2500\u2500 raslan.oliveira \u2502 \u2514\u2500\u2500 singulani \u251c\u2500\u2500 tmp \u2502 \u2514\u2500\u2500 henrique.almeida \u2514\u2500\u2500 wazp_project \u251c\u2500\u2500 carlos \u251c\u2500\u2500 datasets \u251c\u2500\u2500 tiles_slices \u251c\u2500\u2500 wazp \u251c\u2500\u2500 wazp-errors.txt \u251c\u2500\u2500 wazp.log \u251c\u2500\u2500 wazp-outfile.txt \u251c\u2500\u2500 wazp_sdumont \u2514\u2500\u2500 wazp_tiles 24 directories, 163 files du -khs */* 354G cosmo_dc2/EXTRAGALACTIC 12G dp0.2/log_dp0.txt 24K dp0.2/md5sum 6.6G dp0.2/objectTable_tract_2897_DC2_2_2i_runs_DP0_2_v23_0_1_PREOPS-905_step3_1_20220317T233937Z.parq [...] 157 arquivos .parq ocultados 5.9G dp0.2/objectTable_tract_5074_DC2_2_2i_runs_DP0_2_v23_0_1_PREOPS-905_step3_31_20220314T212509Z.parq 32K dp0.2/objectTable_tract.txt 2.5T dp0/lsst_dp0 69G dp0_skinny/DP0","title":"T1"},{"location":"data/pz_server_data.html","text":"Photo-z Server Data \u00b6 Datasets Tradu\u00e7\u00e3o para o portugu\u00eas dispon\u00edvel em breve. Confira a p\u00e1gina original em ingl\u00eas aqui .","title":"Photo-z Server Data"},{"location":"data/pz_server_data.html#photo-z-server-data","text":"Datasets Tradu\u00e7\u00e3o para o portugu\u00eas dispon\u00edvel em breve. Confira a p\u00e1gina original em ingl\u00eas aqui .","title":"Photo-z Server Data"},{"location":"exemplos/exemplos.html","text":"icons and emojis \u00b6 https://squidfunk.github.io/mkdocs-material/reference/icons-emojis/#icons-emojis Quer adicionar um destaque sem quebrar o fluxo do texto? Use admonitions! \u00b6 Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Mais exemplos de admonitions \u00b6 https://squidfunk.github.io/mkdocs-material/reference/admonitions/#usage","title":"Exemplos"},{"location":"processamento/index.html","text":"Computa\u00e7\u00e3o de Alto Desempenho \u00b6 O LIneA oferece acesso recursos computacionais de alto desempenho a usu\u00e1rios membros de colabora\u00e7\u00f5es cient\u00edficas e com projetos apoiados pelo laborat\u00f3rio nos seguintes ambientes: O registro no ambiente do LIneA N\u00c3O concede acesso autom\u00e1tico aos recursos de HPC. Ap\u00f3s ter o seu registro aprovado, \u00e9 necess\u00e1rio abrir um ticket e solicitar o acesso, enviando uma justificativa de uso, que ser\u00e1 encaminhada para avalia\u00e7\u00e3o do Comit\u00ea Gestor. A n\u00e3o observ\u00e2ncia das nossas pol\u00edticas de seguran\u00e7a e de uso do ambiente, pode acarretar no bloqueio de sua conta sem aviso pr\u00e9vio. As pol\u00edticas podem ser encontradas aqui . Cluster HPE Apollo 2000 (LIneA) Open OnDemand JupyterLab over HPC","title":"Processamento (HPC)"},{"location":"processamento/index.html#computacao-de-alto-desempenho","text":"O LIneA oferece acesso recursos computacionais de alto desempenho a usu\u00e1rios membros de colabora\u00e7\u00f5es cient\u00edficas e com projetos apoiados pelo laborat\u00f3rio nos seguintes ambientes: O registro no ambiente do LIneA N\u00c3O concede acesso autom\u00e1tico aos recursos de HPC. Ap\u00f3s ter o seu registro aprovado, \u00e9 necess\u00e1rio abrir um ticket e solicitar o acesso, enviando uma justificativa de uso, que ser\u00e1 encaminhada para avalia\u00e7\u00e3o do Comit\u00ea Gestor. A n\u00e3o observ\u00e2ncia das nossas pol\u00edticas de seguran\u00e7a e de uso do ambiente, pode acarretar no bloqueio de sua conta sem aviso pr\u00e9vio. As pol\u00edticas podem ser encontradas aqui . Cluster HPE Apollo 2000 (LIneA) Open OnDemand JupyterLab over HPC","title":"Computa\u00e7\u00e3o de Alto Desempenho"},{"location":"processamento/sdu.html","text":"O LIneA possui um projeto \"guarda-chuva\" aprovado no LNCC chamado \" Explorando o Universo via big data: do sistema solar \u00e0 energia escura \" (sigla EUBD) que garante o direito \u00e0 utiliza\u00e7\u00e3o de 2 milh\u00f5es de horas de CPU do supercomputador Santos Dumont para apoiar alguns subprojetos que dependem de High-Performance Computing (HPC) nas seguintes \u00e1reas: Sistema Solar Via-L\u00e1ctea/Volume Local Energia Escura Estruturas de Larga Escala Redshifts Fotom\u00e9tricos (programa de contribui\u00e7\u00f5es in-kind LSST) Info Caso seu projeto tenha demandas de HPC e ainda n\u00e3o fa\u00e7a parte do escopo do projeto EUBD, entre em contato conosco atrav\u00e9s do e-mail helpdesk@linea.org.br para receber orienta\u00e7\u00f5es. Registro de usu\u00e1rios para o acesso ao Super Computador - Projeto EUBD \u00b6 Envie um e-mail, seguindo o modelo abaixo para helpdesk@linea.org.br e aguarde o nosso retorno (no m\u00e1ximo em at\u00e9 72hs).: a.) No campo assunto escreva: Acesso ao supercomputador Santos Dumont - Projeto EUBD b.) No corpo do email: 1. Seu nome completo: 2. Nome da institui\u00e7\u00e3o (acr\u00f4nimo): 3. Nome do orientador (se houver): 4. E-mail do orientador/supervisor (se houver): 5. Justificativa de uso: - Escreva em no m\u00e1ximo 5 linhas qual \u00e9 o objetivo e a import\u00e2ncia de uso desses recursos computacionais. - Se poss\u00edvel, informe quantas CPU/horas pretende utilizar e tamb\u00e9m qual o volume de dados de entrada e de sa\u00edda da sua aplica\u00e7\u00e3o. - Utilize KiloBytes(KB), MegaBytes(MB) ou GigaBytes(GB) para representar o volume de seus dados. Acesso ao supercomputador \u00b6 Ap\u00f3s a obten\u00e7\u00e3o das suas credenciais, o acesso e as instru\u00e7\u00f5es de uso s\u00e3o fornecidos diretamente pelo LNCC. A submiss\u00e3o dos jobs ser\u00e1 feita atrav\u00e9s do gerenciador de recursos e filas Slurm. O manual de utiliza\u00e7\u00e3o do usu\u00e1rio est\u00e1 dispon\u00edvel na p\u00e1gina do Santos Dumont no site do LNCC .","title":"Sdu"},{"location":"processamento/sdu.html#registro-de-usuarios-para-o-acesso-ao-super-computador-projeto-eubd","text":"Envie um e-mail, seguindo o modelo abaixo para helpdesk@linea.org.br e aguarde o nosso retorno (no m\u00e1ximo em at\u00e9 72hs).: a.) No campo assunto escreva: Acesso ao supercomputador Santos Dumont - Projeto EUBD b.) No corpo do email: 1. Seu nome completo: 2. Nome da institui\u00e7\u00e3o (acr\u00f4nimo): 3. Nome do orientador (se houver): 4. E-mail do orientador/supervisor (se houver): 5. Justificativa de uso: - Escreva em no m\u00e1ximo 5 linhas qual \u00e9 o objetivo e a import\u00e2ncia de uso desses recursos computacionais. - Se poss\u00edvel, informe quantas CPU/horas pretende utilizar e tamb\u00e9m qual o volume de dados de entrada e de sa\u00edda da sua aplica\u00e7\u00e3o. - Utilize KiloBytes(KB), MegaBytes(MB) ou GigaBytes(GB) para representar o volume de seus dados.","title":"Registro de usu\u00e1rios para o acesso ao Super Computador - Projeto EUBD"},{"location":"processamento/sdu.html#acesso-ao-supercomputador","text":"Ap\u00f3s a obten\u00e7\u00e3o das suas credenciais, o acesso e as instru\u00e7\u00f5es de uso s\u00e3o fornecidos diretamente pelo LNCC. A submiss\u00e3o dos jobs ser\u00e1 feita atrav\u00e9s do gerenciador de recursos e filas Slurm. O manual de utiliza\u00e7\u00e3o do usu\u00e1rio est\u00e1 dispon\u00edvel na p\u00e1gina do Santos Dumont no site do LNCC .","title":"Acesso ao supercomputador"},{"location":"processamento/apollo/index.html","text":"HPE Apollo 2000 \u00b6 O Cluster Apollo possui 28 n\u00f3s computacionais e oferece um total de 1072 cores f\u00edsicos. Seus n\u00f3s s\u00e3o equipados com processadores Intel Xeon Skylake 5120 2.2GHz (apl01-16) e Intel Xeon Gold 5320 2.20GHz (apl17-28). O conjunto de m\u00e1quinas prov\u00ea cerca de 85 Tflops de capacidade computacional. Os 28 n\u00f3s computacionais do Cluster Apollo s\u00e3o da fam\u00edlia de servidores HPE ProLiant, sendo 16 do modelo XL170r e 12 do modelo XL220n. Atualmente, o n\u00famero de cores dispon\u00edveis \u00e9 de 2144 , pois o HT estar ativo nos n\u00f3s de computa\u00e7\u00e3o. Caracter\u00edsticas de cada servidor \u00b6 Modelo # Cores (HT) [1] RAM OS Hosts HPE Proliant XL170r 56 128 GB Rocky Linux 9.5 apl[01-16] HPE Proliant XL220n 104 256 GB Rocky Linux 9.5 apl[17-28] [1] A tecnologia Hyper-Threading (HT) est\u00e1 habilitada em todos os n\u00f3s do cluster. Caracter\u00edsticas do cluster (consolidado) # Nodes # Cores Total de ram Instalado em 16 896 1.5TB Abr-2019 12 624 3TB Jul-2023 O Cluster Apollo \u00e9 gerenciado pelo Slurm v24.05.5 . Filesystem \u00b6 O Cluster Apollo conta com um sistema de arquivos de alta performance Lustre, disponibilizado como \u00e1rea de \"Scratch\" . O \"Home\" dos usu\u00e1rios est\u00e1 acess\u00edvel apenas no n\u00f3 de login e \u00e9 fornecido atrav\u00e9s de NFS. Essas \u00e1reas de armazenamento devem ser utilizadas da seguinte forma: Scratch: Estrutura montada a partir do diret\u00f3rio /scratch/<username> . Utilizado para armazenar todos os arquivos que ser\u00e3o utilizados durante a execu\u00e7\u00e3o de um job (scripts de submiss\u00e3o, execut\u00e1veis, dados de entrada, dados de sa\u00edda etc). Vari\u00e1vel de ambiente $SCRATCH . Home: Estrutura montada a partir do diret\u00f3rio /home/<username> . Utilizado para armazenar especialmente os resultados que se queira manter durante toda a vig\u00eancia do projeto. Vari\u00e1vel de ambiente $HOME . Scriptland: Estrutura montada a partir do diret\u00f3rio /scriptland/<username> . \u00c9 uma \u00e1rea de armazenamento otimizada para armazenar scripts e c\u00f3digos. Vari\u00e1vel de ambiente $SCRIPTLAND . Clique aqui para mais detalhes Aten\u00e7\u00e3o N\u00e3o esque\u00e7a de copiar os arquivos necess\u00e1rios (execut\u00e1vel, bibliotecas, dados de entrada) para dentro da \u00e1rea de SCRATCH, pois a \u00e1rea de HOMEDIR n\u00e3o \u00e9 acess\u00edvel pelos n\u00f3s computacionais. Slurm \u00b6 Slurm \u00e9 um sistema de gerenciamento de cluster e agendamento de tarefas de c\u00f3digo aberto, tolerante a falhas e altamente escalon\u00e1vel para clusters Linux grandes e pequenos. Slurm n\u00e3o requer modifica\u00e7\u00f5es no kernel para sua opera\u00e7\u00e3o e \u00e9 relativamente independente. Como gerenciador de carga de trabalho de cluster, o Slurm tem tr\u00eas fun\u00e7\u00f5es principais: alocar acesso exclusivo e/ou n\u00e3o exclusivo aos recursos (n\u00f3s de computa\u00e7\u00e3o) aos usu\u00e1rios por um determinado per\u00edodo de tempo para que possam realizar o trabalho. oferecer uma estrutura para iniciar, executar e monitorar o trabalho (normalmente um trabalho paralelo) no conjunto de n\u00f3s alocados. gerenciar a fila de submiss\u00e3o, arbitrando conflitos entre os pedidos de recursos computacionais. Parti\u00e7\u00f5es dispon\u00edveis \u00b6 O cluster Apollo \u00e9 organizado em diferentes parti\u00e7\u00f5es (subconjunto de m\u00e1quinas) para atender a diferentes necessidades, por exemplo, a garantia da prioridade m\u00e1xima dos usu\u00e1rios do projeto LSST na utiliza\u00e7\u00e3o das m\u00e1quinas dedicadas ao IDAC-Brasil. PARTITION TIMELIMIT NODES NODELIST cpu_dev 30:00 26 apl[01-28] cpu_small 3-00:00:00 26 apl[01-28] cpu 5-00:00:00 26 apl[01-28] cpu_long 31-00:00:0 26 apl[01-28] lsst_cpu_dev 30:00 12 apl[17-28] lsst_cpu_small 3-00:00:00 12 apl[17-28] lsst_cpu 5-00:00:00 12 apl[17-28] lsst_cpu_long 10-00:00:0 12 apl[17-28] Accounts dispon\u00edveis \u00b6 Workflow \u2013 Interrompe qualquer job que esteja rodando: hpc-photoz (photoz) LSST \u2013 Pr\u00f3ximo da fila: hpc-lsst [somente nas novas apollos apl[17-28]] (lsst) Grupo A - Prioridade Maior: hpc-bpglsst (itteam, bpg-lsst) Grupo B - Prioridade Intermedi\u00e1ria: hpc-collab (des, desi, sdss, tno) Grupo C - Prioridade Menor: hpc-public (linea-members) As parti\u00e7\u00f5es ( cpu_dev , cpu_small , cpu e cpu_long ) possuem todas as apollos ( apl[01-28] ), enquanto as parti\u00e7\u00f5es do grupo LSST possuem apenas as apl[17-28] . Somente a account hpc-lsst poder\u00e1 submeter jobs nas parti\u00e7\u00f5es de prefixo \"lsst\", que possuem maior prioridade nos nodes. Aten\u00e7\u00e3o Como parte do programa de contribui\u00e7\u00e3o in-kind BRA-LIN, o IDAC Brasil possui o compromisso de gerar redshifts fotom\u00e9tricos anualmente para o levantamento LSST, sempre na \u00e9poca que antecede as libera\u00e7\u00f5es oficiais dos dados. Nestes per\u00edodos, o Cluster Apollo ser\u00e1 totalmente ocupado para este prop\u00f3sito por um tempo estimado de algumas horas, mas podendo se estender a alguns dias. Na ocasi\u00e3o, os usu\u00e1rios ser\u00e3o informados com antec\u00eancia sobre a indisponibilidade do cluster por e-mail. Clique aqui para saber mais sobre a produ\u00e7\u00e3o de medidas de redshift e o programa de conrtribui\u00e7\u00e3o in-kind BRA-LIN. Anatomia de um Job \u00b6 Um Job solicita recursos de computa\u00e7\u00e3o e especifica os aplicativos a serem iniciados nesses recursos, juntamente com quaisquer dados/op\u00e7\u00f5es de entrada e diretivas de sa\u00edda. O usu\u00e1rio envia a tarefa, geralmente na forma de um script de tarefa em lote, ao agendador em lote. O script de tarefa em lote \u00e9 composto por quatro componentes principais: O int\u00e9rprete usado para executar o script Diretivas \u201c#\u201d que transmitem op\u00e7\u00f5es de envio padr\u00e3o. A configura\u00e7\u00e3o de vari\u00e1veis de ambiente e/ou script (se necess\u00e1rio) Os aplicativos a serem executados junto com seus argumentos e op\u00e7\u00f5es de entrada. Aqui est\u00e1 um exemplo de um script em lote que solicita 3 n\u00f3s na parti\u00e7\u00e3o \"cpu\" e inicia 36 tarefas do myApp nos 3 n\u00f3s alocados: #!/bin/bash #SBATCH -N 3 #SBATCH -p cpu #SBATCH --ntasks 36 srun myApp Quando a tarefa estiver agendada para execu\u00e7\u00e3o, o gerenciador de recursos executar\u00e1 o script da tarefa em lote no primeiro n\u00f3 da aloca\u00e7\u00e3o. Especificando Recursos \u00b6 O Slurm tem sua pr\u00f3pria sintaxe para solicitar recursos de computa\u00e7\u00e3o. Abaixo est\u00e1 uma tabela de resumo de alguns recursos solicitados com frequ\u00eancia e a sintaxe de Slurm para obt\u00ea-los. Para obter uma lista completa da sintaxe, execute o comando man sbatch. Sintaxe Significado #SBATCH -p partition Define a parti\u00e7\u00e3o em que o job ser\u00e1 executado #SBATCH -J job_name Define o nome do Job #SBATCH -n quantidade Define o n\u00famero total de tarefas da CPU. #SBATCH -N quantidade Define o n\u00famero de n\u00f3s de computa\u00e7\u00e3o solicitados. Comandos B\u00e1sicos do Slurm \u00b6 Para aprender sobre todas as op\u00e7\u00f5es dispon\u00edveis para cada comando, insira man enquanto estiver conectado ao ambiente do Cluster. Comando Defini\u00e7\u00e3o sbatch Envia scripts de tarefas para a fila de execu\u00e7\u00e3o scancel Cancela um job scontrol Usado para exibir o estado Slurm (v\u00e1rias op\u00e7\u00f5es dispon\u00edveis apenas para root) sinfo Exibir estado de parti\u00e7\u00f5es e n\u00f3s squeue Exibir estado dos jobs salloc Envia um job para execu\u00e7\u00e3o ou inicia um trabalho em tempo real Ambiente de Execu\u00e7\u00e3o \u00b6 Para cada tipo de trabalho acima, o usu\u00e1rio tem a capacidade de definir o ambiente de execu\u00e7\u00e3o. Isso inclui defini\u00e7\u00f5es de vari\u00e1veis de ambiente, bem como limites de shell ( bash ulimit ou csh limit ). sbatch e salloc fornecem a op\u00e7\u00e3o --export para transmitir vari\u00e1veis de ambiente espec\u00edficas para o ambiente de execu\u00e7\u00e3o. E tamb\u00e9m tem a op\u00e7\u00e3o --propagate para transmitir limites espec\u00edficos do shell ao ambiente de execu\u00e7\u00e3o. Vari\u00e1veis de \u200b\u200bambiente \u00b6 A primeira categoria de vari\u00e1veis de ambiente s\u00e3o aquelas que o Slurm insere no ambiente de execu\u00e7\u00e3o do trabalho. Eles transmitem ao script da tarefa e informa\u00e7\u00f5es do aplicativo, como ID da tarefa (SLURM_JOB_ID) e ID da tarefa (SLURM_PROCID) . Para obter a lista completa, consulte a se\u00e7\u00e3o \"OUTPUT ENVIRONMENT VARIABLES\" nas p\u00e1ginas sbatch , salloc e srun . A pr\u00f3xima categoria de vari\u00e1veis de ambiente s\u00e3o aquelas que o usu\u00e1rio pode definir em seu ambiente para transmitir op\u00e7\u00f5es padr\u00e3o para cada trabalho enviado. Isso inclui op\u00e7\u00f5es como o limite do rel\u00f3gio de parede. Para obter a lista completa, consulte a se\u00e7\u00e3o \"INPUT ENVIRONMENT VARIABLES\" nas p\u00e1ginas sbatch , salloc e srun . Mais Informa\u00e7\u00f5es ? Saiba como utilizar o Cluster Apollo em Como Utilizar . Para maiores informa\u00e7\u00f5es, entre em contato com o Service Desk .","title":"Cluster Apollo"},{"location":"processamento/apollo/index.html#hpe-apollo-2000","text":"O Cluster Apollo possui 28 n\u00f3s computacionais e oferece um total de 1072 cores f\u00edsicos. Seus n\u00f3s s\u00e3o equipados com processadores Intel Xeon Skylake 5120 2.2GHz (apl01-16) e Intel Xeon Gold 5320 2.20GHz (apl17-28). O conjunto de m\u00e1quinas prov\u00ea cerca de 85 Tflops de capacidade computacional. Os 28 n\u00f3s computacionais do Cluster Apollo s\u00e3o da fam\u00edlia de servidores HPE ProLiant, sendo 16 do modelo XL170r e 12 do modelo XL220n. Atualmente, o n\u00famero de cores dispon\u00edveis \u00e9 de 2144 , pois o HT estar ativo nos n\u00f3s de computa\u00e7\u00e3o.","title":"HPE Apollo 2000"},{"location":"processamento/apollo/index.html#filesystem","text":"O Cluster Apollo conta com um sistema de arquivos de alta performance Lustre, disponibilizado como \u00e1rea de \"Scratch\" . O \"Home\" dos usu\u00e1rios est\u00e1 acess\u00edvel apenas no n\u00f3 de login e \u00e9 fornecido atrav\u00e9s de NFS. Essas \u00e1reas de armazenamento devem ser utilizadas da seguinte forma: Scratch: Estrutura montada a partir do diret\u00f3rio /scratch/<username> . Utilizado para armazenar todos os arquivos que ser\u00e3o utilizados durante a execu\u00e7\u00e3o de um job (scripts de submiss\u00e3o, execut\u00e1veis, dados de entrada, dados de sa\u00edda etc). Vari\u00e1vel de ambiente $SCRATCH . Home: Estrutura montada a partir do diret\u00f3rio /home/<username> . Utilizado para armazenar especialmente os resultados que se queira manter durante toda a vig\u00eancia do projeto. Vari\u00e1vel de ambiente $HOME . Scriptland: Estrutura montada a partir do diret\u00f3rio /scriptland/<username> . \u00c9 uma \u00e1rea de armazenamento otimizada para armazenar scripts e c\u00f3digos. Vari\u00e1vel de ambiente $SCRIPTLAND . Clique aqui para mais detalhes Aten\u00e7\u00e3o N\u00e3o esque\u00e7a de copiar os arquivos necess\u00e1rios (execut\u00e1vel, bibliotecas, dados de entrada) para dentro da \u00e1rea de SCRATCH, pois a \u00e1rea de HOMEDIR n\u00e3o \u00e9 acess\u00edvel pelos n\u00f3s computacionais.","title":"Filesystem"},{"location":"processamento/apollo/index.html#slurm","text":"Slurm \u00e9 um sistema de gerenciamento de cluster e agendamento de tarefas de c\u00f3digo aberto, tolerante a falhas e altamente escalon\u00e1vel para clusters Linux grandes e pequenos. Slurm n\u00e3o requer modifica\u00e7\u00f5es no kernel para sua opera\u00e7\u00e3o e \u00e9 relativamente independente. Como gerenciador de carga de trabalho de cluster, o Slurm tem tr\u00eas fun\u00e7\u00f5es principais: alocar acesso exclusivo e/ou n\u00e3o exclusivo aos recursos (n\u00f3s de computa\u00e7\u00e3o) aos usu\u00e1rios por um determinado per\u00edodo de tempo para que possam realizar o trabalho. oferecer uma estrutura para iniciar, executar e monitorar o trabalho (normalmente um trabalho paralelo) no conjunto de n\u00f3s alocados. gerenciar a fila de submiss\u00e3o, arbitrando conflitos entre os pedidos de recursos computacionais.","title":"Slurm"},{"location":"processamento/apollo/index.html#particoes-disponiveis","text":"O cluster Apollo \u00e9 organizado em diferentes parti\u00e7\u00f5es (subconjunto de m\u00e1quinas) para atender a diferentes necessidades, por exemplo, a garantia da prioridade m\u00e1xima dos usu\u00e1rios do projeto LSST na utiliza\u00e7\u00e3o das m\u00e1quinas dedicadas ao IDAC-Brasil. PARTITION TIMELIMIT NODES NODELIST cpu_dev 30:00 26 apl[01-28] cpu_small 3-00:00:00 26 apl[01-28] cpu 5-00:00:00 26 apl[01-28] cpu_long 31-00:00:0 26 apl[01-28] lsst_cpu_dev 30:00 12 apl[17-28] lsst_cpu_small 3-00:00:00 12 apl[17-28] lsst_cpu 5-00:00:00 12 apl[17-28] lsst_cpu_long 10-00:00:0 12 apl[17-28]","title":"Parti\u00e7\u00f5es dispon\u00edveis"},{"location":"processamento/apollo/index.html#accounts-disponiveis","text":"Workflow \u2013 Interrompe qualquer job que esteja rodando: hpc-photoz (photoz) LSST \u2013 Pr\u00f3ximo da fila: hpc-lsst [somente nas novas apollos apl[17-28]] (lsst) Grupo A - Prioridade Maior: hpc-bpglsst (itteam, bpg-lsst) Grupo B - Prioridade Intermedi\u00e1ria: hpc-collab (des, desi, sdss, tno) Grupo C - Prioridade Menor: hpc-public (linea-members) As parti\u00e7\u00f5es ( cpu_dev , cpu_small , cpu e cpu_long ) possuem todas as apollos ( apl[01-28] ), enquanto as parti\u00e7\u00f5es do grupo LSST possuem apenas as apl[17-28] . Somente a account hpc-lsst poder\u00e1 submeter jobs nas parti\u00e7\u00f5es de prefixo \"lsst\", que possuem maior prioridade nos nodes. Aten\u00e7\u00e3o Como parte do programa de contribui\u00e7\u00e3o in-kind BRA-LIN, o IDAC Brasil possui o compromisso de gerar redshifts fotom\u00e9tricos anualmente para o levantamento LSST, sempre na \u00e9poca que antecede as libera\u00e7\u00f5es oficiais dos dados. Nestes per\u00edodos, o Cluster Apollo ser\u00e1 totalmente ocupado para este prop\u00f3sito por um tempo estimado de algumas horas, mas podendo se estender a alguns dias. Na ocasi\u00e3o, os usu\u00e1rios ser\u00e3o informados com antec\u00eancia sobre a indisponibilidade do cluster por e-mail. Clique aqui para saber mais sobre a produ\u00e7\u00e3o de medidas de redshift e o programa de conrtribui\u00e7\u00e3o in-kind BRA-LIN.","title":"Accounts dispon\u00edveis"},{"location":"processamento/apollo/index.html#anatomia-de-um-job","text":"Um Job solicita recursos de computa\u00e7\u00e3o e especifica os aplicativos a serem iniciados nesses recursos, juntamente com quaisquer dados/op\u00e7\u00f5es de entrada e diretivas de sa\u00edda. O usu\u00e1rio envia a tarefa, geralmente na forma de um script de tarefa em lote, ao agendador em lote. O script de tarefa em lote \u00e9 composto por quatro componentes principais: O int\u00e9rprete usado para executar o script Diretivas \u201c#\u201d que transmitem op\u00e7\u00f5es de envio padr\u00e3o. A configura\u00e7\u00e3o de vari\u00e1veis de ambiente e/ou script (se necess\u00e1rio) Os aplicativos a serem executados junto com seus argumentos e op\u00e7\u00f5es de entrada. Aqui est\u00e1 um exemplo de um script em lote que solicita 3 n\u00f3s na parti\u00e7\u00e3o \"cpu\" e inicia 36 tarefas do myApp nos 3 n\u00f3s alocados: #!/bin/bash #SBATCH -N 3 #SBATCH -p cpu #SBATCH --ntasks 36 srun myApp Quando a tarefa estiver agendada para execu\u00e7\u00e3o, o gerenciador de recursos executar\u00e1 o script da tarefa em lote no primeiro n\u00f3 da aloca\u00e7\u00e3o.","title":"Anatomia de um Job"},{"location":"processamento/uso/howtouse-HPC.html","text":"Como Utilizar \u00b6 Como acessar \u00b6 O acesso ao nosso cluster pode ser feito, atrav\u00e9s do Open OnDemand ou pelo do Terminal do JupyterLab (K8S) . Em ambas op\u00e7\u00f5es, \u00e9 imprescind\u00edvel possuir uma conta v\u00e1lida no ambiente computacional do LIneA. Caso n\u00e3o possua uma conta, entre em contato com o Service Desk por email ( helpdesk@linea.org.br ) para mais informa\u00e7\u00f5es. Aten\u00e7\u00e3o Mesmo possuindo uma conta ativa no LIneA, o acesso ao ambiente de processamento HPC n\u00e3o \u00e9 autom\u00e1tico. Para mais informa\u00e7\u00f5es entre em contato com o Service Desk pelo email helpdesk@linea.org.br . Acessando pelo terminal do JupyterLab Na tela inicial do seu Jupyter Notebook, na se\u00e7\u00e3o \"Other\" , voc\u00ea encontrar\u00e1 o bot\u00e3o do terminal. Ao clicar nele, voc\u00ea ser\u00e1 redirecionado para um terminal Linux, inicialmente localizado em seu diret\u00f3rio home . Para acessar o Cluster Apollo, basta executar o seguinte comando: ssh loginapl01 A m\u00e1quina loginapl01 \u00e9 onde voc\u00ea poder\u00e1 fazer a aloca\u00e7\u00e3o do n\u00f3 de computa\u00e7\u00e3o para submeter o seu job. $HOME e $SCRATCH Os n\u00f3s de computa\u00e7\u00e3o n\u00e3o possuem acesso ao seu diret\u00f3rio home de usu\u00e1rio. Mova ou copie, para seu diret\u00f3rio SCRATCH , todos os arquivos necess\u00e1rios para a submiss\u00e3o do seu job. Como usar a \u00e1rea de Scratch \u00b6 Seu diret\u00f3rio SCRATCH \u00e9 o local para onde voc\u00ea pode direcionar os arquivos resultados do seu job, assim como armazenar temporariamente dados que s\u00e3o utilizados pelo c\u00f3digo no momento do processamento. Para acessar o seu diret\u00f3rio SCRATCH: cd $SCRATCH Para enviar arquivos para seu diret\u00f3rio SCRATCH: cp <ARQUIVO> $SCRATCH Limpeza Autom\u00e1tica do Scratch \u00b6 O scratch \u00e9 uma \u00e1rea de armazenamento tempor\u00e1ria destinada \u00e0 arquivos de sa\u00edda e processamento dos jobs executados no cluster. Para manter o ambiente organizado e garantir espa\u00e7o dispon\u00edvel para todos, est\u00e1 em vigor um script de limpeza autom\u00e1tica , que \u00e9 executado uma vez por semana . Esse processo remove arquivos que n\u00e3o foram acessados dentro do per\u00edodo de reten\u00e7\u00e3o definido - atualmente, 45 dias . Arquivos de configura\u00e7\u00e3o essenciais (ex.: .bashrc , .bash_profile , .ssh , etc.) s\u00e3o preservados automaticamente e n\u00e3o entram no processo de exclus\u00e3o . ATEN\u00c7\u00c3O O scratch n\u00e3o deve ser usado para armazenamento permanente. Recomendamos mover dados importantes para seu diret\u00f3rio home . Como usar a \u00e1rea de Scripts \u00b6 Seu diret\u00f3rio SCRIPTS \u00e9 o local para onde voc\u00ea pode armazenar scripts, c\u00f3digos para serem executados no cluster. Recomenda-se tamb\u00e9m utilizar essa \u00e1rea para cria\u00e7\u00e3o de environments Conda. Para acessar o seu diret\u00f3rio SCRIPTS: cd $SCRIPTS Fique Atento A \u00e1rea de scripts n\u00e3o est\u00e1 inclu\u00edda na rotina de backups. Por isso, n\u00e3o deve ser utilizada como armazenamento permanente de dados. Como Submeter um Job \u00b6 Um Job solicita recursos de computa\u00e7\u00e3o e especifica os aplicativos a serem iniciados nesses recursos, juntamente com quaisquer dados/op\u00e7\u00f5es de entrada e diretivas de sa\u00edda. O gerenciamento e agendamento das tarefas e recursos do cluster \u00e9 feito atrav\u00e9s do Slurm. Logo, para submeter um Job \u00e9 necess\u00e1rio utilizar um script como abaixo: #!/bin/bash #SBATCH -p PARTITION #Name of the Partition to use #SBATCH --nodelist=NODE #Name of the Node to be allocated #SBATCH -J simple-job #Job name #----------------------------------------------------------------------------# ##path to executable code EXEC = /scripts/YOUR.USER/EXECUTABLE.CODE srun $EXEC Nesse script \u00e9 preciso especificar: o nome da fila (Partition) que ser\u00e1 usada; o nome do n\u00f3 que ser\u00e1 alocado para a excecu\u00e7\u00e3o do Job; e o caminho para o c\u00f3digo/programa a ser executado. ATEN\u00c7\u00c3O \u00c9 expressamente proibida a submiss\u00e3o de jobs diretamente para m\u00e1quina loginapl01 . Qualquer c\u00f3digo em execu\u00e7\u00e3o nessa m\u00e1quina ser\u00e1 interrompido imediatamente, sem aviso pr\u00e9vio. Para submeter o Job: sbatch script-submit-job.sh Se o script estiver correto haver\u00e1 uma sa\u00edda que indica o ID do job . Para verificar o andamento e informa\u00e7\u00f5es do Job: scontrol show job <ID> Para cancelar o Job: scancel <ID> Acesso \u00e0 internet Os n\u00f3s de computa\u00e7\u00e3o n\u00e3o t\u00eam acesso \u00e0 internet. Pacotes e bibliotecas devem ser instalados a partir da loginapl01 em sua \u00e1rea de scripts . Gerenciador de pacotes EUPS \u00b6 O EUPS \u00e9 um gerenciador de pacotes alternativo (e oficial do LSST) que permite carregar vari\u00e1veis de ambiente e incluir o caminho para programas e bibliotecas de forma modular. Para carregar o EUPS: Info Atualmente o EUPS \u00e9 carregado automaticamente ap\u00f3s o usu\u00e1rio acessar qualquer m\u00e1quina do cluster apollo. source /opt/eups/bin/setups.sh Para listar todos os pacotes dispon\u00edveis: eups list Para listar um pacotes espec\u00edfico: eups list <PACOTE> Para carregar um pacotes na sess\u00e3o atual: setup <NOME DO PACOTE> <VERS\u00c3O DO PACOTE> Para remover o pacote carregado: unsetup <NOME DO PACOTE> <VERS\u00c3O DO PACOTE> Comandos \u00fateis do Slurm \u00b6 Para aprender sobre todas as op\u00e7\u00f5es dispon\u00edveis para cada comando, insira man <comando> enquanto estiver conectado ao ambiente do Cluster. Comando Defini\u00e7\u00e3o sbatch Envia scripts de tarefas para a fila de execu\u00e7\u00e3o squeue Exibir estado dos jobs scontrol Usado para exibir o estado Slurm (v\u00e1rias op\u00e7\u00f5es dispon\u00edveis apenas para root) sinfo Exibir estado de parti\u00e7\u00f5es e n\u00f3s salloc Envia um job para execu\u00e7\u00e3o ou inicia um trabalho em tempo real V\u00eddeos tutoriais \u00b6 How to login How to use EUPS How to access the Scratch How to submit a Job","title":"Como utilizar"},{"location":"processamento/uso/howtouse-HPC.html#como-utilizar","text":"","title":"Como Utilizar"},{"location":"processamento/uso/howtouse-HPC.html#como-acessar","text":"O acesso ao nosso cluster pode ser feito, atrav\u00e9s do Open OnDemand ou pelo do Terminal do JupyterLab (K8S) . Em ambas op\u00e7\u00f5es, \u00e9 imprescind\u00edvel possuir uma conta v\u00e1lida no ambiente computacional do LIneA. Caso n\u00e3o possua uma conta, entre em contato com o Service Desk por email ( helpdesk@linea.org.br ) para mais informa\u00e7\u00f5es. Aten\u00e7\u00e3o Mesmo possuindo uma conta ativa no LIneA, o acesso ao ambiente de processamento HPC n\u00e3o \u00e9 autom\u00e1tico. Para mais informa\u00e7\u00f5es entre em contato com o Service Desk pelo email helpdesk@linea.org.br . Acessando pelo terminal do JupyterLab Na tela inicial do seu Jupyter Notebook, na se\u00e7\u00e3o \"Other\" , voc\u00ea encontrar\u00e1 o bot\u00e3o do terminal. Ao clicar nele, voc\u00ea ser\u00e1 redirecionado para um terminal Linux, inicialmente localizado em seu diret\u00f3rio home . Para acessar o Cluster Apollo, basta executar o seguinte comando: ssh loginapl01 A m\u00e1quina loginapl01 \u00e9 onde voc\u00ea poder\u00e1 fazer a aloca\u00e7\u00e3o do n\u00f3 de computa\u00e7\u00e3o para submeter o seu job. $HOME e $SCRATCH Os n\u00f3s de computa\u00e7\u00e3o n\u00e3o possuem acesso ao seu diret\u00f3rio home de usu\u00e1rio. Mova ou copie, para seu diret\u00f3rio SCRATCH , todos os arquivos necess\u00e1rios para a submiss\u00e3o do seu job.","title":"Como acessar"},{"location":"processamento/uso/howtouse-HPC.html#como-usar-a-area-de-scratch","text":"Seu diret\u00f3rio SCRATCH \u00e9 o local para onde voc\u00ea pode direcionar os arquivos resultados do seu job, assim como armazenar temporariamente dados que s\u00e3o utilizados pelo c\u00f3digo no momento do processamento. Para acessar o seu diret\u00f3rio SCRATCH: cd $SCRATCH Para enviar arquivos para seu diret\u00f3rio SCRATCH: cp <ARQUIVO> $SCRATCH","title":"Como usar a \u00e1rea de Scratch"},{"location":"processamento/uso/howtouse-HPC.html#como-usar-a-area-de-scripts","text":"Seu diret\u00f3rio SCRIPTS \u00e9 o local para onde voc\u00ea pode armazenar scripts, c\u00f3digos para serem executados no cluster. Recomenda-se tamb\u00e9m utilizar essa \u00e1rea para cria\u00e7\u00e3o de environments Conda. Para acessar o seu diret\u00f3rio SCRIPTS: cd $SCRIPTS Fique Atento A \u00e1rea de scripts n\u00e3o est\u00e1 inclu\u00edda na rotina de backups. Por isso, n\u00e3o deve ser utilizada como armazenamento permanente de dados.","title":"Como usar a \u00e1rea de Scripts"},{"location":"processamento/uso/howtouse-HPC.html#como-submeter-um-job","text":"Um Job solicita recursos de computa\u00e7\u00e3o e especifica os aplicativos a serem iniciados nesses recursos, juntamente com quaisquer dados/op\u00e7\u00f5es de entrada e diretivas de sa\u00edda. O gerenciamento e agendamento das tarefas e recursos do cluster \u00e9 feito atrav\u00e9s do Slurm. Logo, para submeter um Job \u00e9 necess\u00e1rio utilizar um script como abaixo: #!/bin/bash #SBATCH -p PARTITION #Name of the Partition to use #SBATCH --nodelist=NODE #Name of the Node to be allocated #SBATCH -J simple-job #Job name #----------------------------------------------------------------------------# ##path to executable code EXEC = /scripts/YOUR.USER/EXECUTABLE.CODE srun $EXEC Nesse script \u00e9 preciso especificar: o nome da fila (Partition) que ser\u00e1 usada; o nome do n\u00f3 que ser\u00e1 alocado para a excecu\u00e7\u00e3o do Job; e o caminho para o c\u00f3digo/programa a ser executado. ATEN\u00c7\u00c3O \u00c9 expressamente proibida a submiss\u00e3o de jobs diretamente para m\u00e1quina loginapl01 . Qualquer c\u00f3digo em execu\u00e7\u00e3o nessa m\u00e1quina ser\u00e1 interrompido imediatamente, sem aviso pr\u00e9vio. Para submeter o Job: sbatch script-submit-job.sh Se o script estiver correto haver\u00e1 uma sa\u00edda que indica o ID do job . Para verificar o andamento e informa\u00e7\u00f5es do Job: scontrol show job <ID> Para cancelar o Job: scancel <ID> Acesso \u00e0 internet Os n\u00f3s de computa\u00e7\u00e3o n\u00e3o t\u00eam acesso \u00e0 internet. Pacotes e bibliotecas devem ser instalados a partir da loginapl01 em sua \u00e1rea de scripts .","title":"Como Submeter um Job"},{"location":"processamento/uso/howtouse-HPC.html#gerenciador-de-pacotes-eups","text":"O EUPS \u00e9 um gerenciador de pacotes alternativo (e oficial do LSST) que permite carregar vari\u00e1veis de ambiente e incluir o caminho para programas e bibliotecas de forma modular. Para carregar o EUPS: Info Atualmente o EUPS \u00e9 carregado automaticamente ap\u00f3s o usu\u00e1rio acessar qualquer m\u00e1quina do cluster apollo. source /opt/eups/bin/setups.sh Para listar todos os pacotes dispon\u00edveis: eups list Para listar um pacotes espec\u00edfico: eups list <PACOTE> Para carregar um pacotes na sess\u00e3o atual: setup <NOME DO PACOTE> <VERS\u00c3O DO PACOTE> Para remover o pacote carregado: unsetup <NOME DO PACOTE> <VERS\u00c3O DO PACOTE>","title":"Gerenciador de pacotes EUPS"},{"location":"processamento/uso/howtouse-HPC.html#comandos-uteis-do-slurm","text":"Para aprender sobre todas as op\u00e7\u00f5es dispon\u00edveis para cada comando, insira man <comando> enquanto estiver conectado ao ambiente do Cluster. Comando Defini\u00e7\u00e3o sbatch Envia scripts de tarefas para a fila de execu\u00e7\u00e3o squeue Exibir estado dos jobs scontrol Usado para exibir o estado Slurm (v\u00e1rias op\u00e7\u00f5es dispon\u00edveis apenas para root) sinfo Exibir estado de parti\u00e7\u00f5es e n\u00f3s salloc Envia um job para execu\u00e7\u00e3o ou inicia um trabalho em tempo real","title":"Comandos \u00fateis do Slurm"},{"location":"processamento/uso/howtouse-HPC.html#videos-tutoriais","text":"How to login How to use EUPS How to access the Scratch How to submit a Job","title":"V\u00eddeos tutoriais"},{"location":"processamento/uso/openondemand.html","text":"Open OnDemand \u00b6 O Open Ondemand \u00e9 uma interface que facilita a utiliza\u00e7\u00e3o do nosso ambiente de HPC constitu\u00eddo pelo Cluster Apollo. Para acessar \u00e9 necess\u00e1rio possuir uma conta v\u00e1lida no LIneA ( saiba mais ). O acesso ao Open Ondemand \u00e9 atrav\u00e9s de https://ondemand.linea.org.br/ . Na tela inicial da plataforma, na parte superior, \u00e9 poss\u00edvel visualizar um menu com os seguinte itens: Files - fornece uma interface para o seu diret\u00f3rio de usu\u00e1rio ( Home Directory ). Jobs - fornece uma interface para as telas \u201cActive Jobs\u201d e \u201cJob Composer\u201d. Clusters - fornece um acesso para o terminal em um navegador da web. Interactive Apps - fornece acesso ao Jupyter Notebook. Home Directory \u00b6 O Home Directory possibilita a visualiza\u00e7\u00e3o do diret\u00f3rio de usu\u00e1rio, onde est\u00e3o armazenados seus arquivos, al\u00e9m de exibir uma variedade de bot\u00f5es com diferentes funcionalidades. Mudando de Diret\u00f3rio \u00b6 Clicar no bot\u00e3o Change Directory permite que voc\u00ea mude de diret\u00f3rio dentro da nossa infraestrutura. Para isso, basta escrever o caminho no campo Path e em seguida apertar \" ok \". Acessando o Terminal \u00b6 Clicar no bot\u00e3o Open Terminal o levar\u00e1 ao terminal linux dentro da m\u00e1quina de login (loginapl01) do Cluster Apollo. Neste terminal, voc\u00ea se encontra no seu diret\u00f3rio \" Home \" de usu\u00e1rio e tem a capacidade de visualizar seus arquivos. No terminal tamb\u00e9m \u00e9 poss\u00edvel executar todas as opera\u00e7\u00f5es usuais de usu\u00e1rio HPC, como por exemplo, alocar um n\u00f3 de computa\u00e7\u00e3o e verificar a fila Slurm usando comandos. Criando, Transferindo e Movendo Arquivos \u00b6 No Open OnDemand a cria\u00e7\u00e3o de novos arquivos e diret\u00f3rios \u00e9 bem simples, bem como a transfer\u00eancia deles. Clique nos bot\u00f5es \"New File\" ou \"New Directory\" e escolha o nome que deseja para o novo arquivo ou diret\u00f3rio. O bot\u00e3o \"Upload\" permite que voc\u00ea transfira arquivos da sua m\u00e1quina para o seu diret\u00f3rio home em nosso ambiente; O bot\u00e3o \"Download\" possibilita enviar os arquivos que foram selecionados para a sua m\u00e1quina local. Para visualizar, renomear ou editar o novo arquivo criado, clique nos \"tr\u00eas pontinhos\" que aparecem ao lado direito do arquivo e voc\u00ea ver\u00e1 um menu com essas op\u00e7\u00f5es. Para mover ou copiar arquivos \u00e9 preciso seguir os passos: Selecionar o arquivo que deseja; Clicar no bot\u00e3o \"Copy/Move\" ; Clicar em \"Change Directory\" e escrever o caminho do diret\u00f3rio para onde deseja copiar ou mover o arquivo; Clicar em \"Copy\" ou \"Move\" na caixa que aparece no canto esquerdo da tela. Jobs \u00b6 Na se\u00e7\u00e3o Jobs do menu inicial, encontram-se as op\u00e7\u00f5es \"Job Composer\" e \"Active Jobs\". A tela \"Job Composer\" facilita o processo de submiss\u00e3o de jobs e em \"Active Jobs\" voc\u00ea pode acompanhar a execu\u00e7\u00e3o do seu Job com detalhes. Para submeter um job \u00e9 necess\u00e1rio utilizar um script de submiss\u00e3o como este descrito abaixo: ( saiba mais ) #!/bin/bash #SBATCH -p PARTITION #Name of the Partition to use #SBATCH --nodelist=NODE #Name of the Node to be allocated #SBATCH -J simple-job #Job name #----------------------------------------------------------------------------# ##path to executable code EXEC = /scratch/users/YOUR.USER/ondemand/projects/EXECUTABLE.CODE srun $EXEC Para visualizar mais templates de script de submiss\u00e3o de Jobs, clique aqui Job Composer \u00b6 O Open OnDemand facilita todo o processo de submiss\u00e3o de jobs atrav\u00e9s da ferramenta Job Composer . Para isto basta seguir os seguintes passos: Clicar no bot\u00e3o \"New Job\" ; Escolher a op\u00e7\u00e3o \"Default Template\" ; Editar as especifica\u00e7\u00f5es do script de submiss\u00e3o em \"Open Editor\" ; Clicar em \"Submit\" para que o Job entre em execu\u00e7\u00e3o. Aviso Importante Os n\u00f3s de computa\u00e7\u00e3o do cluster n\u00e3o possuem acesso ao seu diret\u00f3rio de usu\u00e1rio (Home Directory). Mova ou copie, para seu diret\u00f3rio SCRATCH, todos os arquivos necess\u00e1rios para a submiss\u00e3o do seu job. JupyterLab \u00b6 Com o Open OnDemand, \u00e9 vi\u00e1vel acessar o Jupyter Notebook em nosso ambiente de HPC. Por meio de \"Interactive Apps\", o Jupyter Notebook iniciar\u00e1 uma sess\u00e3o em um dos n\u00f3s de computa\u00e7\u00e3o do cluster, bastando para isso: Clicar em \"Jupyter Notebook\" ; Em seguida preencher os campos \"Account\", \"Partition\" e \"Select node\"; Depois pressionar o bot\u00e3o \"Launch\" . Para conectar-se ao JupyterLab , clicando em \"Connect to Jupyter\" . Jupyter no Kubernetes VS Jupyter no HPC N\u00f3s possu\u00edmos dois ambientes Jupyter em duas infraestruturas distintas. Um deles est\u00e1 dispon\u00edvel em https://jupyter.linea.org.br e \u00e9 executado sobre Kubernetes . O outro, mencionado acima, \u00e9 executado de forma interativa pela plataforma Open OnDemand diretamente nos n\u00f3s de processamento. Ao abrir um terminal dentro do JupyterLab via Open OnDemand, voc\u00ea poder\u00e1 verificar que est\u00e1 localizado em um n\u00f3 do Cluster Apollo e tem acesso \u00e0 sua \u00e1rea \" scratch \" no storage de armazenamento Lustre. Criando Kernels Python \u00b6 Os comandos a seguir devem ser executados no terminal em \"LIneA Shell Access\" . Para acessar: Na pagina inicial do Open OnDemand, clique em: Clusters -> LIneA Shell Access . Siga os comandos abaixo: V\u00e1 para sua \u00e1rea SCRATCH, Instale e carregue o Miniconda: cd $SCRATCH curl -L -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod +x Miniconda3-latest-Linux-x86_64.sh ./Miniconda3-latest-Linux-x86_64.sh -p $SCRATCH /miniconda source miniconda/bin/activate conda deactivate #Necess\u00e1rio para desativar o env \"base\" Crie, ative um venv conda e instale o ipykernel : conda create -p $SCRATCH /kernelname conda activate kernelname/ conda install -c anaconda ipykernel Configure o JUPYTER_PATH (obrigat\u00f3rio ser esse caminho abaixo): JUPYTER_PATH = $SCRATCH /.local echo $JUPYTER_PATH python -m ipykernel install --prefix = $JUPYTER_PATH --name 'kernelname' Abra uma sess\u00e3o do Jupyter Notebook. O output do \u00faltimo comando deve ser: #[InstallIPythonKernelSpecApp] WARNING | Installing to /scratch/users/YOUR.USER/.local/share/jupyter/kernels, which is not in ['/lustre/t0/scratch/users/YOUR.USER/kernelname/share/jupyter/kernels', '/home/YOUR.USER/.local/share/jupyter/kernels', '/usr/local/share/jupyter/kernels', '/usr/share/jupyter/kernels', '/home/YOUR.USER/.ipython/kernels']. The kernelspec may not be found. Installed kernelspec kernelname in /lustre/t0/scratch/users/YOUR.USER/.local/share/jupyter/kernels/kernelname Ao final dessa execu\u00e7\u00e3o de comandos, ser\u00e1 poss\u00edvel ver o bot\u00e3o do kernel python criado. V\u00eddeos tutoriais \u00b6 Como submeter um Job Como acessar o Jupyter Notebook Qualquer d\u00favida, entre em contato com o Service Desk .","title":"Open OnDemand"},{"location":"processamento/uso/openondemand.html#open-ondemand","text":"O Open Ondemand \u00e9 uma interface que facilita a utiliza\u00e7\u00e3o do nosso ambiente de HPC constitu\u00eddo pelo Cluster Apollo. Para acessar \u00e9 necess\u00e1rio possuir uma conta v\u00e1lida no LIneA ( saiba mais ). O acesso ao Open Ondemand \u00e9 atrav\u00e9s de https://ondemand.linea.org.br/ . Na tela inicial da plataforma, na parte superior, \u00e9 poss\u00edvel visualizar um menu com os seguinte itens: Files - fornece uma interface para o seu diret\u00f3rio de usu\u00e1rio ( Home Directory ). Jobs - fornece uma interface para as telas \u201cActive Jobs\u201d e \u201cJob Composer\u201d. Clusters - fornece um acesso para o terminal em um navegador da web. Interactive Apps - fornece acesso ao Jupyter Notebook.","title":"Open OnDemand"},{"location":"processamento/uso/openondemand.html#home-directory","text":"O Home Directory possibilita a visualiza\u00e7\u00e3o do diret\u00f3rio de usu\u00e1rio, onde est\u00e3o armazenados seus arquivos, al\u00e9m de exibir uma variedade de bot\u00f5es com diferentes funcionalidades.","title":"Home Directory"},{"location":"processamento/uso/openondemand.html#jobs","text":"Na se\u00e7\u00e3o Jobs do menu inicial, encontram-se as op\u00e7\u00f5es \"Job Composer\" e \"Active Jobs\". A tela \"Job Composer\" facilita o processo de submiss\u00e3o de jobs e em \"Active Jobs\" voc\u00ea pode acompanhar a execu\u00e7\u00e3o do seu Job com detalhes. Para submeter um job \u00e9 necess\u00e1rio utilizar um script de submiss\u00e3o como este descrito abaixo: ( saiba mais ) #!/bin/bash #SBATCH -p PARTITION #Name of the Partition to use #SBATCH --nodelist=NODE #Name of the Node to be allocated #SBATCH -J simple-job #Job name #----------------------------------------------------------------------------# ##path to executable code EXEC = /scratch/users/YOUR.USER/ondemand/projects/EXECUTABLE.CODE srun $EXEC Para visualizar mais templates de script de submiss\u00e3o de Jobs, clique aqui","title":"Jobs"},{"location":"processamento/uso/openondemand.html#job-composer","text":"O Open OnDemand facilita todo o processo de submiss\u00e3o de jobs atrav\u00e9s da ferramenta Job Composer . Para isto basta seguir os seguintes passos: Clicar no bot\u00e3o \"New Job\" ; Escolher a op\u00e7\u00e3o \"Default Template\" ; Editar as especifica\u00e7\u00f5es do script de submiss\u00e3o em \"Open Editor\" ; Clicar em \"Submit\" para que o Job entre em execu\u00e7\u00e3o. Aviso Importante Os n\u00f3s de computa\u00e7\u00e3o do cluster n\u00e3o possuem acesso ao seu diret\u00f3rio de usu\u00e1rio (Home Directory). Mova ou copie, para seu diret\u00f3rio SCRATCH, todos os arquivos necess\u00e1rios para a submiss\u00e3o do seu job.","title":"Job Composer"},{"location":"processamento/uso/openondemand.html#jupyterlab","text":"Com o Open OnDemand, \u00e9 vi\u00e1vel acessar o Jupyter Notebook em nosso ambiente de HPC. Por meio de \"Interactive Apps\", o Jupyter Notebook iniciar\u00e1 uma sess\u00e3o em um dos n\u00f3s de computa\u00e7\u00e3o do cluster, bastando para isso: Clicar em \"Jupyter Notebook\" ; Em seguida preencher os campos \"Account\", \"Partition\" e \"Select node\"; Depois pressionar o bot\u00e3o \"Launch\" . Para conectar-se ao JupyterLab , clicando em \"Connect to Jupyter\" . Jupyter no Kubernetes VS Jupyter no HPC N\u00f3s possu\u00edmos dois ambientes Jupyter em duas infraestruturas distintas. Um deles est\u00e1 dispon\u00edvel em https://jupyter.linea.org.br e \u00e9 executado sobre Kubernetes . O outro, mencionado acima, \u00e9 executado de forma interativa pela plataforma Open OnDemand diretamente nos n\u00f3s de processamento. Ao abrir um terminal dentro do JupyterLab via Open OnDemand, voc\u00ea poder\u00e1 verificar que est\u00e1 localizado em um n\u00f3 do Cluster Apollo e tem acesso \u00e0 sua \u00e1rea \" scratch \" no storage de armazenamento Lustre.","title":"JupyterLab"},{"location":"processamento/uso/openondemand.html#criando-kernels-python","text":"Os comandos a seguir devem ser executados no terminal em \"LIneA Shell Access\" . Para acessar: Na pagina inicial do Open OnDemand, clique em: Clusters -> LIneA Shell Access . Siga os comandos abaixo: V\u00e1 para sua \u00e1rea SCRATCH, Instale e carregue o Miniconda: cd $SCRATCH curl -L -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod +x Miniconda3-latest-Linux-x86_64.sh ./Miniconda3-latest-Linux-x86_64.sh -p $SCRATCH /miniconda source miniconda/bin/activate conda deactivate #Necess\u00e1rio para desativar o env \"base\" Crie, ative um venv conda e instale o ipykernel : conda create -p $SCRATCH /kernelname conda activate kernelname/ conda install -c anaconda ipykernel Configure o JUPYTER_PATH (obrigat\u00f3rio ser esse caminho abaixo): JUPYTER_PATH = $SCRATCH /.local echo $JUPYTER_PATH python -m ipykernel install --prefix = $JUPYTER_PATH --name 'kernelname' Abra uma sess\u00e3o do Jupyter Notebook. O output do \u00faltimo comando deve ser: #[InstallIPythonKernelSpecApp] WARNING | Installing to /scratch/users/YOUR.USER/.local/share/jupyter/kernels, which is not in ['/lustre/t0/scratch/users/YOUR.USER/kernelname/share/jupyter/kernels', '/home/YOUR.USER/.local/share/jupyter/kernels', '/usr/local/share/jupyter/kernels', '/usr/share/jupyter/kernels', '/home/YOUR.USER/.ipython/kernels']. The kernelspec may not be found. Installed kernelspec kernelname in /lustre/t0/scratch/users/YOUR.USER/.local/share/jupyter/kernels/kernelname Ao final dessa execu\u00e7\u00e3o de comandos, ser\u00e1 poss\u00edvel ver o bot\u00e3o do kernel python criado.","title":"Criando Kernels Python"},{"location":"processamento/uso/openondemand.html#videos-tutoriais","text":"Como submeter um Job Como acessar o Jupyter Notebook Qualquer d\u00favida, entre em contato com o Service Desk .","title":"V\u00eddeos tutoriais"},{"location":"processamento/uso/politicadeuso.html","text":"Pol\u00edtica de Uso \u00b6 Pol\u00edtica de Uso da Infraestrutura Computacional do Laborat\u00f3rio Interinstitucional de e-Astronomia - LIneA. Esta Pol\u00edtica de Uso da Infraestrutura Computacional do Laborat\u00f3rio Interinstitucional de e-Astronomia abrange todos os recursos computacionais que s\u00e3o disponibilizados \u00e0 pesquisa cient\u00edfica. Esse conjunto de regras visam: Garantir um ambiente computacional amig\u00e1vel, produtivo e confi\u00e1vel; Manter um ambiente que permita o desenvolvimento e a inova\u00e7\u00e3o; Zelar pela reputa\u00e7\u00e3o do Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA) e proteger os seus sistemas de computa\u00e7\u00e3o e os dados neles contidos contra ataques externos e uso n\u00e3o autorizado; Garantir a conformidade com todas as diretrizes legais. Todos os usu\u00e1rios devem procurar manter a seguran\u00e7a do ambiente. A conta do usu\u00e1rio \u00e9 de uso exclusivo, pessoal e n\u00e3o poder\u00e1 ser compartilhada. Tentativas n\u00e3o autorizadas de acesso com o intuito de danificar, alterar, falsificar ou excluir dados, falsificar o endere\u00e7o de e-mail ou causar interrup\u00e7\u00f5es de servi\u00e7os est\u00e3o proibidas. Os recursos computacionais do LIneA devem ser utilizados \u00fanica e exclusivamente para a execu\u00e7\u00e3o de tarefas relacionadas a projetos cient\u00edficos previamente registrados. \u00c9 expressamente proibido o uso dos referidos recursos para: Atividades ilegais de qualquer natureza; Atividades que ofendam ou resultem em constrangimento p\u00fablico de colaboradores, usu\u00e1rios ou terceiros; Atividades que envolvam ganhos financeiros pessoais; Download, upload ou armazenamento de material com conte\u00fados sexuais ou ilegais; Execu\u00e7\u00e3o de tarefas que utilizem quantidade de recursos incompat\u00edvel com o compartilhamento da infraestrutura e que interfiram com o desempenho global do sistema; Execu\u00e7\u00e3o de tarefas que resultem na viola\u00e7\u00e3o de licen\u00e7a de aplicativos computacionais previamente instalados; Distribui\u00e7\u00e3o ou armazenamento de m\u00fasicas, filmes, ou qualquer material protegido por copyright sem a devida licen\u00e7a. Quest\u00f5es sobre a adequa\u00e7\u00e3o do uso dos recursos computacionais dever\u00e3o ser comunicadas diretamente ao LIneA via Service Desk . Os dados em SCRATCH n\u00e3o possuem c\u00f3pias de seguran\u00e7a (backup). Os dados cr\u00edticos devem ser salvos pelo usu\u00e1rio em seu diret\u00f3rio HOME e s\u00e3o de sua pr\u00f3pria responsabilidade. Mais detalhes sobre os sistemas de armazenamento . Todo usu\u00e1rio \u00e9 obrigado a comunicar imediatamente toda e qualquer suspeita de viola\u00e7\u00e3o das regras de seguran\u00e7a, enviando e-mail para helpdesk@linea.org.br . O LIneA zela pela privacidade de todos os colaboradores e usu\u00e1rios. Nenhuma consulta ou vistoria ser\u00e1 feita nos arquivos sem autoriza\u00e7\u00e3o do pr\u00f3prio usu\u00e1rio, salvo em situa\u00e7\u00f5es que envolvam comprometimento da seguran\u00e7a dos sistemas. Todo usu\u00e1rio da infraestrutura computacional do LIneA compromete-se a: * Incluir em todos os artigos submetidos a revistas cient\u00edficas, proceedings de confer\u00eancias, teses, disserta\u00e7\u00f5es, etc., que fa\u00e7am uso dos recursos computacionais do LIneA, o seguinte agradecimento: \u201cThis research was supported by resources supplied by the Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\u201d; ou, \u201cEsta pesquisa tornou-se poss\u00edvel gra\u00e7as aos recursos computacionais disponibilizados pelo Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\u201d; Encaminhar ao CDE uma c\u00f3pia da publica\u00e7\u00e3o acima mencionada a qual dever\u00e1 ser inclu\u00edda no site oficial do LIneA.","title":"Pol\u00edtica de Uso"},{"location":"processamento/uso/politicadeuso.html#politica-de-uso","text":"Pol\u00edtica de Uso da Infraestrutura Computacional do Laborat\u00f3rio Interinstitucional de e-Astronomia - LIneA. Esta Pol\u00edtica de Uso da Infraestrutura Computacional do Laborat\u00f3rio Interinstitucional de e-Astronomia abrange todos os recursos computacionais que s\u00e3o disponibilizados \u00e0 pesquisa cient\u00edfica. Esse conjunto de regras visam: Garantir um ambiente computacional amig\u00e1vel, produtivo e confi\u00e1vel; Manter um ambiente que permita o desenvolvimento e a inova\u00e7\u00e3o; Zelar pela reputa\u00e7\u00e3o do Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA) e proteger os seus sistemas de computa\u00e7\u00e3o e os dados neles contidos contra ataques externos e uso n\u00e3o autorizado; Garantir a conformidade com todas as diretrizes legais. Todos os usu\u00e1rios devem procurar manter a seguran\u00e7a do ambiente. A conta do usu\u00e1rio \u00e9 de uso exclusivo, pessoal e n\u00e3o poder\u00e1 ser compartilhada. Tentativas n\u00e3o autorizadas de acesso com o intuito de danificar, alterar, falsificar ou excluir dados, falsificar o endere\u00e7o de e-mail ou causar interrup\u00e7\u00f5es de servi\u00e7os est\u00e3o proibidas. Os recursos computacionais do LIneA devem ser utilizados \u00fanica e exclusivamente para a execu\u00e7\u00e3o de tarefas relacionadas a projetos cient\u00edficos previamente registrados. \u00c9 expressamente proibido o uso dos referidos recursos para: Atividades ilegais de qualquer natureza; Atividades que ofendam ou resultem em constrangimento p\u00fablico de colaboradores, usu\u00e1rios ou terceiros; Atividades que envolvam ganhos financeiros pessoais; Download, upload ou armazenamento de material com conte\u00fados sexuais ou ilegais; Execu\u00e7\u00e3o de tarefas que utilizem quantidade de recursos incompat\u00edvel com o compartilhamento da infraestrutura e que interfiram com o desempenho global do sistema; Execu\u00e7\u00e3o de tarefas que resultem na viola\u00e7\u00e3o de licen\u00e7a de aplicativos computacionais previamente instalados; Distribui\u00e7\u00e3o ou armazenamento de m\u00fasicas, filmes, ou qualquer material protegido por copyright sem a devida licen\u00e7a. Quest\u00f5es sobre a adequa\u00e7\u00e3o do uso dos recursos computacionais dever\u00e3o ser comunicadas diretamente ao LIneA via Service Desk . Os dados em SCRATCH n\u00e3o possuem c\u00f3pias de seguran\u00e7a (backup). Os dados cr\u00edticos devem ser salvos pelo usu\u00e1rio em seu diret\u00f3rio HOME e s\u00e3o de sua pr\u00f3pria responsabilidade. Mais detalhes sobre os sistemas de armazenamento . Todo usu\u00e1rio \u00e9 obrigado a comunicar imediatamente toda e qualquer suspeita de viola\u00e7\u00e3o das regras de seguran\u00e7a, enviando e-mail para helpdesk@linea.org.br . O LIneA zela pela privacidade de todos os colaboradores e usu\u00e1rios. Nenhuma consulta ou vistoria ser\u00e1 feita nos arquivos sem autoriza\u00e7\u00e3o do pr\u00f3prio usu\u00e1rio, salvo em situa\u00e7\u00f5es que envolvam comprometimento da seguran\u00e7a dos sistemas. Todo usu\u00e1rio da infraestrutura computacional do LIneA compromete-se a: * Incluir em todos os artigos submetidos a revistas cient\u00edficas, proceedings de confer\u00eancias, teses, disserta\u00e7\u00f5es, etc., que fa\u00e7am uso dos recursos computacionais do LIneA, o seguinte agradecimento: \u201cThis research was supported by resources supplied by the Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\u201d; ou, \u201cEsta pesquisa tornou-se poss\u00edvel gra\u00e7as aos recursos computacionais disponibilizados pelo Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\u201d; Encaminhar ao CDE uma c\u00f3pia da publica\u00e7\u00e3o acima mencionada a qual dever\u00e1 ser inclu\u00edda no site oficial do LIneA.","title":"Pol\u00edtica de Uso"},{"location":"processamento/uso/templates-jobs.html","text":"Submit Job Scripts - Templates \u00b6 Simple Job Script \u00b6 #!/bin/bash #SBATCH -p PARTITION #Name of the Partition to use #SBATCH --nodelist=NODENAME #Name of the Node to be allocated #SBATCH -J JOB-NAME #Job name #----------------------------------------------------------------------------# ##path to executable code EXEC = /scripts/YOUR.USER/EXECUTABLE.CODE srun $EXEC Nesse script \u00e9 preciso especificar o nome da fila (Partition) que ser\u00e1 usada, o nome do n\u00f3 que ser\u00e1 alocado para a excecu\u00e7\u00e3o do Job, e o caminho para o c\u00f3digo/programa a ser executado. EUPS Load Script \u00b6 #!/bin/bash #SBATCH -p PARTITION #Name of the Partition to use #SBATCH --nodelist=NODENAME #Name of the Node to be allocated #SBATCH -J JOB-NAME #Job name #----------------------------------------------------------------------------# #Carregar o EUPS . /mnt/eups/linea_eups_setup.sh #Carregar pacote setup <PACKAGE> <VERSION> ##path to executable code EXEC = /scripts/YOUR.USER/EXECUTABLE.CODE srun $EXEC Parallel Submit Job Script \u00b6 OpenMP \u00b6 MPI \u00b6","title":"Job Script (exemplos)"},{"location":"processamento/uso/templates-jobs.html#submit-job-scripts-templates","text":"","title":"Submit Job Scripts - Templates"},{"location":"processamento/uso/templates-jobs.html#parallel-submit-job-script","text":"","title":"Parallel Submit Job Script"},{"location":"sci-platforms/index.html","text":"LIneA Science Platform \u00b6 O LIneA Science Platform ( scienceplatform.linea.org.br ) \u00e9 uma plataforma online, ainda em fase de desenvolvimento, que agregar\u00e1 um conjunto de servi\u00e7os e ferramentas oferecidos para facilitar o acesso e a an\u00e1lise dos dados astron\u00f4micos hospedados no LIneA. Atualmente, uma vers\u00e3o de testes da plataforma est\u00e1 dispon\u00edvel para avalia\u00e7\u00e3o pelos usu\u00e1rios em scienceplatform-dev.linea.org.br/lsp . IDAC Science Platform \u00b6 Durante a opera\u00e7\u00e3o do levantamento LSST , uma vers\u00e3o reduzida do LIneA Science Platform, apenas com os servi\u00e7os relacionados ao projeto LSST, ser\u00e1 o principal ponto de acesso aos dados hospedados no Brazilian Independent Data Access Center (IDAC-Brasil) para os cientistas da comunidade brasileira. Nos anos que antecedem a opera\u00e7\u00e3o do LSST, os servi\u00e7os do IDAC-Brasil j\u00e1 disponibilizam acesso a dados p\u00fablicos de outros levantamentos, como o Dark Energy Survey , para que a comunidade possa se preparar e se familiarizar com as diversas ferramentas oferecidas. Servi\u00e7os \u00b6 Servi\u00e7os de acesso a dados com ferramentas diversas para processamento, explora\u00e7\u00e3o, visualiza\u00e7\u00e3o e an\u00e1lise de dados p\u00fablicos e privados. JupyterHub \u00b6 OnDemand \u00b6 User Query \u00b6 TAP Service \u00b6 Sky Viewer \u00b6 Target Viewer \u00b6 DES Science Server \u00b6 Occultation Prediction Database \u00b6 SDSS Sky Server \u00b6 LSST PZ Server \u00b6 Portais Cient\u00edficos \u00b6 Conjuntos de ferramentas desenvolvidas especificamente para cada projeto e de uso restrito dos membros de cada colabora\u00e7\u00e3o cient\u00edfica. DES Science Portal \u00b6 Solar System Portal \u00b6 MaNGA Portal \u00b6","title":"Plataformas Cient\u00edficas"},{"location":"sci-platforms/index.html#linea-science-platform","text":"O LIneA Science Platform ( scienceplatform.linea.org.br ) \u00e9 uma plataforma online, ainda em fase de desenvolvimento, que agregar\u00e1 um conjunto de servi\u00e7os e ferramentas oferecidos para facilitar o acesso e a an\u00e1lise dos dados astron\u00f4micos hospedados no LIneA. Atualmente, uma vers\u00e3o de testes da plataforma est\u00e1 dispon\u00edvel para avalia\u00e7\u00e3o pelos usu\u00e1rios em scienceplatform-dev.linea.org.br/lsp .","title":"LIneA Science Platform"},{"location":"sci-platforms/index.html#idac-science-platform","text":"Durante a opera\u00e7\u00e3o do levantamento LSST , uma vers\u00e3o reduzida do LIneA Science Platform, apenas com os servi\u00e7os relacionados ao projeto LSST, ser\u00e1 o principal ponto de acesso aos dados hospedados no Brazilian Independent Data Access Center (IDAC-Brasil) para os cientistas da comunidade brasileira. Nos anos que antecedem a opera\u00e7\u00e3o do LSST, os servi\u00e7os do IDAC-Brasil j\u00e1 disponibilizam acesso a dados p\u00fablicos de outros levantamentos, como o Dark Energy Survey , para que a comunidade possa se preparar e se familiarizar com as diversas ferramentas oferecidas.","title":"IDAC Science Platform"},{"location":"sci-platforms/index.html#servicos","text":"Servi\u00e7os de acesso a dados com ferramentas diversas para processamento, explora\u00e7\u00e3o, visualiza\u00e7\u00e3o e an\u00e1lise de dados p\u00fablicos e privados.","title":"Servi\u00e7os"},{"location":"sci-platforms/index.html#portais-cientificos","text":"Conjuntos de ferramentas desenvolvidas especificamente para cada projeto e de uso restrito dos membros de cada colabora\u00e7\u00e3o cient\u00edfica.","title":"Portais Cient\u00edficos"},{"location":"sci-platforms/des.html","text":"O DES Science Portal ( des-portal.linea.org.br ) foi uma plataforma desenvolvida para o projeto Dark Energy Survey (DES) que hospedou uma variedade de pipelines destinados a preparar cat\u00e1logos customizados para diferentes aplica\u00e7\u00f5es na astronomia, bem como para realizar uma variedade de an\u00e1lises cient\u00edficas. A parte que envolve o processamento de dados do DES Science Portal foi descontinuada devido \u00e0 obsolesc\u00eancia das tecnologias envolvidas, mas seu sistema de acesso aos resultados gerados e de controle de proveni\u00eancia continua dispon\u00edvel para os membros da colabora\u00e7\u00e3o DES. As funcionalidades do DES Science Portal foram gradativamente substitu\u00eddas por outros servi\u00e7os mais modernos desenvolvidos pelo LIneA, preparados para lidar com Big Data em prepara\u00e7\u00e3o para o LSST, mantendo sua filosofia e boas pr\u00e1ticas. Os pipelines do DES Science Portal foram agrupados em etapas: Data Preparation \u2013 inclui a cria\u00e7\u00e3o de mapas de efeitos sistem\u00e1ticos, classifica\u00e7\u00e3o de objetos (estrela/gal\u00e1xia), c\u00e1lculo de redshifts fotom\u00e9tricos e outros valores agregados, dependendo da aplica\u00e7\u00e3o. Por exemplo, estimativa de idade, metalicidade ou massa estelar de gal\u00e1xias. Value-added Catalog \u2013 combina os dados e resultados obtidos na etapa anterior, seleciona as colunas de interesse e aplica cortes de qualidade e limpeza dos dados, com crit\u00e9rios fortemente dependentes da aplica\u00e7\u00e3o cient\u00edfica para a qual o cat\u00e1logo ser\u00e1 destinado. Science Workflows \u2013 agrega diversos pipelines de an\u00e1lise cient\u00edfica. Tem como dados de entrada os cat\u00e1logos criados na etapa anterior. Um dos principais pontos fortes do DES Science Portal foi a capacidade de fornecer informa\u00e7\u00f5es completas de todo o hist\u00f3rico de processos executados, permitindo ao usu\u00e1rio rastrear a entrada, a configura\u00e7\u00e3o, a vers\u00e3o dos c\u00f3digos utilizados e os resultados obtidos, na forma de um registro do produto com gr\u00e1ficos e tabelas informativas. O portal tamb\u00e9m forneceu acesso a uma s\u00e9rie de ferramentas destinadas ao usu\u00e1rio, ao desenvolvedor e ao administrador. Para se aprofundar nos detalhes sobre o DES Science Portal, leia os dois artigos publicados na revista Astronomy and Computing : Gschwend et al. 2018 \u2013 DES science portal: Computing photometric redshifts ( doi.org/10.1016/j.ascom.2018.08.008 ) ( arXiv:1708.05643 ) Fausti Neto et al. 2018 \u2013 DES science portal: Creating science-ready catalogs ( doi.org/10.1016/j.ascom.2018.01.002 ) ( arXiv:1708.05642 )","title":"DES Science Portal"},{"location":"sci-platforms/jupyter.html","text":"O JupyterHub \u00e9 um ambiente de desenvolvimento multiusu\u00e1rio baseado em iPython Notebooks que oferece acesso a recursos computacionais compartilhados em um servidor remoto, sem a necessidade de instala\u00e7\u00e3o e manuten\u00e7\u00e3o por parte dos usu\u00e1rios. Os \u00fanicos pr\u00e9-requisitos para acessar o JupyterHub s\u00e3o: ter uma conta de usu\u00e1rio no LIneA (veja aqui como criar sua conta) e um navegador com acesso \u00e0 Internet. Os chamados Jupyter Notebooks permitem combinar c\u00f3digo interativo, resultados de execu\u00e7\u00e3o, texto explicativo e recursos de multim\u00eddia em um s\u00f3 documento. Como parte do LIneA Science Platform , o LIneA JupyterHub est\u00e1 integrado \u00e0s demais ferramentas de visualiza\u00e7\u00e3o ( Science Server ) e acesso a dados ( User Query ). Desse modo, toda a an\u00e1lise de dados pode ser feita online dentro da plataforma, desde a leitura, visualiza\u00e7\u00e3o, processamento e an\u00e1lise de resultados, sem a necessidade de download dos dados. Ao clicar no card \"JupyterHub\" dentro do LIneA Science Platform, voc\u00ea ser\u00e1 direcionado para a p\u00e1gina de login e em seguida para a p\u00e1gina inicial do JupyterHub que mostrar\u00e1 o seu perfil de usu\u00e1rio. Clique no bot\u00e3o START para iniciar. A instala\u00e7\u00e3o padr\u00e3o do JupyterHub utiliza a nova interface JupyterLab e \u00e9 baseada na imagem datascience-notebook . Isto significa que uma s\u00e9rie de bibliotecas Python de grande popularidade como Astropy , Numpy e Matplotlib estar\u00e3o automaticamente dispon\u00edveis. Apoio ao usu\u00e1rio \u00b6 Tutoriais em Jupyter Notebooks \u00b6 No reposit\u00f3rio jupyterhub-tutorial voc\u00ea encontrar\u00e1 os tutoriais em formato notebook : 1-primeiros-passos.ipynb \u00b6 Instru\u00e7\u00f5es gerais de uso da plataforma JupyterLab, dicas e atalhos na escrita de notebooks para diferentes tipos de c\u00e9lulas. 2-acesso-a-dados.ipynb \u00b6 Instru\u00e7\u00f5es para uso da biblioteca dblinea para leitura de dados a partir do banco de dados diretamente de dentro de uma c\u00e9lula no notebook com exemplo de uso (constru\u00e7\u00e3o de um diagrama cor-magnitude simples para uma amostra de estrelas). 3-conda-env.ipynb \u00b6 Instru\u00e7\u00f5es para cria\u00e7\u00e3o de ambientes no conda para gerenciamento de bibliotecas que sejam persistentes e sobrevivam a destrui\u00e7\u00e3o e recria\u00e7\u00e3o dos containers para que os usu\u00e1rios possam retornar em uma nova sess\u00e3o e encontrar o mesmo ambiente da sess\u00e3o anterior (n\u00e3o dispon\u00edvel para usu\u00e1rios de perfil p\u00fablico bronze). Para acessar os notebooks , basta abrir um Terminal no JupyterLab clicando no bot\u00e3o \"+\" na barra superior e em seguida no \u00edcone \"Terminal\" da se\u00e7\u00e3o \"Other\" na aba \"Launcher\", e inserir o comando: git clone https://github.com/linea-it/jupyterhub-tutorial.git Recursos computacionais \u00b6 Configura\u00e7\u00f5es dispon\u00edveis para o Jupyter over K8S Ap\u00f3s efetuar login na plataforma, ser\u00e1 exibido um menu com at\u00e9 tr\u00eas op\u00e7\u00f5es de configura\u00e7\u00e3o. Basta selecionar e clicar em Start My Server . Tamanho CPUs RAM Small 1.0 4 GiB Medium 2.0 8 GiB Large 4.0 16 GiB Configura\u00e7\u00e3o dos servidores do ambiente K8S A plataforma Jupyter \u00e9 executada sobre o Kubernetes (K8S) e possui 12 servidores f\u00edsicos dedicados. Cada m\u00e1quina \u00e9 equipada com os seguintes recursos computacionais: Kubernetes Node Configuration RAM 64 GB Thread(s) per core 2 Core(s) per socket 6 Socket(s) 2 Jupyter over K8S vs Jupyter over HPC O LIneA disponibiliza dois ambientes separados de Jupyter Notebook. O primeiro \u00e9 executado em containers na plataforma Kubernetes (K8S). O segundo est\u00e1 dispon\u00edvel na plataforma Ondemand e acessa diretamente a infraestrutura de HPC.","title":"JupyterHub"},{"location":"sci-platforms/jupyter.html#apoio-ao-usuario","text":"","title":"Apoio ao usu\u00e1rio"},{"location":"sci-platforms/jupyter.html#tutoriais-em-jupyter-notebooks","text":"No reposit\u00f3rio jupyterhub-tutorial voc\u00ea encontrar\u00e1 os tutoriais em formato notebook :","title":"Tutoriais em Jupyter Notebooks"},{"location":"sci-platforms/jupyter.html#recursos-computacionais","text":"Configura\u00e7\u00f5es dispon\u00edveis para o Jupyter over K8S Ap\u00f3s efetuar login na plataforma, ser\u00e1 exibido um menu com at\u00e9 tr\u00eas op\u00e7\u00f5es de configura\u00e7\u00e3o. Basta selecionar e clicar em Start My Server . Tamanho CPUs RAM Small 1.0 4 GiB Medium 2.0 8 GiB Large 4.0 16 GiB Configura\u00e7\u00e3o dos servidores do ambiente K8S A plataforma Jupyter \u00e9 executada sobre o Kubernetes (K8S) e possui 12 servidores f\u00edsicos dedicados. Cada m\u00e1quina \u00e9 equipada com os seguintes recursos computacionais: Kubernetes Node Configuration RAM 64 GB Thread(s) per core 2 Core(s) per socket 6 Socket(s) 2 Jupyter over K8S vs Jupyter over HPC O LIneA disponibiliza dois ambientes separados de Jupyter Notebook. O primeiro \u00e9 executado em containers na plataforma Kubernetes (K8S). O segundo est\u00e1 dispon\u00edvel na plataforma Ondemand e acessa diretamente a infraestrutura de HPC.","title":"Recursos computacionais"},{"location":"sci-platforms/linea-occulation-prediction-database.html","text":"O LIneA Occultation Prediction Database \u00e9 um servi\u00e7o cient\u00edfico que disponibiliza acesso sistem\u00e1tico e atualizado a predi\u00e7\u00f5es de oculta\u00e7\u00f5es estelares por corpos menores do Sistema Solar. Desenvolvido pelo Portal de Sistema Solar do LIneA, o sistema integra dados orbitais atualizados com o cat\u00e1logo estelar Gaia DR3, utilizando algoritmos especializados para c\u00e1lculo de efem\u00e9rides e circunst\u00e2ncias de oculta\u00e7\u00e3o. O servi\u00e7o \u00e9 projetado para atender tanto a aplica\u00e7\u00f5es profissionais de pesquisa quanto a projetos de ci\u00eancia cidad\u00e3. Principais Funcionalidades \u00b6 A plataforma fornece acesso program\u00e1tico via API RESTful com endpoints para consulta por identifica\u00e7\u00e3o de objetos (nome ou designa\u00e7\u00e3o), classes din\u00e2micas (classifica\u00e7\u00e3o Skybot), par\u00e2metros estelares (magnitude), filtros geogr\u00e1ficos, etc. A interface web interativa permite a visualiza\u00e7\u00e3o tabular de eventos, exibi\u00e7\u00e3o de mapas de predi\u00e7\u00e3o din\u00e2micos, download de mapas SORA e arquivos KMZ. Al\u00e9m disso, servi\u00e7o disponibiliza o pacote Python ( lineaSSP ) para integra\u00e7\u00e3o em workflows cient\u00edficos e um servi\u00e7o de subscri\u00e7\u00e3o para notifica\u00e7\u00e3o de eventos personalizados. Documenta\u00e7\u00e3o e Acesso \u00b6 Para informa\u00e7\u00f5es detalhadas sobre par\u00e2metros da API e exemplos de uso, consulte a documenta\u00e7\u00e3o completa . Acesse o servi\u00e7o diretamente: LIneA Occultation Prediction Database","title":"Occultation Predictions Database"},{"location":"sci-platforms/linea-occulation-prediction-database.html#principais-funcionalidades","text":"A plataforma fornece acesso program\u00e1tico via API RESTful com endpoints para consulta por identifica\u00e7\u00e3o de objetos (nome ou designa\u00e7\u00e3o), classes din\u00e2micas (classifica\u00e7\u00e3o Skybot), par\u00e2metros estelares (magnitude), filtros geogr\u00e1ficos, etc. A interface web interativa permite a visualiza\u00e7\u00e3o tabular de eventos, exibi\u00e7\u00e3o de mapas de predi\u00e7\u00e3o din\u00e2micos, download de mapas SORA e arquivos KMZ. Al\u00e9m disso, servi\u00e7o disponibiliza o pacote Python ( lineaSSP ) para integra\u00e7\u00e3o em workflows cient\u00edficos e um servi\u00e7o de subscri\u00e7\u00e3o para notifica\u00e7\u00e3o de eventos personalizados.","title":"Principais Funcionalidades"},{"location":"sci-platforms/linea-occulation-prediction-database.html#documentacao-e-acesso","text":"Para informa\u00e7\u00f5es detalhadas sobre par\u00e2metros da API e exemplos de uso, consulte a documenta\u00e7\u00e3o completa . Acesse o servi\u00e7o diretamente: LIneA Occultation Prediction Database","title":"Documenta\u00e7\u00e3o e Acesso"},{"location":"sci-platforms/manga.html","text":"O portal MaNGA ( manga.linea.org.br ) foi desenvolvido para atender \u00e0s necessidades dos membros do Brazilian Participation Group do levantamento MaNGA do Sloan Digital Sky Survey. O sistema foi projetado para permitir que a equipe visualize, n\u00e3o apenas os cubos de dados de espectroscopia IFU (Integral Field Units) reduzidos, mas os resultados da an\u00e1lise dos dados mostrando mapas de v\u00e1rias quantidades f\u00edsicas derivadas dos espectros. Para mais informa\u00e7\u00f5es sobre como utilizar o servi\u00e7o, consulte os tutoriais no site do portal MaNGA.","title":"MaNGA Portal"},{"location":"sci-platforms/ondemand.html","text":"O OnDemand ( ondemand.linea.org.br ) \u00e9 uma interface que facilita o acesso ao ambiente de computa\u00e7\u00e3o de alto desempenho atrav\u00e9s do navegador web. O servi\u00e7o oferece: JupyterLab integrado ao cluster Apollo Ferramenta de gerenciamento de arquivos Acesso ao ambiente via Terminal Linux Clique aqui para mais detalhes sobre como utilizar o servi\u00e7o OnDemand.","title":"OnDemand"},{"location":"sci-platforms/pz_server.html","text":"Aviso \u00b6 Aviso sobre a vers\u00e3o PT-BR Esta \u00e9 uma tradu\u00e7\u00e3o do documento original dispon\u00edvel em ingl\u00eas. Optamos por n\u00e3o traduzir os nomes dos pipelines e tipos de produtos para manter os nomes consistentes com os que aparecem no Photo-z Server. Al\u00e9m disso, n\u00e3o traduzimos trechos de c\u00f3digo para mant\u00ea-los consistentes com o tutorial disponibilizado como Jupyter notebook no reposit\u00f3rio da biblioteca Python pzserver . Introdu\u00e7\u00e3o \u00b6 Inspirado no DES Science Portal ( Gschwend et al., 2018 ; Fausti Neto et al., 2018 ), o Photo-z Server \u00e9 um servi\u00e7o online complementar ao Rubin Science Platform (RSP) para hospedar e produzir produtos de dados leves relacionados ao photo-z e oferecer ferramentas de gerenciamento de dados que permitam o compartilhamento de produtos de dados entre usu\u00e1rios do RSP, anexar e compartilhar metadados relevantes e auxiliar no rastreamento de proveni\u00eancia. O servi\u00e7o est\u00e1 hospedado no Brazilian Independent Data Access Center ( IDAC ) e est\u00e1 aberto a toda a Comunidade LSST, sem restri\u00e7\u00f5es geogr\u00e1ficas. Ele foi projetado para ser o mais amplo e gen\u00e9rico poss\u00edvel, a fim de ser \u00fatil a todas as Colabora\u00e7\u00f5es Cient\u00edficas do LSST que trabalham com produtos de dados de photo-z. Conforme exigido pelo programa de contribui\u00e7\u00f5es in-kind do LSST, o c\u00f3digo-fonte est\u00e1 dispon\u00edvel publicamente no GitHub . O Photo-z Server foi projetado para auxiliar os usu\u00e1rios do RSP a participar do Photo-z (PZ) Validation Cooperative. Esta iniciativa da equipe de Data Management (DM) ocorrer\u00e1 durante a fase de comissionamento do LSST (consulte a nota t\u00e9cnica dmtn-049 para obter detalhes). O PZ Coordination Group receber\u00e1 credenciais de usu\u00e1rio administrador com permiss\u00f5es especiais para adicionar produtos de dados marcados como official data products . Isso incluir\u00e1 conjuntos padronizados de treinamento e valida\u00e7\u00e3o utilizados para compara\u00e7\u00f5es de desempenho de algoritmos, bem como um meio de coletar resultados de m\u00faltiplos usu\u00e1rios. Al\u00e9m do PZ Validation Cooperative, o Photo-z Server continuar\u00e1 sendo um recurso para a Comunidade LSST nos pr\u00f3ximos anos. Os usu\u00e1rios do RSP podem continuar a us\u00e1-lo para organizar, rastrear e compartilhar arquivos leves contendo diversos resultados de testes. Datasets Os administradores do Photo-z Server mant\u00eam e atualizam periodicamente uma lista selecionada de conjuntos de dados para dar suporte \u00e0 Comunidade LSST com produtos de dados relacionados ao Photo-z. Descri\u00e7\u00f5es detalhadas e links para cada produto de dados est\u00e3o dispon\u00edveis em uma p\u00e1gina separada . Site do Photo-z Server \u00b6 A interface principal do Photo-z Server \u00e9 o seu site no endere\u00e7o pzserver.linea.org.br . Os tr\u00eas cards na p\u00e1gina inicial levam \u00e0 lista de produtos de dados (esquerda e centro) ou aos pipelines do Photo-z Server (direita). Na p\u00e1gina da lista de produtos de dados, os usu\u00e1rios podem navegar, pesquisar e filtrar os produtos enviados por outros usu\u00e1rios ou criados com um pipelines do Photo-z Server. Os produtos de dados enviados para o PZ Server tornam-se automaticamente vis\u00edveis, baix\u00e1veis e compartilh\u00e1veis com todos os usu\u00e1rios registrados. Upload de um novo produto de dados \u00b6 Para fazer upload de um novo produto de dados, clique no bot\u00e3o NOVO PRODUTO no canto superior direito da User-generated Data Products page e preencha o formul\u00e1rio com os metadados relevantes em quatro etapas: Etapa 1: Informe um nome curto e mnem\u00f4nico para o seu novo produto de dados. Selecione o tipo de produto de dados que voc\u00ea est\u00e1 carregando (por exemplo, Reference Redshift Catalog, Training Set, etc.) e ao release ao qual ele pertence (se aplic\u00e1vel). Etapa 2: Selecione o arquivo principal e quantos arquivos auxiliares desejar enviar. O arquivo principal cont\u00e9m o produto de dados em si, enquanto os arquivos auxiliares podem incluir documenta\u00e7\u00e3o, descri\u00e7\u00e3o ou qualquer outra informa\u00e7\u00e3o relevante sobre o produto de dados. Se o produto de dados for tabular, a ferramenta de upload pode exigir formatos de arquivo espec\u00edficos, dependendo do tipo. Os formatos atualmente suportados s\u00e3o: CSV, FITS, HDF5 e Parquet. Entre em contato com a equipe de desenvolvimento se o seu caso cient\u00edfico exigir um formato de arquivo diferente ou se o arquivo for maior que o limite de 200 MB. Etapa 3: Se o produto de dados for um Reference Redshift Catalog ou um Training Set, algumas colunas s\u00e3o obrigat\u00f3rias. Os nomes das colunas s\u00e3o livres, mas voc\u00ea deve fornecer a associa\u00e7\u00e3o com seu significado e UCDs no padr\u00e3o IVOA , como na figura abaixo. Etapa 4: Revise suas informa\u00e7\u00f5es e volte aos passos anteriores, se necess\u00e1rio. N\u00e3o se esque\u00e7a de clicar no bot\u00e3o FINISH na parte inferior da p\u00e1gina para enviar seu produto de dados. Download de um produto de dados \u00b6 Para baixar um produto de dados, clique no \u00edcone na linha do produto na p\u00e1gina Produtos de Dados Gerados pelo Usu\u00e1rio . O clique acionar\u00e1 a prepara\u00e7\u00e3o de um arquivo .zip compactado com todo o conte\u00fado do produto de dados, incluindo arquivos de descri\u00e7\u00e3o auxiliares. H\u00e1 tamb\u00e9m um bot\u00e3o na p\u00e1gina de detalhes do produto, que pode ser acessada clicando no nome do produto na lista. Compartilhar produtos de dados \u00b6 Para compartilhar um produto de dados, clique no \u00edcone na linha do produto na p\u00e1gina Produtos de Dados Gerados pelo Usu\u00e1rio ou na p\u00e1gina de detalhes do produto. O clique abrir\u00e1 uma janela pop-up com o internal_name e o endere\u00e7o URL do produto. Voc\u00ea pode copiar as informa\u00e7\u00f5es para compartilh\u00e1-las com outros usu\u00e1rios. internal_name Cada produto de dados possui um nome \u00fanico (\" internal_name \"), composto automaticamente pelo sistema como um n\u00famero id \u00fanico seguido pelo nome escolhido pelo usu\u00e1rio, com espa\u00e7os substitu\u00eddos por sublinhados. Este nome \u00e9 o endere\u00e7o URL da p\u00e1gina de detalhes do produto de dados no site do Photo-z Server: https://pzserver.linea.org.br/product/internal_name e \u00e9 a chave para acessar os dados usando a API Python do Photo-z Server (veja os detalhes abaixo). A maneira mais f\u00e1cil de compartilhar um produto de dados \u00e9 fornecendo o internal_name ou a URL do produto, que leva \u00e0 p\u00e1gina de download do produto. Tipos de produtos \u00b6 Reference Redshift Catalog \u00b6 No contexto do PZ Server, Reference Redshift Catalogs s\u00e3o definidos como qualquer cat\u00e1logo contendo coordenadas equatoriais esf\u00e9ricas e medidas de redshift (geralmente medidas espectrosc\u00f3picas ou redshift verdadeiros, em caso de simula\u00e7\u00f5es). Colunas obrigat\u00f3rias: Ascens\u00e3o reta [graus] - float Declina\u00e7\u00e3o [graus] - float Redshift - float Coluna recomendada: Erros do redshift - float Um Cat\u00e1logo de Desvio para o Vermelho de Refer\u00eancia pode incluir dados de um \u00fanico levantamento espectrosc\u00f3pico ou uma combina\u00e7\u00e3o de dados de v\u00e1rias fontes. Requisitos do pipeline Se um Reference Redshift Catalog for destinado a ser utilizado como dado de entrada para o pipeline Combine Redshift Catalogs aplicando o recurso de resolu\u00e7\u00e3o de duplicatas (consulte detalhes do pipeline aqui ), recomenda-se incluir as seguintes colunas: Indicador de qualidade (associar com z_flag na etapa 3 do upload) - integer , float ou string (o sinalizador de qualidade original do cat\u00e1logo de origem, quando dispon\u00edvel) Tipo de medi\u00e7\u00e3o - string (por exemplo, \"s\" para \"spectroscopic\", \"g\" para \"grism/prism\", \"p\" para \"photometric\", conforme adotado em SITCOMTN-154 ) Nome do levantamento (associar com survey na etapa 3 do upload) - string (ex.: \"DESI\", \"COSMOS2025\", \"JADES\", etc.) Outras colunas com informa\u00e7\u00f5es adicionais que voc\u00ea deseja usar para resolu\u00e7\u00e3o de duplicatas (ex.: resolu\u00e7\u00e3o de instrumento). Training Set \u00b6 No contexto do Photo-z Server, os Training Sets s\u00e3o definidos como o produto da associa\u00e7\u00e3o espacial entre um determinado Reference Redshift Catalog (levantamento \u00fanico ou compila\u00e7\u00e3o) e os dados fotom\u00e9tricos, neste caso, o LSST Object Catalog. O pipeline Training Set Maker do Photo-z Server permite que os usu\u00e1rios criem Training Sets personalizados com base nos Reference Redshift Catalogs dispon\u00edveis (consulte detalhes do pipeline aqui ). subconjuntos de train/test Os training sets comumente divididos em dois ou mais subconjuntos para fins de valida\u00e7\u00e3o do photo-z. Se o propriet\u00e1rio do training set tiver definido previamente quais objetos devem pertencer a cada subconjunto (treinamento e valida\u00e7\u00e3o/teste), essas informa\u00e7\u00f5es devem estar dispon\u00edveis como uma coluna extra na tabela ou como instru\u00e7\u00f5es claras para reproduzir a separa\u00e7\u00e3o de subconjuntos na descri\u00e7\u00e3o do produto. Para o caso de dois arquivos previamente separados, seus uploads devem ser feitos separadamente, gerando dois produtos de dados independentes, ambos classificados com o tipo de produto \"Training Set\". A sua destina\u00e7\u00e3o pode ser informada explicitamente no nome e/ou descri\u00e7\u00e3o do produto. training sets baseados em imagens O tipo de produto Training Set aceita apenas dados de cat\u00e1logos. Training sets baseados em imagens comumente usados por algoritmos de deep-learning n\u00e3o s\u00e3o compat\u00edveis. Nesse caso, utilize o tipo de produto \"Other\" e forne\u00e7a uma descri\u00e7\u00e3o clara do formato dos dados na descri\u00e7\u00e3o do produto. Para garantir flexibilidade nos observ\u00e1veis, a \u00fanica coluna obrigat\u00f3ria \u00e9 o redshift ( float ). Outras colunas esperadas s\u00e3o: objectId do LSST Objects Catalog - integer Observ\u00e1veis (magnitudes e/ou cores, ou fluxos) do LSST Objects Catalog - float Erros dos observ\u00e1veis - float Ascens\u00e3o reta [graus] - float Declina\u00e7\u00e3o [graus] - float Indicador de Qualidade - integer , float ou string Indicador de Subconjunto - integer , float ou string Training Results \u00b6 Os resultados do treinamento de algoritmos baseados machine learning tamb\u00e9m podem ser hospedados no Photo-z Server para serem compartilhados e reutilizados. Este tipo de produto permite arquivos em formato livre. Quando os resultados do treinamento s\u00e3o gerados com o m\u00e9todo inform do RAIL , eles s\u00e3o armazenados como arquivos pickle . Validation Results \u00b6 O tipo de produto Validation Results destina-se a identificar os resultados de qualquer procedimento de valida\u00e7\u00e3o de foto-z. Ele pode ser usado para armazenar os resultados da PZ Validation Cooperative ou quaisquer outras tarefas de valida\u00e7\u00e3o. Este tipo de produto \u00e9 bastante gen\u00e9rico. Ele pode conter estimativas de photo-z (estimativas individuais e/ou PDF) de um conjunto de teste, m\u00e9tricas de valida\u00e7\u00e3o, gr\u00e1ficos QQ-PIT, etc. Os usu\u00e1rios podem carregar um arquivo principal e uma lista de arquivos auxiliares em qualquer formato. Photo-z Estimates \u00b6 As Estimativas Photo-z s\u00e3o os resultados de qualquer procedimento de estimativa Photo-z, geralmente a sa\u00edda do m\u00e9todo estimate do RAIL . Se os dados forem maiores que o limite de upload de arquivo (200 MB), a entrada do produto armazenar\u00e1 apenas os metadados e as instru\u00e7\u00f5es de acesso aos dados devem ser fornecidas no campo de descri\u00e7\u00e3o. Other \u00b6 Qualquer outro produto de dados que n\u00e3o se enquadre nas categorias anteriores pode ser carregado como um produto do tipo Other. Este \u00e9 um tipo de produto gen\u00e9rico que permite aos usu\u00e1rios carregar qualquer formato de arquivo e fornecer uma descri\u00e7\u00e3o do produto de dados no campo de descri\u00e7\u00e3o. API & Python library \u00b6 O Photo-z Server tamb\u00e9m oferece uma API e uma biblioteca Python para facilitar o acesso de linha de comando a dados e metadados. A API cont\u00e9m fun\u00e7\u00f5es para explorar os produtos de dados dispon\u00edveis, recuperar o conte\u00fado de um determinado produto de dados para trabalhar na mem\u00f3ria ou baixar os arquivos de interesse. O pacote Python pzserver \u00e9 de c\u00f3digo aberto e est\u00e1 dispon\u00edvel no GitHub e pode ser instalado via pip com: pip install pzserver Tutorial notebook \u00b6 Um notebook de tutorial com exemplos para todos os m\u00e9todos pzserver est\u00e1 dispon\u00edvel no reposit\u00f3rio da biblioteca pzserver no GitHub . H\u00e1 tamb\u00e9m a p\u00e1gina de documenta\u00e7\u00e3o da API com mais detalhes voltados para desenvolvedores. Token de acesso \u00b6 Ap\u00f3s instalada e importada em um ambiente Python, a classe PzServer abre a conex\u00e3o remota com o banco de dados do PZ Server. from pzserver import PzServer pz_server = PzServer ( token = \"<cole seu token de acesso aqui>\" ) Um token de acesso \u00e9 necess\u00e1rio para autentica\u00e7\u00e3o. Os usu\u00e1rios podem gerar o token no site do PZ Server (menu no canto superior direito da p\u00e1gina inicial). Comandos b\u00e1sicos \u00b6 Comandos b\u00e1sicos para exibir dados e metadados em uma c\u00e9lula do notebook Jupyter (se n\u00e3o estiver em um notebook Jupyter, substitua display por get para retornar os resultados como dicion\u00e1rios Python): pz_server . display_product_types () pz_server . display_releases () pz_server . display_products_list () pz_server . display_products_list ( filters = { \"release\" : \"DP1\" , \"product_type\" : \"Training Set\" }) search_results = pz_server . get_products_list ( filters = { \"product_type\" : \"results\" }) pz_server . display_product_metadata ( id or \"internal_name\" ) Comandos b\u00e1sicos para baixar ou retornar dados para a mem\u00f3ria: pz_server . download_product ( id or \"internal_name\" , save_in = \".\" ) data = pz_server . get_product ( id or \"internal_name\" ) Consulte o notebook de tutorial para obter a lista completa de exemplos, incluindo instru\u00e7\u00f5es para upload e edi\u00e7\u00e3o de metadados por meio da biblioteca pzserver . Photo-z Server pipelines \u00b6 Os pipelines do Photo-z Server s\u00e3o um conjunto de ferramentas para ajudar os usu\u00e1rios a criar e gerenciar produtos de dados. Os pipelines atualmente dispon\u00edveis s\u00e3o (clique nos links para mais detalhes): Combine Redshift Catalog \u00b6 Training Set Maker \u00b6 C\u00f3digo Aberto \u00b6 O Photo-z Server \u00e9 um projeto de c\u00f3digo aberto. Seu c\u00f3digo-fonte est\u00e1 dispon\u00edvel nos seguintes reposit\u00f3rios do GitHub: pzserver_app : o c\u00f3digo principal da aplica\u00e7\u00e3o, incluindo a interface web e a API. pzserver : a biblioteca Python utilizada para acessar a API do Photo-z Server. pzserver_pipelines : o c\u00f3digo das pipelines dispon\u00edveis no Photo-z Server. orchestration : a aplica\u00e7\u00e3o respons\u00e1vel por enviar as pipelines para o cluster HPC do IDAC e gerenciar sua execu\u00e7\u00e3o. pz-lsst-inkind : c\u00f3digo para tarefas de gerenciamento de dados no programa in-kind do Photo-z Server, incluindo prepara\u00e7\u00e3o de dados, verifica\u00e7\u00e3o de qualidade e notebooks de valida\u00e7\u00e3o das pipelines. pz-lsst-inkind-doc : documenta\u00e7\u00e3o de alto n\u00edvel sobre o programa in-kind do Photo-z Server, publicada via GitHub Pages. O c\u00f3digo est\u00e1 licenciado sob a Licen\u00e7a MIT . Contribui\u00e7\u00f5es s\u00e3o bem-vindas! Agradecimentos \u00b6 O Photo-z Server utiliza recursos computacionais do IDAC-Brasil no Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA) com apoio financeiro do INCT do e-Universo (Processo n.\u00ba 465376/2014-2) e do projeto FINEP: LIneA: Centro de e-Ci\u00eancia para explorar os mist\u00e9rios do Universo e apoiar projetos de Big Data (ref. n.\u00ba 0883/24).","title":"LSST Photo-z Server"},{"location":"sci-platforms/pz_server.html#aviso","text":"Aviso sobre a vers\u00e3o PT-BR Esta \u00e9 uma tradu\u00e7\u00e3o do documento original dispon\u00edvel em ingl\u00eas. Optamos por n\u00e3o traduzir os nomes dos pipelines e tipos de produtos para manter os nomes consistentes com os que aparecem no Photo-z Server. Al\u00e9m disso, n\u00e3o traduzimos trechos de c\u00f3digo para mant\u00ea-los consistentes com o tutorial disponibilizado como Jupyter notebook no reposit\u00f3rio da biblioteca Python pzserver .","title":"Aviso"},{"location":"sci-platforms/pz_server.html#introducao","text":"Inspirado no DES Science Portal ( Gschwend et al., 2018 ; Fausti Neto et al., 2018 ), o Photo-z Server \u00e9 um servi\u00e7o online complementar ao Rubin Science Platform (RSP) para hospedar e produzir produtos de dados leves relacionados ao photo-z e oferecer ferramentas de gerenciamento de dados que permitam o compartilhamento de produtos de dados entre usu\u00e1rios do RSP, anexar e compartilhar metadados relevantes e auxiliar no rastreamento de proveni\u00eancia. O servi\u00e7o est\u00e1 hospedado no Brazilian Independent Data Access Center ( IDAC ) e est\u00e1 aberto a toda a Comunidade LSST, sem restri\u00e7\u00f5es geogr\u00e1ficas. Ele foi projetado para ser o mais amplo e gen\u00e9rico poss\u00edvel, a fim de ser \u00fatil a todas as Colabora\u00e7\u00f5es Cient\u00edficas do LSST que trabalham com produtos de dados de photo-z. Conforme exigido pelo programa de contribui\u00e7\u00f5es in-kind do LSST, o c\u00f3digo-fonte est\u00e1 dispon\u00edvel publicamente no GitHub . O Photo-z Server foi projetado para auxiliar os usu\u00e1rios do RSP a participar do Photo-z (PZ) Validation Cooperative. Esta iniciativa da equipe de Data Management (DM) ocorrer\u00e1 durante a fase de comissionamento do LSST (consulte a nota t\u00e9cnica dmtn-049 para obter detalhes). O PZ Coordination Group receber\u00e1 credenciais de usu\u00e1rio administrador com permiss\u00f5es especiais para adicionar produtos de dados marcados como official data products . Isso incluir\u00e1 conjuntos padronizados de treinamento e valida\u00e7\u00e3o utilizados para compara\u00e7\u00f5es de desempenho de algoritmos, bem como um meio de coletar resultados de m\u00faltiplos usu\u00e1rios. Al\u00e9m do PZ Validation Cooperative, o Photo-z Server continuar\u00e1 sendo um recurso para a Comunidade LSST nos pr\u00f3ximos anos. Os usu\u00e1rios do RSP podem continuar a us\u00e1-lo para organizar, rastrear e compartilhar arquivos leves contendo diversos resultados de testes. Datasets Os administradores do Photo-z Server mant\u00eam e atualizam periodicamente uma lista selecionada de conjuntos de dados para dar suporte \u00e0 Comunidade LSST com produtos de dados relacionados ao Photo-z. Descri\u00e7\u00f5es detalhadas e links para cada produto de dados est\u00e3o dispon\u00edveis em uma p\u00e1gina separada .","title":"Introdu\u00e7\u00e3o"},{"location":"sci-platforms/pz_server.html#site-do-photo-z-server","text":"A interface principal do Photo-z Server \u00e9 o seu site no endere\u00e7o pzserver.linea.org.br . Os tr\u00eas cards na p\u00e1gina inicial levam \u00e0 lista de produtos de dados (esquerda e centro) ou aos pipelines do Photo-z Server (direita). Na p\u00e1gina da lista de produtos de dados, os usu\u00e1rios podem navegar, pesquisar e filtrar os produtos enviados por outros usu\u00e1rios ou criados com um pipelines do Photo-z Server. Os produtos de dados enviados para o PZ Server tornam-se automaticamente vis\u00edveis, baix\u00e1veis e compartilh\u00e1veis com todos os usu\u00e1rios registrados.","title":"Site do Photo-z Server"},{"location":"sci-platforms/pz_server.html#upload-de-um-novo-produto-de-dados","text":"Para fazer upload de um novo produto de dados, clique no bot\u00e3o NOVO PRODUTO no canto superior direito da User-generated Data Products page e preencha o formul\u00e1rio com os metadados relevantes em quatro etapas: Etapa 1: Informe um nome curto e mnem\u00f4nico para o seu novo produto de dados. Selecione o tipo de produto de dados que voc\u00ea est\u00e1 carregando (por exemplo, Reference Redshift Catalog, Training Set, etc.) e ao release ao qual ele pertence (se aplic\u00e1vel). Etapa 2: Selecione o arquivo principal e quantos arquivos auxiliares desejar enviar. O arquivo principal cont\u00e9m o produto de dados em si, enquanto os arquivos auxiliares podem incluir documenta\u00e7\u00e3o, descri\u00e7\u00e3o ou qualquer outra informa\u00e7\u00e3o relevante sobre o produto de dados. Se o produto de dados for tabular, a ferramenta de upload pode exigir formatos de arquivo espec\u00edficos, dependendo do tipo. Os formatos atualmente suportados s\u00e3o: CSV, FITS, HDF5 e Parquet. Entre em contato com a equipe de desenvolvimento se o seu caso cient\u00edfico exigir um formato de arquivo diferente ou se o arquivo for maior que o limite de 200 MB. Etapa 3: Se o produto de dados for um Reference Redshift Catalog ou um Training Set, algumas colunas s\u00e3o obrigat\u00f3rias. Os nomes das colunas s\u00e3o livres, mas voc\u00ea deve fornecer a associa\u00e7\u00e3o com seu significado e UCDs no padr\u00e3o IVOA , como na figura abaixo. Etapa 4: Revise suas informa\u00e7\u00f5es e volte aos passos anteriores, se necess\u00e1rio. N\u00e3o se esque\u00e7a de clicar no bot\u00e3o FINISH na parte inferior da p\u00e1gina para enviar seu produto de dados.","title":"Upload de um novo produto de dados"},{"location":"sci-platforms/pz_server.html#download-de-um-produto-de-dados","text":"Para baixar um produto de dados, clique no \u00edcone na linha do produto na p\u00e1gina Produtos de Dados Gerados pelo Usu\u00e1rio . O clique acionar\u00e1 a prepara\u00e7\u00e3o de um arquivo .zip compactado com todo o conte\u00fado do produto de dados, incluindo arquivos de descri\u00e7\u00e3o auxiliares. H\u00e1 tamb\u00e9m um bot\u00e3o na p\u00e1gina de detalhes do produto, que pode ser acessada clicando no nome do produto na lista.","title":"Download de um produto de dados"},{"location":"sci-platforms/pz_server.html#compartilhar-produtos-de-dados","text":"Para compartilhar um produto de dados, clique no \u00edcone na linha do produto na p\u00e1gina Produtos de Dados Gerados pelo Usu\u00e1rio ou na p\u00e1gina de detalhes do produto. O clique abrir\u00e1 uma janela pop-up com o internal_name e o endere\u00e7o URL do produto. Voc\u00ea pode copiar as informa\u00e7\u00f5es para compartilh\u00e1-las com outros usu\u00e1rios. internal_name Cada produto de dados possui um nome \u00fanico (\" internal_name \"), composto automaticamente pelo sistema como um n\u00famero id \u00fanico seguido pelo nome escolhido pelo usu\u00e1rio, com espa\u00e7os substitu\u00eddos por sublinhados. Este nome \u00e9 o endere\u00e7o URL da p\u00e1gina de detalhes do produto de dados no site do Photo-z Server: https://pzserver.linea.org.br/product/internal_name e \u00e9 a chave para acessar os dados usando a API Python do Photo-z Server (veja os detalhes abaixo). A maneira mais f\u00e1cil de compartilhar um produto de dados \u00e9 fornecendo o internal_name ou a URL do produto, que leva \u00e0 p\u00e1gina de download do produto.","title":"Compartilhar produtos de dados"},{"location":"sci-platforms/pz_server.html#tipos-de-produtos","text":"","title":"Tipos de produtos"},{"location":"sci-platforms/pz_server.html#reference-redshift-catalog","text":"No contexto do PZ Server, Reference Redshift Catalogs s\u00e3o definidos como qualquer cat\u00e1logo contendo coordenadas equatoriais esf\u00e9ricas e medidas de redshift (geralmente medidas espectrosc\u00f3picas ou redshift verdadeiros, em caso de simula\u00e7\u00f5es). Colunas obrigat\u00f3rias: Ascens\u00e3o reta [graus] - float Declina\u00e7\u00e3o [graus] - float Redshift - float Coluna recomendada: Erros do redshift - float Um Cat\u00e1logo de Desvio para o Vermelho de Refer\u00eancia pode incluir dados de um \u00fanico levantamento espectrosc\u00f3pico ou uma combina\u00e7\u00e3o de dados de v\u00e1rias fontes. Requisitos do pipeline Se um Reference Redshift Catalog for destinado a ser utilizado como dado de entrada para o pipeline Combine Redshift Catalogs aplicando o recurso de resolu\u00e7\u00e3o de duplicatas (consulte detalhes do pipeline aqui ), recomenda-se incluir as seguintes colunas: Indicador de qualidade (associar com z_flag na etapa 3 do upload) - integer , float ou string (o sinalizador de qualidade original do cat\u00e1logo de origem, quando dispon\u00edvel) Tipo de medi\u00e7\u00e3o - string (por exemplo, \"s\" para \"spectroscopic\", \"g\" para \"grism/prism\", \"p\" para \"photometric\", conforme adotado em SITCOMTN-154 ) Nome do levantamento (associar com survey na etapa 3 do upload) - string (ex.: \"DESI\", \"COSMOS2025\", \"JADES\", etc.) Outras colunas com informa\u00e7\u00f5es adicionais que voc\u00ea deseja usar para resolu\u00e7\u00e3o de duplicatas (ex.: resolu\u00e7\u00e3o de instrumento).","title":"Reference Redshift Catalog"},{"location":"sci-platforms/pz_server.html#training-set","text":"No contexto do Photo-z Server, os Training Sets s\u00e3o definidos como o produto da associa\u00e7\u00e3o espacial entre um determinado Reference Redshift Catalog (levantamento \u00fanico ou compila\u00e7\u00e3o) e os dados fotom\u00e9tricos, neste caso, o LSST Object Catalog. O pipeline Training Set Maker do Photo-z Server permite que os usu\u00e1rios criem Training Sets personalizados com base nos Reference Redshift Catalogs dispon\u00edveis (consulte detalhes do pipeline aqui ). subconjuntos de train/test Os training sets comumente divididos em dois ou mais subconjuntos para fins de valida\u00e7\u00e3o do photo-z. Se o propriet\u00e1rio do training set tiver definido previamente quais objetos devem pertencer a cada subconjunto (treinamento e valida\u00e7\u00e3o/teste), essas informa\u00e7\u00f5es devem estar dispon\u00edveis como uma coluna extra na tabela ou como instru\u00e7\u00f5es claras para reproduzir a separa\u00e7\u00e3o de subconjuntos na descri\u00e7\u00e3o do produto. Para o caso de dois arquivos previamente separados, seus uploads devem ser feitos separadamente, gerando dois produtos de dados independentes, ambos classificados com o tipo de produto \"Training Set\". A sua destina\u00e7\u00e3o pode ser informada explicitamente no nome e/ou descri\u00e7\u00e3o do produto. training sets baseados em imagens O tipo de produto Training Set aceita apenas dados de cat\u00e1logos. Training sets baseados em imagens comumente usados por algoritmos de deep-learning n\u00e3o s\u00e3o compat\u00edveis. Nesse caso, utilize o tipo de produto \"Other\" e forne\u00e7a uma descri\u00e7\u00e3o clara do formato dos dados na descri\u00e7\u00e3o do produto. Para garantir flexibilidade nos observ\u00e1veis, a \u00fanica coluna obrigat\u00f3ria \u00e9 o redshift ( float ). Outras colunas esperadas s\u00e3o: objectId do LSST Objects Catalog - integer Observ\u00e1veis (magnitudes e/ou cores, ou fluxos) do LSST Objects Catalog - float Erros dos observ\u00e1veis - float Ascens\u00e3o reta [graus] - float Declina\u00e7\u00e3o [graus] - float Indicador de Qualidade - integer , float ou string Indicador de Subconjunto - integer , float ou string","title":"Training Set"},{"location":"sci-platforms/pz_server.html#training-results","text":"Os resultados do treinamento de algoritmos baseados machine learning tamb\u00e9m podem ser hospedados no Photo-z Server para serem compartilhados e reutilizados. Este tipo de produto permite arquivos em formato livre. Quando os resultados do treinamento s\u00e3o gerados com o m\u00e9todo inform do RAIL , eles s\u00e3o armazenados como arquivos pickle .","title":"Training Results"},{"location":"sci-platforms/pz_server.html#validation-results","text":"O tipo de produto Validation Results destina-se a identificar os resultados de qualquer procedimento de valida\u00e7\u00e3o de foto-z. Ele pode ser usado para armazenar os resultados da PZ Validation Cooperative ou quaisquer outras tarefas de valida\u00e7\u00e3o. Este tipo de produto \u00e9 bastante gen\u00e9rico. Ele pode conter estimativas de photo-z (estimativas individuais e/ou PDF) de um conjunto de teste, m\u00e9tricas de valida\u00e7\u00e3o, gr\u00e1ficos QQ-PIT, etc. Os usu\u00e1rios podem carregar um arquivo principal e uma lista de arquivos auxiliares em qualquer formato.","title":"Validation Results"},{"location":"sci-platforms/pz_server.html#photo-z-estimates","text":"As Estimativas Photo-z s\u00e3o os resultados de qualquer procedimento de estimativa Photo-z, geralmente a sa\u00edda do m\u00e9todo estimate do RAIL . Se os dados forem maiores que o limite de upload de arquivo (200 MB), a entrada do produto armazenar\u00e1 apenas os metadados e as instru\u00e7\u00f5es de acesso aos dados devem ser fornecidas no campo de descri\u00e7\u00e3o.","title":"Photo-z Estimates"},{"location":"sci-platforms/pz_server.html#other","text":"Qualquer outro produto de dados que n\u00e3o se enquadre nas categorias anteriores pode ser carregado como um produto do tipo Other. Este \u00e9 um tipo de produto gen\u00e9rico que permite aos usu\u00e1rios carregar qualquer formato de arquivo e fornecer uma descri\u00e7\u00e3o do produto de dados no campo de descri\u00e7\u00e3o.","title":"Other"},{"location":"sci-platforms/pz_server.html#api-python-library","text":"O Photo-z Server tamb\u00e9m oferece uma API e uma biblioteca Python para facilitar o acesso de linha de comando a dados e metadados. A API cont\u00e9m fun\u00e7\u00f5es para explorar os produtos de dados dispon\u00edveis, recuperar o conte\u00fado de um determinado produto de dados para trabalhar na mem\u00f3ria ou baixar os arquivos de interesse. O pacote Python pzserver \u00e9 de c\u00f3digo aberto e est\u00e1 dispon\u00edvel no GitHub e pode ser instalado via pip com: pip install pzserver","title":"API &amp; Python library"},{"location":"sci-platforms/pz_server.html#tutorial-notebook","text":"Um notebook de tutorial com exemplos para todos os m\u00e9todos pzserver est\u00e1 dispon\u00edvel no reposit\u00f3rio da biblioteca pzserver no GitHub . H\u00e1 tamb\u00e9m a p\u00e1gina de documenta\u00e7\u00e3o da API com mais detalhes voltados para desenvolvedores.","title":"Tutorial notebook"},{"location":"sci-platforms/pz_server.html#token-de-acesso","text":"Ap\u00f3s instalada e importada em um ambiente Python, a classe PzServer abre a conex\u00e3o remota com o banco de dados do PZ Server. from pzserver import PzServer pz_server = PzServer ( token = \"<cole seu token de acesso aqui>\" ) Um token de acesso \u00e9 necess\u00e1rio para autentica\u00e7\u00e3o. Os usu\u00e1rios podem gerar o token no site do PZ Server (menu no canto superior direito da p\u00e1gina inicial).","title":"Token de acesso"},{"location":"sci-platforms/pz_server.html#comandos-basicos","text":"Comandos b\u00e1sicos para exibir dados e metadados em uma c\u00e9lula do notebook Jupyter (se n\u00e3o estiver em um notebook Jupyter, substitua display por get para retornar os resultados como dicion\u00e1rios Python): pz_server . display_product_types () pz_server . display_releases () pz_server . display_products_list () pz_server . display_products_list ( filters = { \"release\" : \"DP1\" , \"product_type\" : \"Training Set\" }) search_results = pz_server . get_products_list ( filters = { \"product_type\" : \"results\" }) pz_server . display_product_metadata ( id or \"internal_name\" ) Comandos b\u00e1sicos para baixar ou retornar dados para a mem\u00f3ria: pz_server . download_product ( id or \"internal_name\" , save_in = \".\" ) data = pz_server . get_product ( id or \"internal_name\" ) Consulte o notebook de tutorial para obter a lista completa de exemplos, incluindo instru\u00e7\u00f5es para upload e edi\u00e7\u00e3o de metadados por meio da biblioteca pzserver .","title":"Comandos b\u00e1sicos"},{"location":"sci-platforms/pz_server.html#photo-z-server-pipelines","text":"Os pipelines do Photo-z Server s\u00e3o um conjunto de ferramentas para ajudar os usu\u00e1rios a criar e gerenciar produtos de dados. Os pipelines atualmente dispon\u00edveis s\u00e3o (clique nos links para mais detalhes):","title":"Photo-z Server pipelines"},{"location":"sci-platforms/pz_server.html#combine-redshift-catalog","text":"","title":"Combine Redshift Catalog"},{"location":"sci-platforms/pz_server.html#training-set-maker","text":"","title":"Training Set Maker"},{"location":"sci-platforms/pz_server.html#codigo-aberto","text":"O Photo-z Server \u00e9 um projeto de c\u00f3digo aberto. Seu c\u00f3digo-fonte est\u00e1 dispon\u00edvel nos seguintes reposit\u00f3rios do GitHub: pzserver_app : o c\u00f3digo principal da aplica\u00e7\u00e3o, incluindo a interface web e a API. pzserver : a biblioteca Python utilizada para acessar a API do Photo-z Server. pzserver_pipelines : o c\u00f3digo das pipelines dispon\u00edveis no Photo-z Server. orchestration : a aplica\u00e7\u00e3o respons\u00e1vel por enviar as pipelines para o cluster HPC do IDAC e gerenciar sua execu\u00e7\u00e3o. pz-lsst-inkind : c\u00f3digo para tarefas de gerenciamento de dados no programa in-kind do Photo-z Server, incluindo prepara\u00e7\u00e3o de dados, verifica\u00e7\u00e3o de qualidade e notebooks de valida\u00e7\u00e3o das pipelines. pz-lsst-inkind-doc : documenta\u00e7\u00e3o de alto n\u00edvel sobre o programa in-kind do Photo-z Server, publicada via GitHub Pages. O c\u00f3digo est\u00e1 licenciado sob a Licen\u00e7a MIT . Contribui\u00e7\u00f5es s\u00e3o bem-vindas!","title":"C\u00f3digo Aberto"},{"location":"sci-platforms/pz_server.html#agradecimentos","text":"O Photo-z Server utiliza recursos computacionais do IDAC-Brasil no Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA) com apoio financeiro do INCT do e-Universo (Processo n.\u00ba 465376/2014-2) e do projeto FINEP: LIneA: Centro de e-Ci\u00eancia para explorar os mist\u00e9rios do Universo e apoiar projetos de Big Data (ref. n.\u00ba 0883/24).","title":"Agradecimentos"},{"location":"sci-platforms/pz_server_crc.html","text":"Combine Redshift Catalogs \u00b6 Warning page under construction","title":"Combine Redshift Catalogs"},{"location":"sci-platforms/pz_server_crc.html#combine-redshift-catalogs","text":"Warning page under construction","title":"Combine Redshift Catalogs"},{"location":"sci-platforms/pz_server_tsm.html","text":"Training Set Maker \u00b6 Warning page under construction","title":"Training Set Maker"},{"location":"sci-platforms/pz_server_tsm.html#training-set-maker","text":"Warning page under construction","title":"Training Set Maker"},{"location":"sci-platforms/sci_server.html","text":"O DES Science Server \u00e9 um servi\u00e7o de visualiza\u00e7\u00e3o de imagens e dados de cat\u00e1logos que foi desenvolvido como parte da contribui\u00e7\u00e3o do LIneA para o levantamento Dark Energy Survey (DES). O DES Science Server \u00e9 composto por microsservi\u00e7os independentes para atender a diferentes demandas. Para cada um desses microsservi\u00e7os, uma nova vers\u00e3o mais flex\u00edvel e moderna est\u00e1 sendo desenvolvida para oferecer acesso aos dados privados do projeto LSST para membros e dados p\u00fablicos de diversos levantamentos para toda a comunidade, tudo num s\u00f3 lugar (veja detalhes sobre os novos sevi\u00e7os de visualiza\u00e7\u00e3o Sky Viewer e Target Viewer na p\u00e1gina Plataformas Cient\u00edficas . Sua vers\u00e3o inicial ainda est\u00e1 dispon\u00edvel e oferece acesso \u00e0s imagens e dados tabulares do DES DR2. Sky Viewer \u00b6 Oferece exibi\u00e7\u00e3o panor\u00e2mica das imagens do DES, combinadas para formar uma imagem \u00fanica limitada apenas pelas bordas do footprint , e de mapas em formato HEALPix . Target Viewer \u00b6 Ferramenta para visualizar e manipular imagens de uma lista de objetos previamente definida ( targets ). Oferece a possibilidade de rankear imagens, aplicar filtros baseados em propriedades dos objetos e criar mosaicos com m\u00faltiplas imagens. Tile Viewer \u00b6 Oferece a visualiza\u00e7\u00e3o das imagens do DES com exibi\u00e7\u00e3o baseada nas Tiles , a unidade de \u00e1rea adotada pelo levantamento. O Tile Viewer foi amplamente utilizado na inspe\u00e7\u00e3o e valida\u00e7\u00e3o dos dados nos per\u00edodos que antecederam os data releases internos e p\u00fablicos.","title":"DES Science Server"},{"location":"sci-platforms/sci_server.html#sky-viewer","text":"Oferece exibi\u00e7\u00e3o panor\u00e2mica das imagens do DES, combinadas para formar uma imagem \u00fanica limitada apenas pelas bordas do footprint , e de mapas em formato HEALPix .","title":"Sky Viewer"},{"location":"sci-platforms/sci_server.html#target-viewer","text":"Ferramenta para visualizar e manipular imagens de uma lista de objetos previamente definida ( targets ). Oferece a possibilidade de rankear imagens, aplicar filtros baseados em propriedades dos objetos e criar mosaicos com m\u00faltiplas imagens.","title":"Target Viewer"},{"location":"sci-platforms/sci_server.html#tile-viewer","text":"Oferece a visualiza\u00e7\u00e3o das imagens do DES com exibi\u00e7\u00e3o baseada nas Tiles , a unidade de \u00e1rea adotada pelo levantamento. O Tile Viewer foi amplamente utilizado na inspe\u00e7\u00e3o e valida\u00e7\u00e3o dos dados nos per\u00edodos que antecederam os data releases internos e p\u00fablicos.","title":"Tile Viewer"},{"location":"sci-platforms/sdss_sky_server.html","text":"O SDSS Sky Server \u00e9 um portal online desenvolvido pelo Sloan Digital Sky Survey (SDSS) que permite o acesso p\u00fablico a uma vasta cole\u00e7\u00e3o de dados astron\u00f4micos coletados por seus levantamentos. Ele oferece ferramentas interativas para visualizar imagens do c\u00e9u, explorar espectros e consultar cat\u00e1logos de objetos celestes, como estrelas, gal\u00e1xias e quasares. Com uma interface amig\u00e1vel e recursos que atendem desde entusiastas at\u00e9 pesquisadores profissionais, o Sky Server tamb\u00e9m disponibiliza formas de acessar os dados por meio de consultas SQL, facilitando an\u00e1lises personalizadas diretamente sobre o banco de dados do SDSS. O LIneA hospeda um espelho SDSS Sky Server ( skyserver.linea.org.br ), onde disponibiliza acesso aos dados do release DR17.","title":"SDSS Sky Sever"},{"location":"sci-platforms/sky_viewer.html","text":"O Sky Viewer ( skyviewer.linea.org.br ) \u00e9 um servi\u00e7o, em fase de desenvolvimento, baseado na ferramenta de visualiza\u00e7\u00e3o Aladin que exibe imagens HIPS e sobreposi\u00e7\u00f5es de cat\u00e1logos de diversas fontes, incluindo dados p\u00fablicos dispon\u00edveis online e dados p\u00fablicos e privados hospedados no LIneA. A lista de conjuntos de dados dispon\u00edveis depende dos direitos de acesso de cada usu\u00e1rio. Assim como o Target Viewer, o Sky Viewer foi originalmente desenvolvido para o levantamento DES, e sua vers\u00e3o inicial ainda est\u00e1 dispon\u00edvel na p\u00e1gina DES Science Server , oferecendo acesso \u00e0s imagens e dados tabulares do DES DR2. A nova vers\u00e3o, que est\u00e1 sendo desenvolvida para compor o portf\u00f3lio de servi\u00e7os do IDAC-Brasil, ser\u00e1 flex\u00edvel e oferecer\u00e1 acesso aos dados do LSST para membros, al\u00e9m de dados p\u00fablicos de outros levantamentos para o p\u00fablico geral.","title":"Sky Viewer"},{"location":"sci-platforms/solar-system-portal.html","text":"O estudo de pequenos corpos no Sistema Solar apresenta desafios consider\u00e1veis, principalmente devido aos seus pequenos tamanhos e vastas dist\u00e2ncias. Um m\u00e9todo para superar esses obst\u00e1culos envolve estudos indiretos por meio da t\u00e9cnica de oculta\u00e7\u00e3o estelar . Historicamente, a previs\u00e3o precisa desses eventos mostrou-se dif\u00edcil devido \u00e0 falta de mapas estelares suficientemente precisos. Contudo, avan\u00e7os recentes transformaram esse cen\u00e1rio, permitindo predi\u00e7\u00f5es de oculta\u00e7\u00f5es estelares altamente precisas para corpos do Sistema Solar. A t\u00e9cnica de oculta\u00e7\u00e3o estelar \u00e9 crucial para estudar esses corpos, especialmente os objetos transnetunianos (TNOs), fornecendo informa\u00e7\u00f5es precisas sobre tamanhos e posi\u00e7\u00f5es, caracter\u00edsticas do entorno, etc. Isso \u00e9 poss\u00edvel gra\u00e7as a uma das caracter\u00edsticas mais marcantes da t\u00e9cnica: a convers\u00e3o de alta resolu\u00e7\u00e3o temporal em alta resolu\u00e7\u00e3o angular. Com o aumento esperado de dez vezes no volume de dados do Legacy Survey of Space and Time (LSST), o Portal do Sistema Solar surge como uma solu\u00e7\u00e3o de computa\u00e7\u00e3o de alto desempenho para: Calcular predi\u00e7\u00f5es para eventos de oculta\u00e7\u00e3o estelar; Organizar e distribuir dados; Tornar os eventos acess\u00edveis globalmente; Reduzir a carga computacional do usu\u00e1rio; Acesso ao Portal de Sistema Solar e seus servi\u00e7os \u00b6 Interface p\u00fablica LIneA Occultation Prediction Database ( saiba mais aqui ) Documenta\u00e7\u00e3o \u00c1rea restrita Destinada a membros da colabora\u00e7\u00e3o Transneptunian Occultation Network (TON) Saiba mais sobre a TON aqui","title":"Solar System Portal"},{"location":"sci-platforms/solar-system-portal.html#acesso-ao-portal-de-sistema-solar-e-seus-servicos","text":"Interface p\u00fablica LIneA Occultation Prediction Database ( saiba mais aqui ) Documenta\u00e7\u00e3o \u00c1rea restrita Destinada a membros da colabora\u00e7\u00e3o Transneptunian Occultation Network (TON) Saiba mais sobre a TON aqui","title":"Acesso ao Portal de Sistema Solar e seus servi\u00e7os"},{"location":"sci-platforms/target_viewer.html","text":"O Target Viewer ( targetviewer.linea.org.br ) \u00e9 um servi\u00e7o, em fase de desenvolvimento, personalizado para visualiza\u00e7\u00e3o imagens astron\u00f4micas com base em uma lista de objetos-alvo previamente definida pelo usu\u00e1rio. Passo a passo: Crie uma nova lista de alvos como uma tabela no espa\u00e7o MyDB do usu\u00e1rio no User Query . Para que o Target Viewer consiga localizar as imagens dos alvos, a tabela deve conter as colunas objectId , ra e dec . Clique no bot\u00e3o \"NEW CATALOG\" na p\u00e1gina inicial do Target Viewer e preencha o formul\u00e1rio de 3 etapas para registrar a tabela como uma lista de alvos. Ap\u00f3s o envio do formul\u00e1rio, a lista de alvos aparecer\u00e1 no menu da p\u00e1gina inicial. Assim como o Sky Viewer, o Target Viewer foi originalmente desenvolvido para o levantamento DES, e sua vers\u00e3o inicial ainda est\u00e1 dispon\u00edvel na p\u00e1gina DES Science Server , oferecendo acesso \u00e0s imagens e dados tabulares do DES DR2. A nova vers\u00e3o, que est\u00e1 sendo desenvolvida para compor o portf\u00f3lio de servi\u00e7os do IDAC-Brasil, ser\u00e1 flex\u00edvel e oferecer\u00e1 acesso aos dados do LSST para membros, al\u00e9m de dados p\u00fablicos de outros levantamentos para o p\u00fablico geral.","title":"Target Viewer"},{"location":"sci-platforms/user_query.html","text":"O User Query ( userquery.linea.org.br ) \u00e9 uma interface amig\u00e1vel para consultas ao banco de dados que possibilita a cria\u00e7\u00e3o de tabelas tempor\u00e1rias. As tabelas geradas pelos usu\u00e1rios s\u00e3o gerenciadas pelo MyDB e podem ser acessadas imediatamente no JupyterHub do LIneA ou de qualquer lugar por meio do TAP Service. Al\u00e9m disso, se uma tabela contiver coordenadas equatoriais e uma coluna com identificadores \u00fanicos, ela se torna automaticamente uma lista de alvos, o que permite a visualiza\u00e7\u00e3o das imagens correspondentes aos objetos na ferramenta Target Viewer (imagens do LSST para membros e imagens do DES DR2 para o p\u00fablico geral). TAP Service \u00b6 O TAP Service (Table Access Protocol) do IVOA \u00e9 um padr\u00e3o que permite acesso e consulta a bancos de dados astron\u00f4micos por meio de linguagem SQL, de forma padronizada e interoper\u00e1vel entre diferentes servi\u00e7os. Al\u00e9m do servi\u00e7o User Query, os cat\u00e1logos p\u00fablicos hospedados no LIneA tamb\u00e9m podem ser acessados remotamente por meio do TAP Service no programa TOPCAT ou programaticamente por meio da biblioteca Python pyvo . Acesse a documetata\u00e7\u00e3o do TAP Service no site do User Query para mais detalhes sobre como utilizar o servi\u00e7o.","title":"User Query"},{"location":"sci-platforms/user_query.html#tap-service","text":"O TAP Service (Table Access Protocol) do IVOA \u00e9 um padr\u00e3o que permite acesso e consulta a bancos de dados astron\u00f4micos por meio de linguagem SQL, de forma padronizada e interoper\u00e1vel entre diferentes servi\u00e7os. Al\u00e9m do servi\u00e7o User Query, os cat\u00e1logos p\u00fablicos hospedados no LIneA tamb\u00e9m podem ser acessados remotamente por meio do TAP Service no programa TOPCAT ou programaticamente por meio da biblioteca Python pyvo . Acesse a documetata\u00e7\u00e3o do TAP Service no site do User Query para mais detalhes sobre como utilizar o servi\u00e7o.","title":"TAP Service"},{"location":"en/index.html","text":"Welcome to the LIneA's User Documentation page. This is the central hub to find user guides and documentation about all services and tools provided by LIneA for the astronomical community and the general public. Here you will also find links to relevant external documentation. LIneA - Interinstitutional Laboratory for e-Astronomy - is a multi-user laboratory, operated by a non-profit organization (LIneA Association) with financial support primarily from the Brazilian Ministry of Science, Technology, and Innovation. Our mission is to work to support the Brazilian astronomical community with computational infrastructure and expertise in big data analysis to provide the technical conditions for participation in major astronomical surveys, such as the Sloan Digital Sky Survey (SDSS) , Dark Energy Survey (DES) , and Legacy Survey of Space and Time (LSST) . To learn more, check out our institutional videos below, our YouTube channel , or browse our institutional Website . Comments, questions, suggestions? If you find something missing in this documentation, feel free to open an issue in the LIneA Documentation Repository on GitHub .","title":"Home"},{"location":"en/cursos.html","text":"LIneA offers training courses on the main tools used by our projects. Recordings and teaching materials from all previously offered courses are available on Google Classroom. Visit the courses page on LIneA's website for more details.","title":"Courses and Tutorials"},{"location":"en/faq.html","text":"How to create SSH key pair \u00b6 To access our environment via ssh you need to generate a key pair following the steps below: Linux \u00b6 a) To generate the key pair use the following command in your terminal: ssh-keygen -t rsa -b 4096 Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): [press ENTER] Created directory '/root/.ssh'. Enter passphrase (empty for no passphrase): [enter a password and press ENTER] Enter same passphrase again: [repeat password and press ENTER] b) After receiving the message that the key was generated. You can view the two created files by listing the directory contents: ls $HOME/.ssh id_rsa id_rsa.pub c) After generating the keys, send the .pub key to the IT team via email helpdesk@linea.org.br . The LIneA IT team will configure the key on the server and return with login instructions for the Apollo cluster. Wait for confirmation . Windows \u00b6 To generate key pairs on Windows OS: a) Download and install the Putty application. b) Access the installation folder (this example uses Windows 10) C:\\Program File\\PuTTY (path may vary by OS), open Puttygen. c) Click Generate (keep key type as RSA ). NOTE: Moving the mouse pointer helps generate the key faster by creating random bits . d) Key pair successfully generated. Copy the public key to be saved on the server (highlighted in yellow in image); Set a password for the public key (highlighted in blue); After copying, save both public and private keys on your computer (highlighted in red) and send the .pub key to the IT team via email helpdesk@linea.org.br . The LIneA IT team will configure the key on the server. Wait for confirmation . e) After receiving confirmation email that the .pub key was registered on the access server, configure the Putty program. Create a desktop shortcut, open PuTTY ; Enter Hostname: login.linea.org.br. f) On the left side go to SSH > Auth (blue highlight) > click Browse (yellow highlight) and select the key file with .ppk extension . h) If you need to use a tunnel, make the following configuration. NOTE: tunnels are configured according to what the user needs to access Go to Tunnels option (left side); In Source port enter the port number; Destination > enter destination address > Add. Return to the left side and go to the first menu option Session (red highlight) enter session name (yellow highlight) and click Save (blue highlight) , to connect click Open .","title":"Faq"},{"location":"en/faq.html#how-to-create-ssh-key-pair","text":"To access our environment via ssh you need to generate a key pair following the steps below:","title":"How to create SSH key pair"},{"location":"en/faq.html#linux","text":"a) To generate the key pair use the following command in your terminal: ssh-keygen -t rsa -b 4096 Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): [press ENTER] Created directory '/root/.ssh'. Enter passphrase (empty for no passphrase): [enter a password and press ENTER] Enter same passphrase again: [repeat password and press ENTER] b) After receiving the message that the key was generated. You can view the two created files by listing the directory contents: ls $HOME/.ssh id_rsa id_rsa.pub c) After generating the keys, send the .pub key to the IT team via email helpdesk@linea.org.br . The LIneA IT team will configure the key on the server and return with login instructions for the Apollo cluster. Wait for confirmation .","title":"Linux"},{"location":"en/faq.html#windows","text":"To generate key pairs on Windows OS: a) Download and install the Putty application. b) Access the installation folder (this example uses Windows 10) C:\\Program File\\PuTTY (path may vary by OS), open Puttygen. c) Click Generate (keep key type as RSA ). NOTE: Moving the mouse pointer helps generate the key faster by creating random bits . d) Key pair successfully generated. Copy the public key to be saved on the server (highlighted in yellow in image); Set a password for the public key (highlighted in blue); After copying, save both public and private keys on your computer (highlighted in red) and send the .pub key to the IT team via email helpdesk@linea.org.br . The LIneA IT team will configure the key on the server. Wait for confirmation . e) After receiving confirmation email that the .pub key was registered on the access server, configure the Putty program. Create a desktop shortcut, open PuTTY ; Enter Hostname: login.linea.org.br. f) On the left side go to SSH > Auth (blue highlight) > click Browse (yellow highlight) and select the key file with .ppk extension . h) If you need to use a tunnel, make the following configuration. NOTE: tunnels are configured according to what the user needs to access Go to Tunnels option (left side); In Source port enter the port number; Destination > enter destination address > Add. Return to the left side and go to the first menu option Session (red highlight) enter session name (yellow highlight) and click Save (blue highlight) , to connect click Open .","title":"Windows"},{"location":"en/glossario.html","text":"Access the list of technical terms and acronyms available on our website here .","title":"Glossary"},{"location":"en/politicas.html","text":"Information Security Policy (ISP) \u00b6 The Information Security Policy (ISP) of LIneA defines strategic guidelines on how Information Security will be addressed within the association's environment. It was developed by the Information Security Management Committee (CGSI/LIneA) with support from RNP's Security Incident Response Center (CAIS/RNP), and covers various aspects of information security, defining roles and responsibilities for both LIneA Association and its collaborators and users. To access the ISP, click here HPC Environment Use Policy \u00b6 In accordance with the Information Security Policy (ISP), LIneA also establishes guidelines for the use of the high-performance computing environment. The HPC Environment Usage Policy aims to inform collaborators and users about the responsible, efficient, and scientifically-aligned use of the infrastructure. To access the HPC Environment Usage Policy, click here Acknowledgment of LIneA Computing Resources \u00b6 Please commit to recognizing linea in your publications, for example: In english: \u201cThis research was supported by resources supplied by the Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\u201d Or In portuguese: \u201cEsta pesquisa tornou-se poss\u00edvel gra\u00e7as aos recursos computacionais disponibilizados pelo Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\u201d Security Incidents \u00b6 If you suspect a security incident has occurred with your account, service, or application you are using, you must contact LIneA immediately at helpdesk@linea.org.br . We recommend saving any evidence related to the incident (logs, messages, screenshots, etc.) and including as many details as possible in your email to us. Privacy Policy \u00b6 Coming soon Data Treatment Policy \u00b6 Coming soon","title":"Policies"},{"location":"en/politicas.html#information-security-policy-isp","text":"The Information Security Policy (ISP) of LIneA defines strategic guidelines on how Information Security will be addressed within the association's environment. It was developed by the Information Security Management Committee (CGSI/LIneA) with support from RNP's Security Incident Response Center (CAIS/RNP), and covers various aspects of information security, defining roles and responsibilities for both LIneA Association and its collaborators and users. To access the ISP, click here","title":"Information Security Policy (ISP)"},{"location":"en/politicas.html#hpc-environment-use-policy","text":"In accordance with the Information Security Policy (ISP), LIneA also establishes guidelines for the use of the high-performance computing environment. The HPC Environment Usage Policy aims to inform collaborators and users about the responsible, efficient, and scientifically-aligned use of the infrastructure. To access the HPC Environment Usage Policy, click here","title":"HPC Environment Use Policy"},{"location":"en/politicas.html#acknowledgment-of-linea-computing-resources","text":"Please commit to recognizing linea in your publications, for example: In english: \u201cThis research was supported by resources supplied by the Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\u201d Or In portuguese: \u201cEsta pesquisa tornou-se poss\u00edvel gra\u00e7as aos recursos computacionais disponibilizados pelo Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\u201d","title":"Acknowledgment of LIneA Computing Resources"},{"location":"en/politicas.html#security-incidents","text":"If you suspect a security incident has occurred with your account, service, or application you are using, you must contact LIneA immediately at helpdesk@linea.org.br . We recommend saving any evidence related to the incident (logs, messages, screenshots, etc.) and including as many details as possible in your email to us.","title":"Security Incidents"},{"location":"en/politicas.html#privacy-policy","text":"Coming soon","title":"Privacy Policy"},{"location":"en/politicas.html#data-treatment-policy","text":"Coming soon","title":"Data Treatment Policy"},{"location":"en/primeiros_passos.html","text":"To access the platforms and services provided by LIneA, you need to have an active registration in our environment and log in using one of the following options: Institutional Credentials Login - Recommended for those with institutional affiliation (undergraduates, graduate students, researchers, etc.). Login is done via CILogon. Google Login - Recommended for amateur astronomers, high school students, astronomy enthusiasts, etc. Accessible to anyone with a Gmail account. Members of Scientific Collaborations Login with institutional credentials is mandatory to access services exclusive to members of the DES, DESI, LSST, SDSS, and TON collaborations. Don't have an account? \u00b6 To register as a user in LIneA, follow the instructions below: Registration for General Public \u00b6 Public with institutional affiliation: Undergraduates, graduate students, researchers, etc., affiliated with an institution register here . Public without institutional affiliation: Amateur astronomers, high school students, astronomy enthusiasts, etc., register here with Google login . ATTENTION: Public Without Institutional Affiliation You must search for and select Google in the \"Select an Identity Provider\" field on the CILogon homepage. Registration for Collaboration Members \u00b6 Members of the DES, DESI, LSST, SDSS, and TON collaborations must complete the screening form below, filling in all required fields and submitting the form at the end. Registration request for Collaboration members. After reviewing the data, we will contact you via email. For any questions or issues, please contact our Service Desk .","title":"Getting Started"},{"location":"en/primeiros_passos.html#dont-have-an-account","text":"To register as a user in LIneA, follow the instructions below:","title":"Don't have an account?"},{"location":"en/suporte.html","text":"For technical support, contact our Service Desk at helpdesk@linea.org.br . If needed, our team can schedule a video call. If you want to join the LIneA community and chat with other users, click here to receive an invitation to the LIneA Users Slack workspace.","title":"Support"},{"location":"en/armazenamento/index.html","text":"LustreFS (HPC) \u00b6 The Apollo cluster environment features a high-performance Lustre filesystem with two storage tiers: SSD with ~70TB (T0) and HDD with ~500TB (T1), both connected to a 100Gb/s EDR Infiniband network. Both storage tiers are available at /scratch and /data . Scratch area and quota \u00b6 Users will be able to access their Scratch directory through the environment variable, or accessing the directory with the full path. cd $SCRATCH Or cd /scratch/users/<username> ATTENTION There is no backup of /scratch! Files that have not been modified in the last 60 days will be automatically removed, which makes file storage temporary in this area. It is recommended that users will transfer the important $SCRATCH files to their homedir . Warning The cleaning script runs once a week, always on weekends. The standard quota of /scratch available to users entitled to use the cluster is: area bsoft bhard isoft ihard grace period /scratch 100 GB 120 GB 100000 120000 7 days Best Practices \u00b6 Distributed filesystems like Lustre are ideal for HPC and HTC environments. These environments typically handle large files that need to be accessed from many compute nodes with very high bandwidth and/or low latency. Therefore, these filesystems are quite different from those used in desktop computers or standalone servers. While excellent at handling large files, they have strong limitations when dealing with small files and access patterns more commonly found in corporate and desktop environments. Operations that might be extremely fast on a local workstation disk can be painfully slow and expensive on a Lustre filesystem, affecting both the users performing these operations and potentially all other users. These best practices and recommendations aim to enable smooth use of Lustre by minimizing or avoiding unnecessary or very expensive filesystem operations. Avoid accessing file and directory attributes Accessing metadata information like file attributes (e.g., type, ownership, protection, size, dates, etc.) on Lustre consumes significant resources and can degrade filesystem performance, especially when performed frequently or on directories containing many files. Minimize use of system calls that access or modify these attributes, such as stat() , statx() , open() , openat() , etc. The same applies to commands like ls -l or ls --color that use the aforementioned calls. Instead, use a simple ls or ls -l filename . Avoid commands that massively access metadata Avoid using commands like ls -R , find , locate , du , df and similar. These commands recursively traverse the filesystem and/or perform heavy metadata operations. They are very intensive in filesystem metadata access and can severely degrade overall filesystem performance. If you absolutely need to recursively traverse the filesystem, use the Lustre-provided command lfs find instead of find , for example. Use the Lustre lfs command To minimize the number of Lustre RPC calls, whenever possible use lfs commands instead of system-provided ones: lfs df => instead of df lfs find => instead of find Avoid using wildcards Wildcard expansion is resource-intensive. Running commands with wildcards on a large number of files can take a long time and severely impact filesystem performance. Instead of using wildcards, create a list of target files and apply the command to each file. Read-only access Whenever possible, open files as read-only using O_RDONLY , and if you don't need to update file access time, open files as O_RDONLY | O_NOATIME . If access time information is needed during parallel I/O, have the parent process open files as O_RDONLY and all other ranks open the same files as O_RDONLY|O_NOATIME . Avoid having many files in a single directory When a file is accessed, Lustre locks its parent directory. When many files in the same directory need to be opened, this creates contention. Writing thousands of files to a single directory produces massive load on Lustre metadata servers, often resulting in filesystem unavailability. Accessing a single directory containing thousands of files can cause significant resource contention, degrading filesystem performance. The alternative is to organize data into multiple subdirectories and distribute files among them. A common approach is to use the square root of the number of files - for 90,000 files the square root would be 300, so create 300 directories containing 300 files each. Avoid small files Accessing small files on Lustre is very inefficient. The recommended file size is above 1GB. Reorganize data into large files or use file formats like HDF5 . Alternatively, if the total file size is small (a few GB), copy small files to /tmp or to a local temporary directory on each compute node at job start (remember to transfer and/or delete files at the end). This approach can be combined with archiving tools like tar - storing small files in one or more large tarballs can be maintained on Lustre more efficiently. When reading or writing files, Lustre performs much better with large buffer sizes (>= 1MB). It's highly recommended to aggregate small read/write operations into larger ones. MPI-IO collective buffering allows aggregated I/O. Avoid repetitive small file operations Avoid performing repetitive small I/O operations like frequently opening files in append mode, writing small amounts of data, and closing the file. Instead, open the file once, perform all I/O operations, then close. Avoid multiple processes opening the same files simultaneously Multiple processes opening the same files simultaneously can create contention and file opening errors. Instead, perform the opening from a single (parent) process, or open the file read-only to avoid locking, or implement opening with a try-and-error approach with sleep on error. Avoid multiple processes accessing the same file region If multiple processes access the same file region simultaneously, the Lustre distributed lock manager will enforce coherence so all clients see consistent results. Having many processes trying to access the same file region simultaneously can cause performance degradation. In this case, it may be preferable to: replicate the file, split the file, perform I/O operations from a single process rank, or ensure simultaneous access won't occur. In any case, it's recommended to keep the amount of parallel file opening and locking operations as small as possible to reduce contention. If multiple processes try to append to the same file, this will trigger locking and may cause significant contention. Ideally, only one process should append to each file. File operations through parent process When accessing small shared files in a parallel task, it's often more efficient to perform all required operations through the parent process and, if needed, broadcast the data to other ranks, rather than accessing the same files from all ranks. Similarly, if multiple ranks of a parallel job require information about a particular file, the most efficient approach is to have the parent process perform the necessary calls (e.g. stat() , fstat() , etc) and then broadcast the information to other ranks. File striping In Lustre, large files can be split into segments that can be automatically distributed across multiple storage devices. File striping is useful for parallel I/O on large files. For this to work, the mount point in question must point to multiple storage devices (OSTs). The lfs df command can be used to verify if a given mount point points to multiple OSTs. To get striping information for a particular file, use: lfs getstripe filename File striping can be set using the lfs setstripe command. If applied to a directory, it will set default striping settings for files created in that directory. A subdirectory inherits all striping settings from its parent directory. If applied to a file, it will stripe that file across OSTs according to the specified settings. lfs setstripe -s 128m -c 8 filename => splits the file into 128MB segments and distributes them across 8 OSTs If a large file is shared in parallel by multiple processes, with each process working on its own part of the file, then it may be useful to stripe the file into a number of segments equal to the number of processes, or a multiple of the number of processes. For maximum performance, I/O requests should be aligned to stripes, meaning processes accessing the file should do so at offsets matching stripe boundaries. This minimizes the chances of a process needing to access more than one stripe (and more than one OST) to get required data. For small files, striping should be disabled by setting a stripe count of 1. The same applies if a large file is accessed by a single process. lfs setstripe -s 1m -c 1 mydir/smallfiles/ Avoid installing software on Lustre Software typically consists of many small files, and as mentioned earlier, accessing many small files on Lustre can overload metadata servers. Software compilations in particular are better performed locally by copying or unpacking the software to /tmp/$USER/ or to your homedir . Additionally, under high load, I/O access to Lustre filesystems may be blocked. If executables are stored on Lustre and filesystem access fails, executables may crash. Therefore, whenever possible, it's better to copy executables to cluster nodes' /tmp . Scripts area \u00b6 Users will be able to access their script directory through the environment variable, or accessing the directory with the full path. cd $SCRIPTS Or cd /scripts/<username> This area is intended for the storage of Jobs submission scripts to the cluster and others. It is also recommended to use this path to the creation of Pynton environments (ENVs) and kernels. The standard quota of /scripts available to users is: area bsoft bhard isoft ihard grace period /scripts 10 GB 12 GB 100k 120k 7 days Note: The /scripts is not affected by the automatic cleaning process. Homedir \u00b6 The Home directory is an area for users to store their personal files and is accessible through the cluster login nodes and also on the Jupyter platform. The standard quota of homedir available to users, according to their profile, is: profile bsoft bhard isoft ihard grace period public general 5 GB 7 GB 7000 10000 7 days public institucional 25 GB 30 GB 40000 50000 7 days collaboration 100 GB 120 GB 1000000 1200000 7 days Tip To check the quota values \u200b\u200bconfigured simply use the command: quota -s -u <username> /home . Note: The /home directory is not affected by the automatic cleaning process. Useful Commands \u00b6 a) How to check my available quota? show_quota b) How to find my files created more than 60 days ago? lfs find $SCRATCH --uid $UID -mtime +60 --print c) How to find my files created less than 60 days ago? lfs find $SCRATCH --uid $UID -mtime -60 --print d) How to list Lustre OSTs? lfs osts $SCRATCH e) How to list files older than 60 days on a specific Lustre OST? lfs find $SCRATCH -mtime +60 --print --obd t0-OST0002_UUID f) How to configure striping on a directory to \"split\" files and distribute \"chunks\" across 10 OSTs? lfs setstripe -c 10 $SCRATCH/my_large_files g) How to check file/directory striping? lfs getstripe $SCRATCH/my_large_files Tip LIneA's Lustre is designed to work at 100Gbps - to achieve maximum performance use striping and always with large files (+1GB). NAS (NFS) \u00b6 NAS storage systems are used for long-term storage and are not accessible through compute nodes (HPC). Current specifications: Manufacturer Model Capacity Installed SGI IS5500 [1] 540TB Dec-2011 SGI IS5600 240TB Jul-2014 [1] this equipment was decommissioned in Jun/2023 due to physical issues. Backup \u00b6 areas frequency type retention /home daily incremental 30 days /home weekly differential 30 days /home monthly full 90 days /archive - - - /scratch - - - /scripts - - - References \u00b6 These best practices were compiled from LIneA team experience and the following sources: https://www.nas.nasa.gov/hecc/support/kb/lustre-best-practices_226.html https://hpcf.umbc.edu/general-productivity/lustre-best-practices/ https://wiki.gsi.de/foswiki/bin/view/Linux/LustreFs https://doc.lustre.org/lustre_manual.pdf","title":"Storage"},{"location":"en/armazenamento/index.html#lustrefs-hpc","text":"The Apollo cluster environment features a high-performance Lustre filesystem with two storage tiers: SSD with ~70TB (T0) and HDD with ~500TB (T1), both connected to a 100Gb/s EDR Infiniband network. Both storage tiers are available at /scratch and /data .","title":"LustreFS (HPC)"},{"location":"en/armazenamento/index.html#scratch-area-and-quota","text":"Users will be able to access their Scratch directory through the environment variable, or accessing the directory with the full path. cd $SCRATCH Or cd /scratch/users/<username> ATTENTION There is no backup of /scratch! Files that have not been modified in the last 60 days will be automatically removed, which makes file storage temporary in this area. It is recommended that users will transfer the important $SCRATCH files to their homedir . Warning The cleaning script runs once a week, always on weekends. The standard quota of /scratch available to users entitled to use the cluster is: area bsoft bhard isoft ihard grace period /scratch 100 GB 120 GB 100000 120000 7 days","title":"Scratch area and quota"},{"location":"en/armazenamento/index.html#best-practices","text":"Distributed filesystems like Lustre are ideal for HPC and HTC environments. These environments typically handle large files that need to be accessed from many compute nodes with very high bandwidth and/or low latency. Therefore, these filesystems are quite different from those used in desktop computers or standalone servers. While excellent at handling large files, they have strong limitations when dealing with small files and access patterns more commonly found in corporate and desktop environments. Operations that might be extremely fast on a local workstation disk can be painfully slow and expensive on a Lustre filesystem, affecting both the users performing these operations and potentially all other users. These best practices and recommendations aim to enable smooth use of Lustre by minimizing or avoiding unnecessary or very expensive filesystem operations. Avoid accessing file and directory attributes Accessing metadata information like file attributes (e.g., type, ownership, protection, size, dates, etc.) on Lustre consumes significant resources and can degrade filesystem performance, especially when performed frequently or on directories containing many files. Minimize use of system calls that access or modify these attributes, such as stat() , statx() , open() , openat() , etc. The same applies to commands like ls -l or ls --color that use the aforementioned calls. Instead, use a simple ls or ls -l filename . Avoid commands that massively access metadata Avoid using commands like ls -R , find , locate , du , df and similar. These commands recursively traverse the filesystem and/or perform heavy metadata operations. They are very intensive in filesystem metadata access and can severely degrade overall filesystem performance. If you absolutely need to recursively traverse the filesystem, use the Lustre-provided command lfs find instead of find , for example. Use the Lustre lfs command To minimize the number of Lustre RPC calls, whenever possible use lfs commands instead of system-provided ones: lfs df => instead of df lfs find => instead of find Avoid using wildcards Wildcard expansion is resource-intensive. Running commands with wildcards on a large number of files can take a long time and severely impact filesystem performance. Instead of using wildcards, create a list of target files and apply the command to each file. Read-only access Whenever possible, open files as read-only using O_RDONLY , and if you don't need to update file access time, open files as O_RDONLY | O_NOATIME . If access time information is needed during parallel I/O, have the parent process open files as O_RDONLY and all other ranks open the same files as O_RDONLY|O_NOATIME . Avoid having many files in a single directory When a file is accessed, Lustre locks its parent directory. When many files in the same directory need to be opened, this creates contention. Writing thousands of files to a single directory produces massive load on Lustre metadata servers, often resulting in filesystem unavailability. Accessing a single directory containing thousands of files can cause significant resource contention, degrading filesystem performance. The alternative is to organize data into multiple subdirectories and distribute files among them. A common approach is to use the square root of the number of files - for 90,000 files the square root would be 300, so create 300 directories containing 300 files each. Avoid small files Accessing small files on Lustre is very inefficient. The recommended file size is above 1GB. Reorganize data into large files or use file formats like HDF5 . Alternatively, if the total file size is small (a few GB), copy small files to /tmp or to a local temporary directory on each compute node at job start (remember to transfer and/or delete files at the end). This approach can be combined with archiving tools like tar - storing small files in one or more large tarballs can be maintained on Lustre more efficiently. When reading or writing files, Lustre performs much better with large buffer sizes (>= 1MB). It's highly recommended to aggregate small read/write operations into larger ones. MPI-IO collective buffering allows aggregated I/O. Avoid repetitive small file operations Avoid performing repetitive small I/O operations like frequently opening files in append mode, writing small amounts of data, and closing the file. Instead, open the file once, perform all I/O operations, then close. Avoid multiple processes opening the same files simultaneously Multiple processes opening the same files simultaneously can create contention and file opening errors. Instead, perform the opening from a single (parent) process, or open the file read-only to avoid locking, or implement opening with a try-and-error approach with sleep on error. Avoid multiple processes accessing the same file region If multiple processes access the same file region simultaneously, the Lustre distributed lock manager will enforce coherence so all clients see consistent results. Having many processes trying to access the same file region simultaneously can cause performance degradation. In this case, it may be preferable to: replicate the file, split the file, perform I/O operations from a single process rank, or ensure simultaneous access won't occur. In any case, it's recommended to keep the amount of parallel file opening and locking operations as small as possible to reduce contention. If multiple processes try to append to the same file, this will trigger locking and may cause significant contention. Ideally, only one process should append to each file. File operations through parent process When accessing small shared files in a parallel task, it's often more efficient to perform all required operations through the parent process and, if needed, broadcast the data to other ranks, rather than accessing the same files from all ranks. Similarly, if multiple ranks of a parallel job require information about a particular file, the most efficient approach is to have the parent process perform the necessary calls (e.g. stat() , fstat() , etc) and then broadcast the information to other ranks. File striping In Lustre, large files can be split into segments that can be automatically distributed across multiple storage devices. File striping is useful for parallel I/O on large files. For this to work, the mount point in question must point to multiple storage devices (OSTs). The lfs df command can be used to verify if a given mount point points to multiple OSTs. To get striping information for a particular file, use: lfs getstripe filename File striping can be set using the lfs setstripe command. If applied to a directory, it will set default striping settings for files created in that directory. A subdirectory inherits all striping settings from its parent directory. If applied to a file, it will stripe that file across OSTs according to the specified settings. lfs setstripe -s 128m -c 8 filename => splits the file into 128MB segments and distributes them across 8 OSTs If a large file is shared in parallel by multiple processes, with each process working on its own part of the file, then it may be useful to stripe the file into a number of segments equal to the number of processes, or a multiple of the number of processes. For maximum performance, I/O requests should be aligned to stripes, meaning processes accessing the file should do so at offsets matching stripe boundaries. This minimizes the chances of a process needing to access more than one stripe (and more than one OST) to get required data. For small files, striping should be disabled by setting a stripe count of 1. The same applies if a large file is accessed by a single process. lfs setstripe -s 1m -c 1 mydir/smallfiles/ Avoid installing software on Lustre Software typically consists of many small files, and as mentioned earlier, accessing many small files on Lustre can overload metadata servers. Software compilations in particular are better performed locally by copying or unpacking the software to /tmp/$USER/ or to your homedir . Additionally, under high load, I/O access to Lustre filesystems may be blocked. If executables are stored on Lustre and filesystem access fails, executables may crash. Therefore, whenever possible, it's better to copy executables to cluster nodes' /tmp .","title":"Best Practices"},{"location":"en/armazenamento/index.html#scripts-area","text":"Users will be able to access their script directory through the environment variable, or accessing the directory with the full path. cd $SCRIPTS Or cd /scripts/<username> This area is intended for the storage of Jobs submission scripts to the cluster and others. It is also recommended to use this path to the creation of Pynton environments (ENVs) and kernels. The standard quota of /scripts available to users is: area bsoft bhard isoft ihard grace period /scripts 10 GB 12 GB 100k 120k 7 days Note: The /scripts is not affected by the automatic cleaning process.","title":"Scripts area"},{"location":"en/armazenamento/index.html#homedir","text":"The Home directory is an area for users to store their personal files and is accessible through the cluster login nodes and also on the Jupyter platform. The standard quota of homedir available to users, according to their profile, is: profile bsoft bhard isoft ihard grace period public general 5 GB 7 GB 7000 10000 7 days public institucional 25 GB 30 GB 40000 50000 7 days collaboration 100 GB 120 GB 1000000 1200000 7 days Tip To check the quota values \u200b\u200bconfigured simply use the command: quota -s -u <username> /home . Note: The /home directory is not affected by the automatic cleaning process.","title":"Homedir"},{"location":"en/armazenamento/index.html#useful-commands","text":"a) How to check my available quota? show_quota b) How to find my files created more than 60 days ago? lfs find $SCRATCH --uid $UID -mtime +60 --print c) How to find my files created less than 60 days ago? lfs find $SCRATCH --uid $UID -mtime -60 --print d) How to list Lustre OSTs? lfs osts $SCRATCH e) How to list files older than 60 days on a specific Lustre OST? lfs find $SCRATCH -mtime +60 --print --obd t0-OST0002_UUID f) How to configure striping on a directory to \"split\" files and distribute \"chunks\" across 10 OSTs? lfs setstripe -c 10 $SCRATCH/my_large_files g) How to check file/directory striping? lfs getstripe $SCRATCH/my_large_files Tip LIneA's Lustre is designed to work at 100Gbps - to achieve maximum performance use striping and always with large files (+1GB).","title":"Useful Commands"},{"location":"en/armazenamento/index.html#nas-nfs","text":"NAS storage systems are used for long-term storage and are not accessible through compute nodes (HPC). Current specifications: Manufacturer Model Capacity Installed SGI IS5500 [1] 540TB Dec-2011 SGI IS5600 240TB Jul-2014 [1] this equipment was decommissioned in Jun/2023 due to physical issues.","title":"NAS (NFS)"},{"location":"en/armazenamento/index.html#backup","text":"areas frequency type retention /home daily incremental 30 days /home weekly differential 30 days /home monthly full 90 days /archive - - - /scratch - - - /scripts - - -","title":"Backup"},{"location":"en/armazenamento/index.html#references","text":"These best practices were compiled from LIneA team experience and the following sources: https://www.nas.nasa.gov/hecc/support/kb/lustre-best-practices_226.html https://hpcf.umbc.edu/general-productivity/lustre-best-practices/ https://wiki.gsi.de/foswiki/bin/view/Linux/LustreFs https://doc.lustre.org/lustre_manual.pdf","title":"References"},{"location":"en/data/index.html","text":"Data Repository \u00b6 Catalogs \u00b6 Relevant information about public astronomical catalogs hosted at LIneA is available on the User Query service documentation pages. To access them, click on the \"Database Tables\" menu and select the desired dataset. Images \u00b6 Public images hosted at LIneA and available through visualization services belong to the DES DR2 release. For more information, see the publication The Dark Energy Survey Data Release 2 . Photo-z Data Products \u00b6 Public Photo-z Data Products \u00b6 (soon) Private LSST Photo-z Data Products \u00b6 LSST Photo-z Server Data Products Documentation","title":"Data"},{"location":"en/data/index.html#data-repository","text":"","title":"Data Repository"},{"location":"en/data/index.html#catalogs","text":"Relevant information about public astronomical catalogs hosted at LIneA is available on the User Query service documentation pages. To access them, click on the \"Database Tables\" menu and select the desired dataset.","title":"Catalogs"},{"location":"en/data/index.html#images","text":"Public images hosted at LIneA and available through visualization services belong to the DES DR2 release. For more information, see the publication The Dark Energy Survey Data Release 2 .","title":"Images"},{"location":"en/data/index.html#photo-z-data-products","text":"","title":"Photo-z Data Products"},{"location":"en/data/index.html#public-photo-z-data-products","text":"(soon)","title":"Public Photo-z Data Products"},{"location":"en/data/index.html#private-lsst-photo-z-data-products","text":"LSST Photo-z Server Data Products Documentation","title":"Private LSST Photo-z Data Products"},{"location":"en/data/backup_text.html","text":"Data product Type Confidence Matches Reference 2dFGRS SITCOMTN-154 s 1.00 3 Colless et al. (2001) 0.99 4 0.90\u2217 1 2dflens SITCOMTN-154 s 1.00 1 Blake et al. (2016) 2MRS SITCOMTN-154 s 0.95 1 Huchra et al. (2012) 6dFGRS SITCOMTN-154 s 0.98 2 Jones et al. (2009) 3D-HSTs SITCOMTN-154 g 0.99 5 Momcheva et al. (2016) 0.95 277 ASTRODEEP SITCOMTN-154 s 1.00 4165 Merlin et al. (2021) p 0.97 8212 ASTRODEEP-JWST SITCOMTN-154 s 1.00 594 Merlin et al. (2024) p 0.92\u2217 628 0.90\u2217 455 CANDELS SITCOMTN-154 s 1.00 53 Kodra et al. (2023) p 0.93\u2217 6 JADES SITCOMTN-154 s 0.99 11 D\u2019Eugenio et al. (2025) 0.95 34 0.90\u2217 24 MOSDEF SITCOMTN-154 s 0.99 9 Kriek et al. (2015) NED SITCOMTN-154 s 0.95 847 Helou et al. (1991) OzDES SITCOMTN-154 s 0.99 897 Lidman et al. (2020) PRIMUS SITCOMTN-154 g 0.92\u2217 3653 Cool et al. (2013) 0.85\u2217 1687 VANDELS SITCOMTN-154 s 1.00 196 Garilli et al. (2021) VIMOS SITCOMTN-154 s 1.00 499 Balestra et al. (2010) 0.95 43 VUDS SITCOMTN-154 s 1.00 9 Tasca et al. (2017) 0.95 9 0.80\u2217 3 VVDS SITCOMTN-154 s 1.00 101 Le F\u00e8vre et al. (2005) 0.95 193","title":"Backup text"},{"location":"en/data/pz_server_data.html","text":"Photo-z Server Data \u00b6 Back to Photo-z Server documentation Datasets classification \u00b6 Official Datasets The official datasets will be produced by Rubin's Data Management team and will be available to the LSST Community through the Photo-z Server. These datasets will include reference redshift catalogs, training sets, and photo-z estimates associated with the LSST data releases. The datasets will be released in a phased manner, starting with the first data release (DR1) and continuing with subsequent releases as the LSST survey progresses. For now, the Rubin Observatory PZ Data Products page is empty. Unofficial Datasets Besides data uploaded by users, the User-generated Data Products page hosts set of datasets that were prepared by the LIneA team for educational purposes, to serve as use case examples for the Photo-z Server tutorials. In addition, the DP1 datasets generated by PZ Science Unit from Rubin\u2019s Commissioning Team, described in the tech note SITCOMTN-154 are also available there. All these datasets are not classified as Official Datasets. Data Preview 1 \u00b6 ATTENTION: Preliminary Datasets These datasets were produced by the PZ Science Unit \u2014 a working group from Rubin\u2019s Commissioning Team \u2014 during the Initial studies of photometric redshifts with LSSTComCam from DP1 . All results, along with detailed dataset descriptions, are available in the tech note SITCOMTN-154 . These datasets are NOT classified as Official Datasets released by Rubin's DM team. The datasets described in the tech note are available in the Photo-z Server as data products with SITCOMTN-154 suffix. Their links and short description are organized in product types below. Object Catalogs \u00b6 Data products containing object tables described in Section 2.1 and listed in Table 1 from the tech note SITCOMTN-154. Data Product Data set Selection Number of objects DP1 (available in the RSP) Complete DP1 Object Catalog None 2,299,757 ECDFS+EDFS+SV_95 gold SITCOMTN-154 ECDFS+EDFS+SV_95 gold 375,610 SV_38 gold_4_band SITCOMTN-154 SV_38 gold_4_band 169,034 All DP1 fields comprehensive Gold dataset beyond the fields ECDFS+EDFS+SV_95 and SV_38, where spectroscopic data are available (dataset not listed in Table 1 from the tech note SITCOMTN-154): Data Product Data set Selection Number of objects DP1 Gold all SITCOMTN-154 All fields gold 686,334 Reference Redshift Catalogs \u00b6 Reference Redshift Catalogs from Individual Surveys \u00b6 Data products containing reference redshifts catalogs (before matching with DP1 Object table) separated by the origin survey as listed in Table 2 from the tech note SITCOMTN-154. Note: These datasets are already filtered to the ECDFS field and cleaned with the selection criteria described in Section 2.2.1 from the tech note SITCOMTN-154. For the complete original catalogs, please go to the External datasets section below. Data Product Reference Number of Redshifts 2dFGRS SITCOMTN-154 Colless et al. (2001) 278 2dflens SITCOMTN-154 Blake et al. (2016) 1 2MRS SITCOMTN-154 Huchra et al. (2012) 9 3D-HST SITCOMTN-154 Momcheva et al. (2016) 4237 6dFGS SITCOMTN-154 Jones et al. (2009) 5 astrodeep SITCOMTN-154 Merlin et al. (2021) 35108 astrodeep_jwst SITCOMTN-154 Merlin et al. (2024) 76748 CANDELS SITCOMTN-154 Kodra et al. (2023) 31440 JADES SITCOMTN-154 D\u2019Eugenio et al. (2025) 1224 MOSDEF SITCOMTN-154 Kriek et al. (2015) 51 NED SITCOMTN-154 Helou et al. (1991) 3770 OzDES SITCOMTN-154 Lidman et al. (2020) 1147 PRIMUS SITCOMTN-154 Cool et al. (2013) 7352 VANDELS SITCOMTN-154 Garilli et al. (2021) 450 VIMOS SITCOMTN-154 Balestra et al. (2010) 1578 VUDS SITCOMTN-154 Tasca et al. (2017) 160 VVDS SITCOMTN-154 Le F\u00e8vre et al. (2005) 669 Additional spectroscopic data from DESI DR1 were used as an independent test set for validating the photo-z estimates. Since DESI DR1 is a very large dataset that extends beyond the DP1 footprint, it was filtered to include only the ECDFS field. Data Product Reference Number of Redshifts DESI DR1 inside DP1 footprint DESI Collaboration et al. (2025) 50,634 Combined Redshift Catalog \u00b6 A single file containing all reference redshifts combined from the individual surveys listed above (excluding DESI), as described in Section 2.2.1 of the technical note SITCOMTN-154. Data Product Number of Redshifts ComCam ECDFS z catalog SITCOMTN-154 104,070 Training and Test Sets \u00b6 On the Photo-z Server, the product type \"Training Set\" comprehends all samples resulting from the matching between a reference redshift and an object catalog. That might include training and test sets together in a same file or independent sub-samples uploaded separately. For the latter, both training and test sets are tagged as \"Training Set\". Data products containing training and test sets generated from the ComCam ECDFS z catalog listed in Table 1 from the tech note SITCOMTN-154 are: Data Product Data set Selection Number of objects training_v1 match_prelim SITCOMTN-154 training_v1 match_prelim 7,000 test_v1 match_prelim SITCOMTN-154 test_v1 match_prelim 2,437 training_v4 match_ecdfs SITCOMTN-154 training_v4 match_ecdfs 6,778 test_v4 match_ecdfs SITCOMTN-154 test_v4 match_ecdfs 2,905 test_DESI match_desi SITCOMTN-154 test_DESI match_desi 2,728 Training Results \u00b6 Estimator data models listed in Table 7 and described in the Appendix A.3 from the tech note SITCOMTN-154. Gold baseline Gold DP1 optimize Gold DP1 optimize 4band BPZ model gold baseline SITCOMTN-154 BPZ model gold DP1 optimize SITCOMTN-154 BPZ model gold DP1 optimize 4band SITCOMTN-154 CMNN model gold baseline SITCOMTN-154 CMNN model gold DP1 optimize SITCOMTN-154 CMNN model gold DP1 optimize 4band SITCOMTN-154 DNF model gold baseline SITCOMTN-154 DNF model gold DP1 optimize SITCOMTN-154 DNF model gold DP1 optimize 4band SITCOMTN-154 FlexZBoost model gold baseline SITCOMTN-154 FlexZBoost model gold DP1 optimize SITCOMTN-154 FlexZBoost model gold DP1 optimize 4band SITCOMTN-154 GPz model gold baseline SITCOMTN-154 GPz model gold DP1 optimize SITCOMTN-154 GPz model gold DP1 optimize 4band SITCOMTN-154 kNN model gold baseline SITCOMTN-154 kNN model gold DP1 optimize SITCOMTN-154 kNN model gold DP1 optimize 4band SITCOMTN-154 N/A LePhare model gold DP1 optimize SITCOMTN-154 LePhare model gold DP1 optimize 4band SITCOMTN-154 TPZ model gold baseline SITCOMTN-154 TPZ model gold DP1 optimize SITCOMTN-154 TPZ model gold DP1 optimize 4band SITCOMTN-154 Configuration Files As mentioned in Section 3.4 and Appendix A.1 from the tech note SITCOMTN-154, the configuration files dp1.yaml (complete set of configurations tested, labeled as analysis flavors ) and dp1_v4.yaml (optimized configuration parameters) are available on the GitHub repository rail_project_config . Validation Results \u00b6 Photo-z point estimates, QP Ensaembles, and evaluation metrics related to the results shown in Table 4. Files uploaded from directories listed in Table 7 and described in the Appendix A.4 from the tech note SITCOMTN-154. Gold baseline Gold DP1 optimize Gold DP1 optimize 4band DESI test set BPZ test gold baseline SITCOMTN-154 BPZ test gold DP1 optimize SITCOMTN-154 \u23f3 \u23f3 CMNN test gold baseline SITCOMTN-154 CMNN test gold DP1 optimize SITCOMTN-154 \u23f3 \u23f3 DNF test gold baseline SITCOMTN-154 DNF test gold DP1 optimize SITCOMTN-154 \u23f3 \u23f3 FlexZBoost test gold baseline SITCOMTN-154 FlexZBoost test gold DP1 optimize SITCOMTN-154 \u23f3 \u23f3 GPz test gold baseline SITCOMTN-154 GPz test gold DP1 optimize SITCOMTN-154 \u23f3 \u23f3 kNN test gold baseline SITCOMTN-154 kNN test gold DP1 optimize SITCOMTN-154 \u23f3 \u23f3 N/A LePhare test gold DP1 optimize SITCOMTN-154 \u23f3 \u23f3 TPZ test gold baseline SITCOMTN-154 TPZ test gold DP1 optimize SITCOMTN-154 \u23f3 \u23f3 Photo-z Estimates \u00b6 Photo-z Tables \u00b6 PZ tables produced as part of the initial studies with commissioning data described in SITCOMTN-154. Data uploaded from the directories listed in Table 7. For more datasets from the tech note, visit https://docs.linea.org.br/en/data/pz_server_data.html Data Product Number of objects PZ table dp1_all gold baseline SITCOMTN-154 686,334 PZ table dp1_all gold dp1_optimize \u23f3 PZ table dp1_all gold dp1_optimize 4band \u23f3 PZ table dp1_sv38 gold baseline SITCOMTN-154 169,034 PZ table dp1_sv_38 gold dp1_optimize \u23f3 PZ table dp1_sv_38 gold dp1_optimize 4band \u23f3 Data Product Number of objects PZ table DESI gold baseline 2728 QP Ensembles \u00b6 QP Ensables for the photometric sets of DP1 are not lightweight files, so they are not uploaded to the Photo-z Server. However, they are distributed via LSDB, USDF, and NERSC, as described in Appendix B.1, B.3, and B.4 respectively. LSDB.io at LIneA Large datasets including photo-z results will be made available at LSDB.io at LIneA service soon. External datasets \u00b6 Public data collected from the literature and hosted on the Photo-z Server. ATTENTION: External Datasets \u23f3 Documentation in preparation. Back to Photo-z Server documentation","title":"Photo-z Server Data"},{"location":"en/data/pz_server_data.html#photo-z-server-data","text":"Back to Photo-z Server documentation","title":"Photo-z Server Data"},{"location":"en/data/pz_server_data.html#datasets-classification","text":"Official Datasets The official datasets will be produced by Rubin's Data Management team and will be available to the LSST Community through the Photo-z Server. These datasets will include reference redshift catalogs, training sets, and photo-z estimates associated with the LSST data releases. The datasets will be released in a phased manner, starting with the first data release (DR1) and continuing with subsequent releases as the LSST survey progresses. For now, the Rubin Observatory PZ Data Products page is empty. Unofficial Datasets Besides data uploaded by users, the User-generated Data Products page hosts set of datasets that were prepared by the LIneA team for educational purposes, to serve as use case examples for the Photo-z Server tutorials. In addition, the DP1 datasets generated by PZ Science Unit from Rubin\u2019s Commissioning Team, described in the tech note SITCOMTN-154 are also available there. All these datasets are not classified as Official Datasets.","title":"Datasets classification"},{"location":"en/data/pz_server_data.html#data-preview-1","text":"ATTENTION: Preliminary Datasets These datasets were produced by the PZ Science Unit \u2014 a working group from Rubin\u2019s Commissioning Team \u2014 during the Initial studies of photometric redshifts with LSSTComCam from DP1 . All results, along with detailed dataset descriptions, are available in the tech note SITCOMTN-154 . These datasets are NOT classified as Official Datasets released by Rubin's DM team. The datasets described in the tech note are available in the Photo-z Server as data products with SITCOMTN-154 suffix. Their links and short description are organized in product types below.","title":"Data Preview 1"},{"location":"en/data/pz_server_data.html#object-catalogs","text":"Data products containing object tables described in Section 2.1 and listed in Table 1 from the tech note SITCOMTN-154. Data Product Data set Selection Number of objects DP1 (available in the RSP) Complete DP1 Object Catalog None 2,299,757 ECDFS+EDFS+SV_95 gold SITCOMTN-154 ECDFS+EDFS+SV_95 gold 375,610 SV_38 gold_4_band SITCOMTN-154 SV_38 gold_4_band 169,034 All DP1 fields comprehensive Gold dataset beyond the fields ECDFS+EDFS+SV_95 and SV_38, where spectroscopic data are available (dataset not listed in Table 1 from the tech note SITCOMTN-154): Data Product Data set Selection Number of objects DP1 Gold all SITCOMTN-154 All fields gold 686,334","title":"Object Catalogs"},{"location":"en/data/pz_server_data.html#reference-redshift-catalogs","text":"","title":"Reference Redshift Catalogs"},{"location":"en/data/pz_server_data.html#training-and-test-sets","text":"On the Photo-z Server, the product type \"Training Set\" comprehends all samples resulting from the matching between a reference redshift and an object catalog. That might include training and test sets together in a same file or independent sub-samples uploaded separately. For the latter, both training and test sets are tagged as \"Training Set\". Data products containing training and test sets generated from the ComCam ECDFS z catalog listed in Table 1 from the tech note SITCOMTN-154 are: Data Product Data set Selection Number of objects training_v1 match_prelim SITCOMTN-154 training_v1 match_prelim 7,000 test_v1 match_prelim SITCOMTN-154 test_v1 match_prelim 2,437 training_v4 match_ecdfs SITCOMTN-154 training_v4 match_ecdfs 6,778 test_v4 match_ecdfs SITCOMTN-154 test_v4 match_ecdfs 2,905 test_DESI match_desi SITCOMTN-154 test_DESI match_desi 2,728","title":"Training and Test Sets"},{"location":"en/data/pz_server_data.html#training-results","text":"Estimator data models listed in Table 7 and described in the Appendix A.3 from the tech note SITCOMTN-154. Gold baseline Gold DP1 optimize Gold DP1 optimize 4band BPZ model gold baseline SITCOMTN-154 BPZ model gold DP1 optimize SITCOMTN-154 BPZ model gold DP1 optimize 4band SITCOMTN-154 CMNN model gold baseline SITCOMTN-154 CMNN model gold DP1 optimize SITCOMTN-154 CMNN model gold DP1 optimize 4band SITCOMTN-154 DNF model gold baseline SITCOMTN-154 DNF model gold DP1 optimize SITCOMTN-154 DNF model gold DP1 optimize 4band SITCOMTN-154 FlexZBoost model gold baseline SITCOMTN-154 FlexZBoost model gold DP1 optimize SITCOMTN-154 FlexZBoost model gold DP1 optimize 4band SITCOMTN-154 GPz model gold baseline SITCOMTN-154 GPz model gold DP1 optimize SITCOMTN-154 GPz model gold DP1 optimize 4band SITCOMTN-154 kNN model gold baseline SITCOMTN-154 kNN model gold DP1 optimize SITCOMTN-154 kNN model gold DP1 optimize 4band SITCOMTN-154 N/A LePhare model gold DP1 optimize SITCOMTN-154 LePhare model gold DP1 optimize 4band SITCOMTN-154 TPZ model gold baseline SITCOMTN-154 TPZ model gold DP1 optimize SITCOMTN-154 TPZ model gold DP1 optimize 4band SITCOMTN-154 Configuration Files As mentioned in Section 3.4 and Appendix A.1 from the tech note SITCOMTN-154, the configuration files dp1.yaml (complete set of configurations tested, labeled as analysis flavors ) and dp1_v4.yaml (optimized configuration parameters) are available on the GitHub repository rail_project_config .","title":"Training Results"},{"location":"en/data/pz_server_data.html#validation-results","text":"Photo-z point estimates, QP Ensaembles, and evaluation metrics related to the results shown in Table 4. Files uploaded from directories listed in Table 7 and described in the Appendix A.4 from the tech note SITCOMTN-154. Gold baseline Gold DP1 optimize Gold DP1 optimize 4band DESI test set BPZ test gold baseline SITCOMTN-154 BPZ test gold DP1 optimize SITCOMTN-154 \u23f3 \u23f3 CMNN test gold baseline SITCOMTN-154 CMNN test gold DP1 optimize SITCOMTN-154 \u23f3 \u23f3 DNF test gold baseline SITCOMTN-154 DNF test gold DP1 optimize SITCOMTN-154 \u23f3 \u23f3 FlexZBoost test gold baseline SITCOMTN-154 FlexZBoost test gold DP1 optimize SITCOMTN-154 \u23f3 \u23f3 GPz test gold baseline SITCOMTN-154 GPz test gold DP1 optimize SITCOMTN-154 \u23f3 \u23f3 kNN test gold baseline SITCOMTN-154 kNN test gold DP1 optimize SITCOMTN-154 \u23f3 \u23f3 N/A LePhare test gold DP1 optimize SITCOMTN-154 \u23f3 \u23f3 TPZ test gold baseline SITCOMTN-154 TPZ test gold DP1 optimize SITCOMTN-154 \u23f3 \u23f3","title":"Validation Results"},{"location":"en/data/pz_server_data.html#photo-z-estimates","text":"","title":"Photo-z Estimates"},{"location":"en/data/pz_server_data.html#external-datasets","text":"Public data collected from the literature and hosted on the Photo-z Server. ATTENTION: External Datasets \u23f3 Documentation in preparation. Back to Photo-z Server documentation","title":"External datasets"},{"location":"en/processamento/index.html","text":"High Performance Computing \u00b6 LIneA provides access to high-performance computing resources for members of scientific collaborations and projects supported by the laboratory in the following environments: Registration in the LIneA environment does NOT automatically grant access to HPC resources. After your registration is approved, you must open a ticket and request access, submitting a usage justification that will be forwarded to the Management Committee for evaluation. Failure to comply with our security policies and environment usage policies may result in your account being blocked without prior notice. The policies can be found here . HPE Apollo 2000 Cluster (LIneA) Open OnDemand JupyterLab over HPC","title":"Processing (HPC)"},{"location":"en/processamento/index.html#high-performance-computing","text":"LIneA provides access to high-performance computing resources for members of scientific collaborations and projects supported by the laboratory in the following environments: Registration in the LIneA environment does NOT automatically grant access to HPC resources. After your registration is approved, you must open a ticket and request access, submitting a usage justification that will be forwarded to the Management Committee for evaluation. Failure to comply with our security policies and environment usage policies may result in your account being blocked without prior notice. The policies can be found here . HPE Apollo 2000 Cluster (LIneA) Open OnDemand JupyterLab over HPC","title":"High Performance Computing"},{"location":"en/processamento/sdu.html","text":"LIneA has an approved \"umbrella\" project at LNCC called \" Exploring the Universe via big data: from the solar system to dark energy \" (acronym EUBD) that guarantees the right to use 2 million CPU hours of the Santos Dumont supercomputer to support some subprojects that depend on High-Performance Computing (HPC) in the following areas: Solar System Milky Way/Local Volume Dark Energy Large-Scale Structures Photometric Redshifts (LSST in-kind contributions program) Info If your project has HPC demands and is not yet part of the EUBD project scope, contact us at helpdesk@linea.org.br for guidance. User registration for access to the Supercomputer - EUBD Project \u00b6 Send an email following the template below to helpdesk@linea.org.br and wait for our response (maximum 72 hours): a.) In the subject field write: Access to Santos Dumont supercomputer - EUBD Project b.) In the email body: 1. Your full name: 2. Institution name (acronym): 3. Advisor name (if applicable): 4. Advisor/supervisor email (if applicable): 5. Usage justification: - Write in maximum 5 lines the objective and importance of using these computational resources. - If possible, inform how many CPU/hours you intend to use and also the input and output data volume of your application. - Use KiloBytes(KB), MegaBytes(MB) or GigaBytes(GB) to represent your data volume. Access to the supercomputer \u00b6 After obtaining your credentials, access and usage instructions are provided directly by LNCC. Job submission will be done through the Slurm resource and queue manager. The user manual is available on the Santos Dumont page on LNCC's website .","title":"Sdu"},{"location":"en/processamento/sdu.html#user-registration-for-access-to-the-supercomputer-eubd-project","text":"Send an email following the template below to helpdesk@linea.org.br and wait for our response (maximum 72 hours): a.) In the subject field write: Access to Santos Dumont supercomputer - EUBD Project b.) In the email body: 1. Your full name: 2. Institution name (acronym): 3. Advisor name (if applicable): 4. Advisor/supervisor email (if applicable): 5. Usage justification: - Write in maximum 5 lines the objective and importance of using these computational resources. - If possible, inform how many CPU/hours you intend to use and also the input and output data volume of your application. - Use KiloBytes(KB), MegaBytes(MB) or GigaBytes(GB) to represent your data volume.","title":"User registration for access to the Supercomputer - EUBD Project"},{"location":"en/processamento/sdu.html#access-to-the-supercomputer","text":"After obtaining your credentials, access and usage instructions are provided directly by LNCC. Job submission will be done through the Slurm resource and queue manager. The user manual is available on the Santos Dumont page on LNCC's website .","title":"Access to the supercomputer"},{"location":"en/processamento/apollo/index.html","text":"HPE Apollo 2000 \u00b6 The Apollo Cluster has 28 compute nodes and offers a total of 1072 physical cores . Its nodes are equipped with Intel Xeon Skylake 5120 2.2GHz processors (apl01-16) and Intel Xeon Gold 5320 2.20GHz processors (apl17-28). The system provides approximately 85 Tflops of computational capacity. The 28 compute nodes of the Apollo Cluster are from the HPE ProLiant server family, with 16 being XL170r models and 12 being XL220n models. Currently, the number of available cores is 2144 , as HT is active on the compute nodes. Server Specifications \u00b6 Model # Cores (HT) [1] RAM OS Hosts HPE Proliant XL170r 56 128 GB Rocky Linux 9.5 apl[01-16] HPE Proliant XL220n 104 256 GB Rocky Linux 9.5 apl[17-28] [1] Hyper-Threading (HT) technology is enabled on all cluster nodes. Cluster Characteristics (Consolidated) # Nodes # Cores Total RAM Installed 16 896 1.5TB Apr-2019 12 624 3TB Jul-2023 The Apollo Cluster is managed by Slurm v24.05.5 . Filesystem \u00b6 The Apollo Cluster features a high-performance Lustre filesystem, available as a \"Scratch\" area. User \"Home\" directories are only accessible on the login node and are provided via NFS. These storage areas should be used as follows: Scratch: Mounted from /scratch/<username> . Used to store all files needed during job execution (submission scripts, executables, input data, output data, etc). Environment variable $SCRATCH . Home: Mounted from /home/<username> . Used especially to store results that should be kept throughout the project duration. Environment variable $HOME . Scriptland: Mounted from /scriptland/<username> . A storage area optimized for scripts and code. Environment variable $SCRIPTLAND . Click here for more details Attention Remember to copy necessary files (executables, libraries, input data) to the SCRATCH area, as the HOMEDIR area is not accessible by compute nodes. Slurm \u00b6 Slurm is an open-source cluster management and job scheduling system, fault-tolerant and highly scalable for both large and small Linux clusters. Slurm requires no kernel modifications for operation and is relatively independent. As a cluster workload manager, Slurm has three key functions: Allocate exclusive and/or non-exclusive access to resources (compute nodes) to users for specified durations Provide a framework for starting, executing, and monitoring work (typically parallel jobs) on allocated nodes Manage the submission queue, arbitrating conflicts between computational resource requests Available Partitions \u00b6 The Apollo cluster is organized into different partitions (machine subsets) to meet different needs, such as guaranteeing maximum priority for LSST project users on machines dedicated to IDAC-Brazil. PARTITION TIMELIMIT NODES NODELIST cpu_dev 30:00 26 apl[01-28] cpu_small 3-00:00:00 26 apl[01-28] cpu 5-00:00:00 26 apl[01-28] cpu_long 31-00:00:0 26 apl[01-28] lsst_cpu_dev 30:00 12 apl[17-28] lsst_cpu_small 3-00:00:00 12 apl[17-28] lsst_cpu 5-00:00:00 12 apl[17-28] lsst_cpu_long 10-00:00:0 12 apl[17-28] Available Accounts \u00b6 Workflow \u2013 Interrupts any running job: hpc-photoz (photoz) LSST \u2013 Next in queue: hpc-lsst [only on new apollos apl[17-28]] (lsst) Group A - Highest Priority: hpc-bpglsst (itteam, bpg-lsst) Group B - Medium Priority: hpc-collab (des, desi, sdss, tno) Group C - Lowest Priority: hpc-public (linea-members) The partitions ( cpu_dev , cpu_small , cpu and cpu_long ) include all apollos ( apl[01-28] ), while LSST group partitions only include apl[17-28] . Only the hpc-lsst account can submit jobs to \"lsst\" prefixed partitions, which have higher priority on nodes. Attention As part of the BRA-LIN in-kind contribution program, IDAC Brazil is committed to generating photometric redshifts annually for the LSST survey, always preceding official data releases. During these periods, the Apollo Cluster will be fully occupied for this purpose for an estimated duration of several hours, potentially extending to several days. Users will be notified in advance via email about cluster unavailability. Click here to learn more about photometric redshift production and the BRA-LIN in-kind contribution program. Anatomy of a Job \u00b6 A Job requests computing resources and specifies the applications to be launched on those resources, along with any input data/options and output directives. The user submits the job, typically as a batch job script, to the batch scheduler. The batch job script consists of four main components: The interpreter used to execute the script \"#\" directives that convey default submission options Environment variable and/or script configuration (if needed) The applications to be executed along with their input arguments and options Here's an example batch script requesting 3 nodes in the \"cpu\" partition and launching 36 myApp tasks on the 3 allocated nodes: #!/bin/bash #SBATCH -N 3 #SBATCH -p cpu #SBATCH --ntasks 36 srun myApp When the job is scheduled for execution, the resource manager will execute the batch job script on the first node of the allocation. Specifying Resources \u00b6 Slurm has its own syntax for requesting computing resources. Below is a summary table of some frequently requested resources and the Slurm syntax to obtain them. For a complete syntax list, run the man sbatch command. Syntax Meaning #SBATCH -p partition Defines the partition where the job will run #SBATCH -J job_name Defines the Job name #SBATCH -n quantity Defines the total number of CPU tasks #SBATCH -N quantity Defines the number of requested compute nodes Basic Slurm Commands \u00b6 To learn about all available options for each command, enter man while connected to the Cluster environment. Command Definition sbatch Submits job scripts to the execution queue scancel Cancels a job scontrol Used to display Slurm state (various options available only to root) sinfo Displays partition and node states squeue Displays job states salloc Submits a job for execution or starts a real-time job Execution Environment \u00b6 For each job type above, the user can define the execution environment. This includes environment variable settings as well as shell limits ( bash ulimit or csh limit ). sbatch and salloc provide the --export option to pass specific environment variables to the execution environment. They also have the --propagate option to pass specific shell limits to the execution environment. Environment Variables \u00b6 The first category of environment variables are those that Slurm inserts into the job execution environment. They convey to the job script and application information such as job ID (SLURM_JOB_ID) and task ID (SLURM_PROCID) . For the complete list, see the \"OUTPUT ENVIRONMENT VARIABLES\" section in the sbatch , salloc and srun man pages. The next category are environment variables that users can define in their environment to pass default options to each submitted job. This includes options like the wall clock limit. For the complete list, see the \"INPUT ENVIRONMENT VARIABLES\" section in the sbatch , salloc and srun man pages. More Information? Learn how to use the Apollo Cluster at How to Use . For more information, contact the Service Desk .","title":"Apollo Cluster"},{"location":"en/processamento/apollo/index.html#hpe-apollo-2000","text":"The Apollo Cluster has 28 compute nodes and offers a total of 1072 physical cores . Its nodes are equipped with Intel Xeon Skylake 5120 2.2GHz processors (apl01-16) and Intel Xeon Gold 5320 2.20GHz processors (apl17-28). The system provides approximately 85 Tflops of computational capacity. The 28 compute nodes of the Apollo Cluster are from the HPE ProLiant server family, with 16 being XL170r models and 12 being XL220n models. Currently, the number of available cores is 2144 , as HT is active on the compute nodes.","title":"HPE Apollo 2000"},{"location":"en/processamento/apollo/index.html#filesystem","text":"The Apollo Cluster features a high-performance Lustre filesystem, available as a \"Scratch\" area. User \"Home\" directories are only accessible on the login node and are provided via NFS. These storage areas should be used as follows: Scratch: Mounted from /scratch/<username> . Used to store all files needed during job execution (submission scripts, executables, input data, output data, etc). Environment variable $SCRATCH . Home: Mounted from /home/<username> . Used especially to store results that should be kept throughout the project duration. Environment variable $HOME . Scriptland: Mounted from /scriptland/<username> . A storage area optimized for scripts and code. Environment variable $SCRIPTLAND . Click here for more details Attention Remember to copy necessary files (executables, libraries, input data) to the SCRATCH area, as the HOMEDIR area is not accessible by compute nodes.","title":"Filesystem"},{"location":"en/processamento/apollo/index.html#slurm","text":"Slurm is an open-source cluster management and job scheduling system, fault-tolerant and highly scalable for both large and small Linux clusters. Slurm requires no kernel modifications for operation and is relatively independent. As a cluster workload manager, Slurm has three key functions: Allocate exclusive and/or non-exclusive access to resources (compute nodes) to users for specified durations Provide a framework for starting, executing, and monitoring work (typically parallel jobs) on allocated nodes Manage the submission queue, arbitrating conflicts between computational resource requests","title":"Slurm"},{"location":"en/processamento/apollo/index.html#available-partitions","text":"The Apollo cluster is organized into different partitions (machine subsets) to meet different needs, such as guaranteeing maximum priority for LSST project users on machines dedicated to IDAC-Brazil. PARTITION TIMELIMIT NODES NODELIST cpu_dev 30:00 26 apl[01-28] cpu_small 3-00:00:00 26 apl[01-28] cpu 5-00:00:00 26 apl[01-28] cpu_long 31-00:00:0 26 apl[01-28] lsst_cpu_dev 30:00 12 apl[17-28] lsst_cpu_small 3-00:00:00 12 apl[17-28] lsst_cpu 5-00:00:00 12 apl[17-28] lsst_cpu_long 10-00:00:0 12 apl[17-28]","title":"Available Partitions"},{"location":"en/processamento/apollo/index.html#available-accounts","text":"Workflow \u2013 Interrupts any running job: hpc-photoz (photoz) LSST \u2013 Next in queue: hpc-lsst [only on new apollos apl[17-28]] (lsst) Group A - Highest Priority: hpc-bpglsst (itteam, bpg-lsst) Group B - Medium Priority: hpc-collab (des, desi, sdss, tno) Group C - Lowest Priority: hpc-public (linea-members) The partitions ( cpu_dev , cpu_small , cpu and cpu_long ) include all apollos ( apl[01-28] ), while LSST group partitions only include apl[17-28] . Only the hpc-lsst account can submit jobs to \"lsst\" prefixed partitions, which have higher priority on nodes. Attention As part of the BRA-LIN in-kind contribution program, IDAC Brazil is committed to generating photometric redshifts annually for the LSST survey, always preceding official data releases. During these periods, the Apollo Cluster will be fully occupied for this purpose for an estimated duration of several hours, potentially extending to several days. Users will be notified in advance via email about cluster unavailability. Click here to learn more about photometric redshift production and the BRA-LIN in-kind contribution program.","title":"Available Accounts"},{"location":"en/processamento/apollo/index.html#anatomy-of-a-job","text":"A Job requests computing resources and specifies the applications to be launched on those resources, along with any input data/options and output directives. The user submits the job, typically as a batch job script, to the batch scheduler. The batch job script consists of four main components: The interpreter used to execute the script \"#\" directives that convey default submission options Environment variable and/or script configuration (if needed) The applications to be executed along with their input arguments and options Here's an example batch script requesting 3 nodes in the \"cpu\" partition and launching 36 myApp tasks on the 3 allocated nodes: #!/bin/bash #SBATCH -N 3 #SBATCH -p cpu #SBATCH --ntasks 36 srun myApp When the job is scheduled for execution, the resource manager will execute the batch job script on the first node of the allocation.","title":"Anatomy of a Job"},{"location":"en/processamento/uso/howtouse-HPC.html","text":"How to Use \u00b6 How to Access \u00b6 Access to our cluster can be done through Open OnDemand or via the JupyterLab (K8S) Terminal . In both options, it is essential to have a valid account in LIneA's computational environment. If you don't have an account, contact the Service Desk by email ( helpdesk@linea.org.br ) for more information. Attention Even with an active LIneA account, access to the HPC processing environment is not automatic. For more information contact the Service Desk at helpdesk@linea.org.br . Accessing via JupyterLab terminal In the home screen of your Jupyter Notebook, in the \"Other\" section, you will find the terminal button. When clicking it, you will be redirected to a Linux terminal, initially located in your home directory. To access the Apollo Cluster, simply execute the following command: ssh loginapl01 The machine loginapl01 is where you can allocate compute nodes to submit your job. $HOME and $SCRATCH Compute nodes don't have access to your user home directory. Move or copy all files needed for job submission to your SCRATCH directory. How to use the Scratch area \u00b6 Your SCRATCH directory is the place where you can direct your job results files, as well as temporarily store data that is used by the code at the time of processing. To access your SCRATCH directory: cd $SCRATCH To send files to your SCRATCH directory: cp <FILE> $SCRATCH Automatic Scratch cleaning \u00b6 The scratch is a temporary storage area for the output and processing files of the cluster. To maintain the environment organized and ensure space available for all, a automatic cleaning script is in force, which is performed once a week . This process removes files that have not been accessed within the definite retention period - Currently, 45 days . Essential configuration files (eg: .Bashrc , .Bash_Profile , .ssh , etc.) are automatically preserved and do not enter the exclusion process . ATTENTION Scratch should not be used for permanent storage. We recommend moving important data for your home directory. How to use the Scripts area \u00b6 Your SCRIPTS directory is where you can store scripts and codes to run in the cluster. It is also recommended to use this area to create environments Conda. To access your SCRIPTS Directory: cd $SCRIPTS Stay attentive The script area is not included in the backup routine. Therefore, it should not be used as permanent data storage. How to Submit a Job \u00b6 A Job requests computing resources and specifies applications to be launched on those resources, along with any input data/options and output directives. Cluster task and resource management is done through Slurm. Therefore, to submit a Job you need to use a script like below: #!/bin/bash #SBATCH -p PARTITION #Name of the Partition to use #SBATCH --nodelist=NODE #Name of the Node to be allocated #SBATCH -J simple-job #Job name #----------------------------------------------------------------------------# ##path to executable code EXEC = /scripts/YOUR.USER/EXECUTABLE.CODE srun $EXEC In this script you need to specify: the queue name (Partition) to be used; the node name to be allocated for Job execution; and the path to the code/program to be executed. WARNING It is strictly prohibited to submit jobs directly to the loginapl01 machine. Any code running on this machine will be immediately interrupted without prior notice. To submit the Job: sbatch script-submit-job.sh If the script is correct there will be an output indicating the job ID . To check job progress and information: scontrol show job <ID> To cancel the Job: scancel <ID> Internet access Compute nodes do not have internet access. Packages and libraries must be installed from loginapl01 in your scratch area. EUPS Package Manager \u00b6 EUPS is an alternative package manager (and official LSST one) that allows loading environment variables and including paths to programs and libraries in a modular way. To load EUPS: Info Currently, EUPS is automatically loaded after the user logs into any machine in the Apollo cluster. source /opt/eups/bin/setups.sh To list all available packages: eups list To list a specific package: eups list <PACKAGE> To load a package in current session: setup <PACKAGE NAME> <PACKAGE VERSION> To remove loaded package: unsetup <PACKAGE NAME> <PACKAGE VERSION> Useful Slurm Commands \u00b6 To learn about all available options for each command, enter man <command> while connected to the Cluster environment. Command Definition sbatch Submits job scripts to execution queue squeue Displays job status scontrol Used to display Slurm state (various options available only to root) sinfo Displays partition and node status salloc Submits a job for execution or starts a real-time job Tutorial videos \u00b6 How to login How to use EUPS How to access the Scratch How to submit a Job","title":"How to Use"},{"location":"en/processamento/uso/howtouse-HPC.html#how-to-use","text":"","title":"How to Use"},{"location":"en/processamento/uso/howtouse-HPC.html#how-to-access","text":"Access to our cluster can be done through Open OnDemand or via the JupyterLab (K8S) Terminal . In both options, it is essential to have a valid account in LIneA's computational environment. If you don't have an account, contact the Service Desk by email ( helpdesk@linea.org.br ) for more information. Attention Even with an active LIneA account, access to the HPC processing environment is not automatic. For more information contact the Service Desk at helpdesk@linea.org.br . Accessing via JupyterLab terminal In the home screen of your Jupyter Notebook, in the \"Other\" section, you will find the terminal button. When clicking it, you will be redirected to a Linux terminal, initially located in your home directory. To access the Apollo Cluster, simply execute the following command: ssh loginapl01 The machine loginapl01 is where you can allocate compute nodes to submit your job. $HOME and $SCRATCH Compute nodes don't have access to your user home directory. Move or copy all files needed for job submission to your SCRATCH directory.","title":"How to Access"},{"location":"en/processamento/uso/howtouse-HPC.html#how-to-use-the-scratch-area","text":"Your SCRATCH directory is the place where you can direct your job results files, as well as temporarily store data that is used by the code at the time of processing. To access your SCRATCH directory: cd $SCRATCH To send files to your SCRATCH directory: cp <FILE> $SCRATCH","title":"How to use the Scratch area"},{"location":"en/processamento/uso/howtouse-HPC.html#how-to-use-the-scripts-area","text":"Your SCRIPTS directory is where you can store scripts and codes to run in the cluster. It is also recommended to use this area to create environments Conda. To access your SCRIPTS Directory: cd $SCRIPTS Stay attentive The script area is not included in the backup routine. Therefore, it should not be used as permanent data storage.","title":"How to use the Scripts area"},{"location":"en/processamento/uso/howtouse-HPC.html#how-to-submit-a-job","text":"A Job requests computing resources and specifies applications to be launched on those resources, along with any input data/options and output directives. Cluster task and resource management is done through Slurm. Therefore, to submit a Job you need to use a script like below: #!/bin/bash #SBATCH -p PARTITION #Name of the Partition to use #SBATCH --nodelist=NODE #Name of the Node to be allocated #SBATCH -J simple-job #Job name #----------------------------------------------------------------------------# ##path to executable code EXEC = /scripts/YOUR.USER/EXECUTABLE.CODE srun $EXEC In this script you need to specify: the queue name (Partition) to be used; the node name to be allocated for Job execution; and the path to the code/program to be executed. WARNING It is strictly prohibited to submit jobs directly to the loginapl01 machine. Any code running on this machine will be immediately interrupted without prior notice. To submit the Job: sbatch script-submit-job.sh If the script is correct there will be an output indicating the job ID . To check job progress and information: scontrol show job <ID> To cancel the Job: scancel <ID> Internet access Compute nodes do not have internet access. Packages and libraries must be installed from loginapl01 in your scratch area.","title":"How to Submit a Job"},{"location":"en/processamento/uso/howtouse-HPC.html#eups-package-manager","text":"EUPS is an alternative package manager (and official LSST one) that allows loading environment variables and including paths to programs and libraries in a modular way. To load EUPS: Info Currently, EUPS is automatically loaded after the user logs into any machine in the Apollo cluster. source /opt/eups/bin/setups.sh To list all available packages: eups list To list a specific package: eups list <PACKAGE> To load a package in current session: setup <PACKAGE NAME> <PACKAGE VERSION> To remove loaded package: unsetup <PACKAGE NAME> <PACKAGE VERSION>","title":"EUPS Package Manager"},{"location":"en/processamento/uso/howtouse-HPC.html#useful-slurm-commands","text":"To learn about all available options for each command, enter man <command> while connected to the Cluster environment. Command Definition sbatch Submits job scripts to execution queue squeue Displays job status scontrol Used to display Slurm state (various options available only to root) sinfo Displays partition and node status salloc Submits a job for execution or starts a real-time job","title":"Useful Slurm Commands"},{"location":"en/processamento/uso/howtouse-HPC.html#tutorial-videos","text":"How to login How to use EUPS How to access the Scratch How to submit a Job","title":"Tutorial videos"},{"location":"en/processamento/uso/openondemand.html","text":"Open OnDemand \u00b6 Open OnDemand is an interface that facilitates the use of our HPC environment consisting of the Apollo Cluster. To access it, you need a valid LIneA account ( learn more ). Access to Open OnDemand is through https://ondemand.linea.org.br/ . On the home screen of the platform, at the top, you can see a menu with the following items: Files - provides an interface to your user directory ( Home Directory ). Jobs - provides interfaces for \"Active Jobs\" and \"Job Composer\". Clusters - provides web browser terminal access. Interactive Apps - provides access to Jupyter Notebook. Home Directory \u00b6 The Home Directory allows you to view your user directory where your files are stored, along with various buttons offering different functionalities. Changing Directory \u00b6 Clicking the Change Directory button lets you navigate through our infrastructure. Simply enter the path in the Path field and click \" ok \". Accessing Terminal \u00b6 Clicking the Open Terminal button takes you to a Linux terminal on the login machine (loginapl01) of the Apollo Cluster. In this terminal, you're in your user \" Home \" directory and can view your files. You can also perform all standard HPC user operations, such as allocating compute nodes and checking the Slurm queue using commands. Creating, Transferring and Moving Files \u00b6 In Open OnDemand, creating new files and directories is simple, as is transferring them. Click the \"New File\" or \"New Directory\" buttons and choose names for your new files/directories. The \"Upload\" button lets you transfer files from your local machine to your home directory in our environment. The \"Download\" button lets you send selected files to your local machine. To view, rename or edit created files, click the \"three dots\" next to the file to see a menu with these options. To move or copy files: Select the desired file Click \"Copy/Move\" Click \"Change Directory\" and enter the destination path Click \"Copy\" or \"Move\" in the box that appears on the left side of the screen. Jobs \u00b6 In the Jobs section of the main menu, you'll find \"Job Composer\" and \"Active Jobs\". The \"Job Composer\" screen simplifies job submission, while \"Active Jobs\" lets you monitor your job execution in detail. To submit a job, you need a submission script like this: ( learn more ) #!/bin/bash #SBATCH -p PARTITION #Name of the Partition to use #SBATCH --nodelist=NODE #Name of the Node to be allocated #SBATCH -J simple-job #Job name #----------------------------------------------------------------------------# ##path to executable code EXEC = /scratch/users/YOUR.USER/ondemand/projects/EXECUTABLE.CODE srun $EXEC To view more job submission script templates, click here Job Composer \u00b6 Open OnDemand simplifies the entire job submission process through the Job Composer tool. Just follow these steps: Click the \"New Job\" button Choose \"Default Template\" Edit submission script specifications in \"Open Editor\" Click \"Submit\" to start job execution. Important Notice The cluster's compute nodes don't have access to your user directory (Home Directory). Move or copy all files needed for job submission to your SCRATCH directory. JupyterLab \u00b6 With Open OnDemand, you can access Jupyter Notebook in our HPC environment. Through \"Interactive Apps\", Jupyter Notebook will start a session on one of the cluster's compute nodes by: Clicking \"Jupyter Notebook\" Filling in the \"Account\", \"Partition\" and \"Select node\" fields Pressing the \"Launch\" button Connecting to JupyterLab by clicking \"Connect to Jupyter\" . Jupyter on Kubernetes VS Jupyter on HPC We have two Jupyter environments on different infrastructures. One is available at https://jupyter.linea.org.br running on Kubernetes . The other, mentioned above, runs interactively through Open OnDemand directly on compute nodes. When opening a terminal within JupyterLab via Open OnDemand, you'll see you're on an Apollo Cluster node with access to your \" scratch \" area on Lustre storage. Creating Python Kernels \u00b6 The following commands must be executed in the terminal in \"LIneA Shell Access\" . To access: On the Open OnDemand home page, click: Clusters -> LIneA Shell Access . Follow these commands: Go to your SCRATCH area, install and load Miniconda: cd $SCRATCH curl -L -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod +x Miniconda3-latest-Linux-x86_64.sh ./Miniconda3-latest-Linux-x86_64.sh -p $SCRATCH /miniconda source miniconda/bin/activate conda deactivate #Required to deactivate \"base\" env Create, activate a conda venv and install ipykernel : conda create -p $SCRATCH /kernelname conda activate kernelname/ conda install -c anaconda ipykernel Configure JUPYTER_PATH (must be this exact path): JUPYTER_PATH = $SCRATCH /.local echo $JUPYTER_PATH python -m ipykernel install --prefix = $JUPYTER_PATH --name 'kernelname' Open a Jupyter Notebook session. The last command's output should be: #[InstallIPythonKernelSpecApp] WARNING | Installing to /lustre/t0/scratch/users/YOUR.USER/.local/share/jupyter/kernels, which is not in ['/lustre/t0/scratch/users/YOUR.USER/kernelname/share/jupyter/kernels', '/home/YOUR.USER/.local/share/jupyter/kernels', '/usr/local/share/jupyter/kernels', '/usr/share/jupyter/kernels', '/home/YOUR.USER/.ipython/kernels']. The kernelspec may not be found. Installed kernelspec kernelname in /lustre/t0/scratch/users/YOUR.USER/.local/share/jupyter/kernels/kernelname After executing these commands, you'll see the created Python kernel button. Tutorial videos \u00b6 How to submit a Job How to access Jupyter Notebook For any questions, contact the Service Desk .","title":"Open OnDemand"},{"location":"en/processamento/uso/openondemand.html#open-ondemand","text":"Open OnDemand is an interface that facilitates the use of our HPC environment consisting of the Apollo Cluster. To access it, you need a valid LIneA account ( learn more ). Access to Open OnDemand is through https://ondemand.linea.org.br/ . On the home screen of the platform, at the top, you can see a menu with the following items: Files - provides an interface to your user directory ( Home Directory ). Jobs - provides interfaces for \"Active Jobs\" and \"Job Composer\". Clusters - provides web browser terminal access. Interactive Apps - provides access to Jupyter Notebook.","title":"Open OnDemand"},{"location":"en/processamento/uso/openondemand.html#home-directory","text":"The Home Directory allows you to view your user directory where your files are stored, along with various buttons offering different functionalities.","title":"Home Directory"},{"location":"en/processamento/uso/openondemand.html#jobs","text":"In the Jobs section of the main menu, you'll find \"Job Composer\" and \"Active Jobs\". The \"Job Composer\" screen simplifies job submission, while \"Active Jobs\" lets you monitor your job execution in detail. To submit a job, you need a submission script like this: ( learn more ) #!/bin/bash #SBATCH -p PARTITION #Name of the Partition to use #SBATCH --nodelist=NODE #Name of the Node to be allocated #SBATCH -J simple-job #Job name #----------------------------------------------------------------------------# ##path to executable code EXEC = /scratch/users/YOUR.USER/ondemand/projects/EXECUTABLE.CODE srun $EXEC To view more job submission script templates, click here","title":"Jobs"},{"location":"en/processamento/uso/openondemand.html#job-composer","text":"Open OnDemand simplifies the entire job submission process through the Job Composer tool. Just follow these steps: Click the \"New Job\" button Choose \"Default Template\" Edit submission script specifications in \"Open Editor\" Click \"Submit\" to start job execution. Important Notice The cluster's compute nodes don't have access to your user directory (Home Directory). Move or copy all files needed for job submission to your SCRATCH directory.","title":"Job Composer"},{"location":"en/processamento/uso/openondemand.html#jupyterlab","text":"With Open OnDemand, you can access Jupyter Notebook in our HPC environment. Through \"Interactive Apps\", Jupyter Notebook will start a session on one of the cluster's compute nodes by: Clicking \"Jupyter Notebook\" Filling in the \"Account\", \"Partition\" and \"Select node\" fields Pressing the \"Launch\" button Connecting to JupyterLab by clicking \"Connect to Jupyter\" . Jupyter on Kubernetes VS Jupyter on HPC We have two Jupyter environments on different infrastructures. One is available at https://jupyter.linea.org.br running on Kubernetes . The other, mentioned above, runs interactively through Open OnDemand directly on compute nodes. When opening a terminal within JupyterLab via Open OnDemand, you'll see you're on an Apollo Cluster node with access to your \" scratch \" area on Lustre storage.","title":"JupyterLab"},{"location":"en/processamento/uso/openondemand.html#creating-python-kernels","text":"The following commands must be executed in the terminal in \"LIneA Shell Access\" . To access: On the Open OnDemand home page, click: Clusters -> LIneA Shell Access . Follow these commands: Go to your SCRATCH area, install and load Miniconda: cd $SCRATCH curl -L -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod +x Miniconda3-latest-Linux-x86_64.sh ./Miniconda3-latest-Linux-x86_64.sh -p $SCRATCH /miniconda source miniconda/bin/activate conda deactivate #Required to deactivate \"base\" env Create, activate a conda venv and install ipykernel : conda create -p $SCRATCH /kernelname conda activate kernelname/ conda install -c anaconda ipykernel Configure JUPYTER_PATH (must be this exact path): JUPYTER_PATH = $SCRATCH /.local echo $JUPYTER_PATH python -m ipykernel install --prefix = $JUPYTER_PATH --name 'kernelname' Open a Jupyter Notebook session. The last command's output should be: #[InstallIPythonKernelSpecApp] WARNING | Installing to /lustre/t0/scratch/users/YOUR.USER/.local/share/jupyter/kernels, which is not in ['/lustre/t0/scratch/users/YOUR.USER/kernelname/share/jupyter/kernels', '/home/YOUR.USER/.local/share/jupyter/kernels', '/usr/local/share/jupyter/kernels', '/usr/share/jupyter/kernels', '/home/YOUR.USER/.ipython/kernels']. The kernelspec may not be found. Installed kernelspec kernelname in /lustre/t0/scratch/users/YOUR.USER/.local/share/jupyter/kernels/kernelname After executing these commands, you'll see the created Python kernel button.","title":"Creating Python Kernels"},{"location":"en/processamento/uso/openondemand.html#tutorial-videos","text":"How to submit a Job How to access Jupyter Notebook For any questions, contact the Service Desk .","title":"Tutorial videos"},{"location":"en/processamento/uso/politicadeuso.html","text":"Usage Policy \u00b6 Usage Policy of the Computational Infrastructure of the Interinstitutional Laboratory of e-Astronomy - LIneA. This Usage Policy of the Computational Infrastructure of the Interinstitutional Laboratory of e-Astronomy covers all computational resources made available for scientific research. This set of rules aims to: Ensure a friendly, productive and reliable computational environment; Maintain an environment that allows development and innovation; Safeguard the reputation of the Interinstitutional Laboratory of e-Astronomy (LIneA) and protect its computing systems and the data they contain against external attacks and unauthorized use; Ensure compliance with all legal guidelines. All users must strive to maintain the security of the environment. The user account is for exclusive, personal use and cannot be shared. Unauthorized access attempts aimed at damaging, altering, falsifying or deleting data, falsifying email addresses or causing service interruptions are prohibited. LIneA's computational resources must be used solely and exclusively for the execution of tasks related to previously registered scientific projects. It is expressly prohibited to use these resources for: Illegal activities of any nature; Activities that offend or result in public embarrassment of collaborators, users or third parties; Activities involving personal financial gain; Download, upload or storage of material with sexual or illegal content; Execution of tasks that use an amount of resources incompatible with infrastructure sharing and that interfere with the overall system performance; Execution of tasks that result in violation of licenses of previously installed computational applications; Distribution or storage of music, movies, or any material protected by copyright without proper license. Questions about the appropriateness of the use of computational resources should be communicated directly to LIneA via Service Desk . Data in SCRATCH does not have backups. Critical data must be saved by the user in their HOME directory and is their own responsibility. More details about storage systems . Every user is required to immediately report any suspicion of violation of security rules by sending an email to helpdesk@linea.org.br . LIneA values the privacy of all collaborators and users. No query or inspection will be made of files without the user's authorization, except in situations involving compromise of system security. Every user of LIneA's computational infrastructure commits to: * Include in all articles submitted to scientific journals, conference proceedings, theses, dissertations, etc., that make use of LIneA's computational resources, the following acknowledgment: \"This research was supported by resources supplied by the Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\"; or, \"Esta pesquisa tornou-se poss\u00edvel gra\u00e7as aos recursos computacionais disponibilizados pelo Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\"; Forward to CDE a copy of the aforementioned publication which should be included on LIneA's official website.","title":"Usage Policy"},{"location":"en/processamento/uso/politicadeuso.html#usage-policy","text":"Usage Policy of the Computational Infrastructure of the Interinstitutional Laboratory of e-Astronomy - LIneA. This Usage Policy of the Computational Infrastructure of the Interinstitutional Laboratory of e-Astronomy covers all computational resources made available for scientific research. This set of rules aims to: Ensure a friendly, productive and reliable computational environment; Maintain an environment that allows development and innovation; Safeguard the reputation of the Interinstitutional Laboratory of e-Astronomy (LIneA) and protect its computing systems and the data they contain against external attacks and unauthorized use; Ensure compliance with all legal guidelines. All users must strive to maintain the security of the environment. The user account is for exclusive, personal use and cannot be shared. Unauthorized access attempts aimed at damaging, altering, falsifying or deleting data, falsifying email addresses or causing service interruptions are prohibited. LIneA's computational resources must be used solely and exclusively for the execution of tasks related to previously registered scientific projects. It is expressly prohibited to use these resources for: Illegal activities of any nature; Activities that offend or result in public embarrassment of collaborators, users or third parties; Activities involving personal financial gain; Download, upload or storage of material with sexual or illegal content; Execution of tasks that use an amount of resources incompatible with infrastructure sharing and that interfere with the overall system performance; Execution of tasks that result in violation of licenses of previously installed computational applications; Distribution or storage of music, movies, or any material protected by copyright without proper license. Questions about the appropriateness of the use of computational resources should be communicated directly to LIneA via Service Desk . Data in SCRATCH does not have backups. Critical data must be saved by the user in their HOME directory and is their own responsibility. More details about storage systems . Every user is required to immediately report any suspicion of violation of security rules by sending an email to helpdesk@linea.org.br . LIneA values the privacy of all collaborators and users. No query or inspection will be made of files without the user's authorization, except in situations involving compromise of system security. Every user of LIneA's computational infrastructure commits to: * Include in all articles submitted to scientific journals, conference proceedings, theses, dissertations, etc., that make use of LIneA's computational resources, the following acknowledgment: \"This research was supported by resources supplied by the Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\"; or, \"Esta pesquisa tornou-se poss\u00edvel gra\u00e7as aos recursos computacionais disponibilizados pelo Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\"; Forward to CDE a copy of the aforementioned publication which should be included on LIneA's official website.","title":"Usage Policy"},{"location":"en/processamento/uso/templates-jobs.html","text":"Submit Job Scripts - Templates \u00b6 Simple Job Script \u00b6 #!/bin/bash #SBATCH -p PARTITION #Name of the Partition to use #SBATCH --nodelist=NODENAME #Name of the Node to be allocated #SBATCH -J JOB-NAME #Job name #----------------------------------------------------------------------------# ##path to executable code EXEC = /scripts/YOUR.USER/EXECUTABLE.CODE srun $EXEC In this script you need to specify the queue name (Partition) to be used, the node name to be allocated for Job execution, and the path to the code/program to be executed. EUPS Load Script \u00b6 #!/bin/bash #SBATCH -p PARTITION #Name of the Partition to use #SBATCH --nodelist=NODENAME #Name of the Node to be allocated #SBATCH -J JOB-NAME #Job name #----------------------------------------------------------------------------# #Load EUPS . /mnt/eups/linea_eups_setup.sh #Load package setup <PACKAGE> <VERSION> ##path to executable code EXEC = /scripts/YOUR.USER/EXECUTABLE.CODE srun $EXEC Parallel Submit Job Script \u00b6 OpenMP \u00b6 MPI \u00b6","title":"Job Script (examples)"},{"location":"en/sci-platforms/index.html","text":"LIneA Science Platform \u00b6 The LIneA Science Platform ( scienceplatform.linea.org.br ) is an online platform, still under development, that will aggregate a set of services and tools offered to facilitate access and analysis of astronomical data hosted at LIneA. Currently, a test version of the platform is available for user evaluation at scienceplatform-dev.linea.org.br/lsp . IDAC Science Platform \u00b6 During the operation of the LSST survey, a reduced version of the LIneA Science Platform, containing only services related to the LSST project, will be the main access point to data hosted at the Brazilian Independent Data Access Center (IDAC-Brazil) for scientists in the Brazilian community. In the years preceding LSST operations, IDAC-Brazil services already provide access to public data from other surveys, such as the Dark Energy Survey , so the community can prepare and familiarize themselves with the various tools offered. Services \u00b6 Data access services with various tools for processing, exploring, visualizing, and analyzing public and private data. JupyterHub \u00b6 OnDemand \u00b6 User Query \u00b6 TAP Service \u00b6 Sky Viewer \u00b6 Target Viewer \u00b6 DES Science Server \u00b6 Occultation Prediction Database \u00b6 SDSS Sky Server \u00b6 LSST PZ Server \u00b6 Scientific Portals \u00b6 Sets of tools specifically developed for each project and restricted to members of each scientific collaboration. DES Science Portal \u00b6 Solar System Portal \u00b6 MaNGA Portal \u00b6","title":"Scientific Platforms"},{"location":"en/sci-platforms/index.html#linea-science-platform","text":"The LIneA Science Platform ( scienceplatform.linea.org.br ) is an online platform, still under development, that will aggregate a set of services and tools offered to facilitate access and analysis of astronomical data hosted at LIneA. Currently, a test version of the platform is available for user evaluation at scienceplatform-dev.linea.org.br/lsp .","title":"LIneA Science Platform"},{"location":"en/sci-platforms/index.html#idac-science-platform","text":"During the operation of the LSST survey, a reduced version of the LIneA Science Platform, containing only services related to the LSST project, will be the main access point to data hosted at the Brazilian Independent Data Access Center (IDAC-Brazil) for scientists in the Brazilian community. In the years preceding LSST operations, IDAC-Brazil services already provide access to public data from other surveys, such as the Dark Energy Survey , so the community can prepare and familiarize themselves with the various tools offered.","title":"IDAC Science Platform"},{"location":"en/sci-platforms/index.html#services","text":"Data access services with various tools for processing, exploring, visualizing, and analyzing public and private data.","title":"Services"},{"location":"en/sci-platforms/index.html#scientific-portals","text":"Sets of tools specifically developed for each project and restricted to members of each scientific collaboration.","title":"Scientific Portals"},{"location":"en/sci-platforms/des.html","text":"The DES Science Portal ( des-portal.linea.org.br ) was a platform developed for the Dark Energy Survey (DES) project that hosted a variety of pipelines designed to prepare customized catalogs for different astronomy applications, as well as to conduct various scientific analyses. The data processing component of the DES Science Portal has been discontinued due to the obsolescence of the technologies involved, but its system for accessing generated results and provenance control remains available to DES collaboration members. The functionalities of the DES Science Portal have been gradually replaced by more modern services developed by LIneA, designed to handle Big Data in preparation for LSST, while maintaining its philosophy and best practices. The DES Science Portal pipelines were grouped into stages: Data Preparation - includes creating systematic effect maps, object classification (star/galaxy), calculating photometric redshifts and other aggregated values, depending on the application. For example, estimating age, metallicity or stellar mass of galaxies. Value-added Catalog - combines data and results obtained in the previous stage, selects columns of interest and applies quality cuts and data cleaning, with criteria strongly dependent on the scientific application for which the catalog will be used. Science Workflows - aggregates various scientific analysis pipelines. Takes as input the catalogs created in the previous stage. One of the main strengths of the DES Science Portal was its ability to provide complete information about the entire history of executed processes, allowing users to track inputs, configurations, code versions used and results obtained, in the form of a product record with informative graphs and tables. The portal also provided access to a series of tools designed for users, developers and administrators. To learn more details about the DES Science Portal, read the two articles published in Astronomy and Computing : Gschwend et al. 2018 - DES science portal: Computing photometric redshifts ( doi.org/10.1016/j.ascom.2018.08.008 ) ( arXiv:1708.05643 ) Fausti Neto et al. 2018 - DES science portal: Creating science-ready catalogs ( doi.org/10.1016/j.ascom.2018.01.002 ) ( arXiv:1708.05642 )","title":"DES Science Portal"},{"location":"en/sci-platforms/jupyter.html","text":"JupyterHub is a multi-user development environment based on iPython Notebooks that provides access to shared computing resources on a remote server, without requiring installation or maintenance by users. The only prerequisites to access JupyterHub are: having a LIneA user account (see here how to create your account) and a web browser with internet access. So-called Jupyter Notebooks allow combining interactive code, execution results, explanatory text and multimedia resources in a single document. As part of the LIneA Science Platform , the LIneA JupyterHub is integrated with other visualization tools ( Science Server ) and data access tools ( User Query ). This way, all data analysis can be done online within the platform, from reading, visualization, processing to result analysis, without needing to download the data. When clicking on the \"JupyterHub\" card within the LIneA Science Platform, you will be directed to the login page and then to the JupyterHub homepage showing your user profile. Click the START button to begin. The standard JupyterHub installation uses the new JupyterLab interface and is based on the datascience-notebook image, extended with the Astropy and dblinea libraries (the library that connects to the database). This means several popular Python libraries like Numpy and Matplotlib will be automatically available. User Support \u00b6 Tutorials in Jupyter Notebooks \u00b6 In the jupyterhub-tutorial repository you will find tutorials in notebook format: 1-primeiros-passos.ipynb \u00b6 General instructions for using the JupyterLab platform, tips and shortcuts for writing notebooks for different cell types. 2-acesso-a-dados.ipynb \u00b6 Instructions for using the dblinea library to read data from the database directly from a notebook cell with usage examples (building a simple color-magnitude diagram for a star sample). 3-conda-env.ipynb \u00b6 Instructions for creating conda environments to manage libraries that persist and survive container destruction and recreation, allowing users to return in a new session and find the same environment as the previous session (not available for public bronze profile users). To access the notebooks , simply open a Terminal in JupyterLab by clicking the \"+\" button in the top bar and then the \"Terminal\" icon in the \"Other\" section of the \"Launcher\" tab, and enter the command: git clone https://github.com/linea-it/jupyterhub-tutorial.git Computational Resources \u00b6 Available configurations for Jupyter over K8S After logging into the platform, a menu with up to three configuration options will be displayed. Simply select and click Start My Server . Size CPUs RAM Small 1.0 4 GiB Medium 2.0 8 GiB Large 4.0 16 GiB K8S environment server configuration The Jupyter platform runs on Kubernetes (K8S) and has 12 dedicated physical servers. Each machine is equipped with the following computational resources: Kubernetes Node Configuration RAM 64 GB Thread(s) per core 2 Core(s) per socket 6 Socket(s) 2 Jupyter over K8S vs Jupyter over HPC LIneA provides two separate Jupyter Notebook environments. The first runs in containers on the Kubernetes (K8S) platform. The second is available on the Ondemand platform and directly accesses the HPC infrastructure.","title":"JupyterHub"},{"location":"en/sci-platforms/jupyter.html#user-support","text":"","title":"User Support"},{"location":"en/sci-platforms/jupyter.html#tutorials-in-jupyter-notebooks","text":"In the jupyterhub-tutorial repository you will find tutorials in notebook format:","title":"Tutorials in Jupyter Notebooks"},{"location":"en/sci-platforms/jupyter.html#computational-resources","text":"Available configurations for Jupyter over K8S After logging into the platform, a menu with up to three configuration options will be displayed. Simply select and click Start My Server . Size CPUs RAM Small 1.0 4 GiB Medium 2.0 8 GiB Large 4.0 16 GiB K8S environment server configuration The Jupyter platform runs on Kubernetes (K8S) and has 12 dedicated physical servers. Each machine is equipped with the following computational resources: Kubernetes Node Configuration RAM 64 GB Thread(s) per core 2 Core(s) per socket 6 Socket(s) 2 Jupyter over K8S vs Jupyter over HPC LIneA provides two separate Jupyter Notebook environments. The first runs in containers on the Kubernetes (K8S) platform. The second is available on the Ondemand platform and directly accesses the HPC infrastructure.","title":"Computational Resources"},{"location":"en/sci-platforms/linea-occulation-prediction-database.html","text":"The LIneA Occultation Prediction Database is a scientific service that provides systematic and updated access to predictions of stellar occultations by minor Solar System bodies. Developed by LIneA's Solar System Portal, the system integrates updated orbital data with the Gaia DR3 stellar catalog, using specialized algorithms to calculate ephemerides and occultation circumstances. The service is designed to serve both professional research applications and citizen science projects. Main Features \u00b6 The platform provides programmatic access via a RESTful API with endpoints for querying by object identification (name or designation), dynamic classes (Skybot classification), stellar parameters (magnitude), geographic filters, etc. The interactive web interface allows tabular viewing of events, display of dynamic prediction maps, and download of SORA maps and KMZ files. Additionally, the service provides a Python package ( lineaSSP ) for integration into scientific workflows and a subscription service for customized event notifications. Documentation and Access \u00b6 For detailed information about API parameters and usage examples, consult the complete documentation . Access the service directly: LIneA Occultation Prediction Database","title":"Occultation Predictions Database"},{"location":"en/sci-platforms/linea-occulation-prediction-database.html#main-features","text":"The platform provides programmatic access via a RESTful API with endpoints for querying by object identification (name or designation), dynamic classes (Skybot classification), stellar parameters (magnitude), geographic filters, etc. The interactive web interface allows tabular viewing of events, display of dynamic prediction maps, and download of SORA maps and KMZ files. Additionally, the service provides a Python package ( lineaSSP ) for integration into scientific workflows and a subscription service for customized event notifications.","title":"Main Features"},{"location":"en/sci-platforms/linea-occulation-prediction-database.html#documentation-and-access","text":"For detailed information about API parameters and usage examples, consult the complete documentation . Access the service directly: LIneA Occultation Prediction Database","title":"Documentation and Access"},{"location":"en/sci-platforms/manga.html","text":"The MaNGA portal ( manga.linea.org.br ) was developed to meet the needs of members of the Brazilian Participation Group in the MaNGA survey of the Sloan Digital Sky Survey. The system was designed to allow the team to visualize not only the reduced IFU (Integral Field Units) spectroscopy data cubes, but also the results of data analysis showing maps of various physical quantities derived from the spectra. For more information on how to use the service, see the tutorials on the MaNGA portal website.","title":"MaNGA Portal"},{"location":"en/sci-platforms/ondemand.html","text":"OnDemand ( ondemand.linea.org.br ) is an interface that facilitates access to the high-performance computing environment through a web browser. The service offers: JupyterLab integrated with the Apollo cluster File management tool Access to the environment via Linux Terminal Click here for more details on how to use the OnDemand service.","title":"OnDemand"},{"location":"en/sci-platforms/pz_server.html","text":"Introduction \u00b6 Inspired by the DES Science Portal ( Gschwend et al., 2018 ; Fausti Neto et al., 2018 ), the Photo-z Server is an online service complementary to the Rubin Science Platform (RSP) to host and produce photo-z-related lightweight data products and to offer data management tools that allow sharing data products among RSP users, attach and share relevant metadata, and assist in provenance tracking. The service is hosted at the Brazilian Independent Data Access Center ( IDAC ) and is open to the whole LSST Community without geographic constraints. It is designed to be as broad and generic as possible to be helpful for all LSST Science Collaborations working with photo-z data products. As required by the LSST in-kind program, the source code will be publicly available on GitHub . The Photo-z Server was conceived to support RSP users participating in the Photo-z (PZ) Validation Cooperative, an initiative led by the Data Management (DM) team during the LSST commissioning phase (see technical note DMTN-049 for details). Members of the PZ Coordination Group will be granted administrative credentials with special permissions to upload data products marked as official data products . These will include standardized training and validation sets used for algorithm performance comparisons, as well as a means to collect results from multiple users. Beyond the PZ Validation Cooperative, the Photo-z Server will remain a resource for the LSST Community in the years to follow. RSP users can continue using it to organize, track, and share lightweight files containing diverse test results. Datasets The Photo-z Server administrators maintain and periodically update a curated list of data resources to support the LSST Community with photo-z-related data products. Detailed descriptions and links to each data product are available on a separate page . Photo-z Server website \u00b6 The main user interface of the Photo-z Server website is its website at pzserver.linea.org.br . The three cards on the landing page lead to the list of data products (left and center) or to the Photo-z Server pipelines (right). On the data products list page, users can browse, search and filter the products uploaded by users or created with a Photo-z Server pipeline. The data products uploaded on the Photo-z Server become automatically visible, downloadable and shareable to all registered users. Upload a new data product \u00b6 To upload a new data product, click the NEW PRODUCT button at the top right of the User-generated Data Products page and fill in the upload form with relevant metadata in four steps: Step 1: Inform a short and mnemonic name for your new data product. Select the type of data product you are uploading (e.g., Reference Redshift Catalog, Training Set, etc.) and the data release to which it belongs (if applicable). Step 2: Select your main file and as many auxiliary files as you want to upload. The main file is the one containing the data product itself, while auxiliary files can include documentation, description, or any other relevant information about the data product. If the data product is tabular, the upload tool might require specific file formats depending on its type. The formats currently supported are: CSV, FITS, HDF5, and Parquet. Get in touch with the development team if your science case requires a different file format or if your file is larger than the limit of 200MB. Step 3: If the data product is a Reference Redshift Catalog or Training Set, some columns are mandatory. The columns names are free, but you must provide the association with their meaning and UCDs in the IVOA standard as in the figure below. Step 4: Revise your information and go back to the previous steps if necessary. Don't forget to press the FINISH button at the bottom of the page to submit your data product. Download a data product \u00b6 To download a data product, click on the icon on the product's row in the User-generated Data Products page . The click will trigger the preparation of a compressed .zip file with all the contents of the data product, including auxiliary description files. There is also a button on the product's details page, which can be accessed by clicking on the product's name on the list. Share data products \u00b6 To share a data product, click on the icon on the product's row in the User-generated Data Products page or on the product's details page. The click will open a pop-up window with the product's internal_name and URL address. You can copy the information to share it with other users. internal_name Each data product has a unique name (\" internal_name \") automatically composed by the system as a unique id number followed by the name chosen by the user with spaces replaced by underlines. This name is the URL address of the data product's details page on the Photo-z Server website: https://pzserver.linea.org.br/product/internal_name and is the key to access the data using the Photo-z Server Python API (see details below). The easiest way to share a data product is by providing the product's internal_name or URL, which leads to the product's download page. Product types \u00b6 Reference Redshift Catalog \u00b6 In the context of the Photo-z Server, Reference Redshift Catalogs are defined as any catalog containing spherical equatorial coordinates and redshift measurements (usually spectroscopic or true redshifts for simulations). Mandatory columns: Right ascension [degrees] - float Declination [degrees] - float Redshift - float Recommended column: Redshift error - float A Reference Redshift Catalog can include data from a single spectroscopic survey or a combination of data from several sources. Pipeline requirements If a Reference Redshift Catalog is intended to be used as input data for the Combine Redshift Catalogs , applying the duplicates resolution feature (see pipeline details here ), it is recommended to include the following columns: Quality flag (associate with z_flag in upload step 3) - integer , float , or string (the original quality flag from the source catalog, when available) Measurement type - string (e.g., \"s\" for \"spectroscopic\", \"g\" for \"grism/prism\", \"p\" for \"photometric\", as adopted in SITCOMTN-154 ) Survey name (associate with survey in upload step 3) - string (e.g., \"DESI\", \"COSMOS2025\", \"JADES\", etc.) Other columns with additional information you want to use for duplicates resolution (e.g., instrument resolution). Training Set \u00b6 In the context of the Photo-z Server, Training Sets are defined as the product of the spatial cross-matching between a given Reference Redshift Catalog (single survey or compilation) and the photometric data, in this case, the LSST Object Catalog. The Photo-z Server's Training Set Maker pipeline allows users to build customized Training Sets based on the available Reference Redshift Catalogs (see pipeline details here ). train/test subsets Training sets are commonly split into two or more subsets for photo-z validation purposes. If the Training Set owner has previously defined which objects should belong to each subset (training and validation/test sets), this information must be available as an extra column in the table or as clear instructions for reproducing the subset separation in the data product description. For two separate files each one must be uploaded separately and will become an independent data product, both with product type set as \"Training Set\", and their destination can be explicitly informed in the product name and/or description. image-based training sets The product type Training Set only supports catalog-level training sets. Image-based training sets commonly used by deep-learning algorithms are not supported. For this case, use the product type \"Other\" and provide a clear description of the data format in the product description. To ensure flexibility in the observables, the only mandatory column is the redshift ( float ). Other expected columns are: objectId from LSST Objects Catalog - integer Observables (e.g., magnitudes and/or colors, or fluxes) from the LSST Object Catalog - float Observable errors - float Right ascension [degrees] - float Declination [degrees] - float Quality Flag - integer , float , or string Subset Flag - integer , float , or string Training Results \u00b6 The training results of machine learning-based algorithms can also be hosted in the Photo-z Server to be shared and reused. This product type allows files in free format. When the training results are generated with RAIL's inform method , they are stored as pickle files. Validation Results \u00b6 The product type Validation Results is intended to identify the results of any photo-z validation procedure. It can be used to store the results of the PZ Validation Cooperative or any other validation tasks. This type of product is quite generic. It might contain photo-z estimates (single estimates and/or PDF) of a test set, photo-z validation metrics, QQ-PIT plots, etc. Users can upload one main file and a list of auxiliary files in any format. Photo-z Estimates \u00b6 Photo-z Estimates are the results of any photo-z estimation procedure, usually the output of RAIL's estimate method . If the data is larger than the file upload limit (200MB), the product entry stores only the metadata and instructions on accessing the data should be provided in the description field. Other \u00b6 Any other data product that does not fit in the previous categories can be uploaded as a product of type Other. This is a generic product type that allows users to upload any file format and provide a description of the data product in the description field. API & Python library \u00b6 The Photo-z Server also offers an API and a Python library to facilitate the command-line access of data and metadata. The API contains functions to explore the data products available, retrieve the contents of a given data product to work on memory or download the files of interest. The Python package pzserver is open source available on GitHub and is installable via pip with: pip install pzserver Tutorial notebook \u00b6 A tutorial notebook with examples for all pzserver methods is available on the pzserver library's repository on GitHub . There is also the API documentation page with further details targeted for developers. Access token \u00b6 Once installed and imported in a Python environment, the PzServer class opens the remote connection to the Photo-z Server database. from pzserver import PzServer pz_server = PzServer ( token = \"<paste your access token here>\" ) An access token is required for authentication. Users can generate the token on the Photo-z Server website (top right corner menu on the home page). Basic commands \u00b6 Basic commands to display data and metadata in a Jupyter notebook cell (if not in a Jupyter notebook, replace display for get to return the results as Python dictionaries): pz_server . display_product_types () pz_server . display_releases () pz_server . display_products_list () pz_server . display_products_list ( filters = { \"release\" : \"DP1\" , \"product_type\" : \"Training Set\" }) search_results = pz_server . get_products_list ( filters = { \"product_type\" : \"results\" }) pz_server . display_product_metadata ( id or \"internal_name\" ) Basic commands to download or retrieve data to memory: pz_server . download_product ( id or \"internal_name\" , save_in = \".\" ) data = pz_server . get_product ( id or \"internal_name\" ) See the tutorial notebook for the complete list of examples, including instructions to upload and modify data products via the pzserver library. Photo-z Server pipelines \u00b6 The Photo-z Server pipelines are a set of tools to help users create and manage data products. The pipelines available now are (click on the links for more details): Combine Redshift Catalogs \u00b6 Training Set Maker \u00b6 Open Source Code \u00b6 The Photo-z Server is an open-source project. Its source code is available in the following GitHub repositories: pzserver_app : the main application code, including the web interface and API. pzserver : the Python library used to access the Photo-z Server API. pzserver_pipelines : the pipeline code available on the Photo-z Server. orchestration : the application responsible for submitting pipelines to the IDAC HPC cluster and managing their execution. pz-lsst-inkind : code for data management tasks in the Photo-z Server's in-kind program, including data preparation, quality assurance, and pipeline validation notebooks. pz-lsst-inkind-doc : high-level documentation for the Photo-z Server in-kind program, published via GitHub Pages. The code is licensed under the MIT License . Contributions are welcome! Acknowledgement \u00b6 The Photo-z Server uses computational resources of IDAC-Brazil at the Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA) with financial support from INCT do e-Universo (Process no. 465376/2014-2) and FINEP project: LIneA: e-Science Center for exploring the mysteries of the Universe and support Big Data projects (ref no. 0883/24).","title":"LSST Photo-z Server"},{"location":"en/sci-platforms/pz_server.html#introduction","text":"Inspired by the DES Science Portal ( Gschwend et al., 2018 ; Fausti Neto et al., 2018 ), the Photo-z Server is an online service complementary to the Rubin Science Platform (RSP) to host and produce photo-z-related lightweight data products and to offer data management tools that allow sharing data products among RSP users, attach and share relevant metadata, and assist in provenance tracking. The service is hosted at the Brazilian Independent Data Access Center ( IDAC ) and is open to the whole LSST Community without geographic constraints. It is designed to be as broad and generic as possible to be helpful for all LSST Science Collaborations working with photo-z data products. As required by the LSST in-kind program, the source code will be publicly available on GitHub . The Photo-z Server was conceived to support RSP users participating in the Photo-z (PZ) Validation Cooperative, an initiative led by the Data Management (DM) team during the LSST commissioning phase (see technical note DMTN-049 for details). Members of the PZ Coordination Group will be granted administrative credentials with special permissions to upload data products marked as official data products . These will include standardized training and validation sets used for algorithm performance comparisons, as well as a means to collect results from multiple users. Beyond the PZ Validation Cooperative, the Photo-z Server will remain a resource for the LSST Community in the years to follow. RSP users can continue using it to organize, track, and share lightweight files containing diverse test results. Datasets The Photo-z Server administrators maintain and periodically update a curated list of data resources to support the LSST Community with photo-z-related data products. Detailed descriptions and links to each data product are available on a separate page .","title":"Introduction"},{"location":"en/sci-platforms/pz_server.html#photo-z-server-website","text":"The main user interface of the Photo-z Server website is its website at pzserver.linea.org.br . The three cards on the landing page lead to the list of data products (left and center) or to the Photo-z Server pipelines (right). On the data products list page, users can browse, search and filter the products uploaded by users or created with a Photo-z Server pipeline. The data products uploaded on the Photo-z Server become automatically visible, downloadable and shareable to all registered users.","title":"Photo-z Server website"},{"location":"en/sci-platforms/pz_server.html#upload-a-new-data-product","text":"To upload a new data product, click the NEW PRODUCT button at the top right of the User-generated Data Products page and fill in the upload form with relevant metadata in four steps: Step 1: Inform a short and mnemonic name for your new data product. Select the type of data product you are uploading (e.g., Reference Redshift Catalog, Training Set, etc.) and the data release to which it belongs (if applicable). Step 2: Select your main file and as many auxiliary files as you want to upload. The main file is the one containing the data product itself, while auxiliary files can include documentation, description, or any other relevant information about the data product. If the data product is tabular, the upload tool might require specific file formats depending on its type. The formats currently supported are: CSV, FITS, HDF5, and Parquet. Get in touch with the development team if your science case requires a different file format or if your file is larger than the limit of 200MB. Step 3: If the data product is a Reference Redshift Catalog or Training Set, some columns are mandatory. The columns names are free, but you must provide the association with their meaning and UCDs in the IVOA standard as in the figure below. Step 4: Revise your information and go back to the previous steps if necessary. Don't forget to press the FINISH button at the bottom of the page to submit your data product.","title":"Upload a new data product"},{"location":"en/sci-platforms/pz_server.html#download-a-data-product","text":"To download a data product, click on the icon on the product's row in the User-generated Data Products page . The click will trigger the preparation of a compressed .zip file with all the contents of the data product, including auxiliary description files. There is also a button on the product's details page, which can be accessed by clicking on the product's name on the list.","title":"Download a data product"},{"location":"en/sci-platforms/pz_server.html#share-data-products","text":"To share a data product, click on the icon on the product's row in the User-generated Data Products page or on the product's details page. The click will open a pop-up window with the product's internal_name and URL address. You can copy the information to share it with other users. internal_name Each data product has a unique name (\" internal_name \") automatically composed by the system as a unique id number followed by the name chosen by the user with spaces replaced by underlines. This name is the URL address of the data product's details page on the Photo-z Server website: https://pzserver.linea.org.br/product/internal_name and is the key to access the data using the Photo-z Server Python API (see details below). The easiest way to share a data product is by providing the product's internal_name or URL, which leads to the product's download page.","title":"Share data products"},{"location":"en/sci-platforms/pz_server.html#product-types","text":"","title":"Product types"},{"location":"en/sci-platforms/pz_server.html#reference-redshift-catalog","text":"In the context of the Photo-z Server, Reference Redshift Catalogs are defined as any catalog containing spherical equatorial coordinates and redshift measurements (usually spectroscopic or true redshifts for simulations). Mandatory columns: Right ascension [degrees] - float Declination [degrees] - float Redshift - float Recommended column: Redshift error - float A Reference Redshift Catalog can include data from a single spectroscopic survey or a combination of data from several sources. Pipeline requirements If a Reference Redshift Catalog is intended to be used as input data for the Combine Redshift Catalogs , applying the duplicates resolution feature (see pipeline details here ), it is recommended to include the following columns: Quality flag (associate with z_flag in upload step 3) - integer , float , or string (the original quality flag from the source catalog, when available) Measurement type - string (e.g., \"s\" for \"spectroscopic\", \"g\" for \"grism/prism\", \"p\" for \"photometric\", as adopted in SITCOMTN-154 ) Survey name (associate with survey in upload step 3) - string (e.g., \"DESI\", \"COSMOS2025\", \"JADES\", etc.) Other columns with additional information you want to use for duplicates resolution (e.g., instrument resolution).","title":"Reference Redshift Catalog"},{"location":"en/sci-platforms/pz_server.html#training-set","text":"In the context of the Photo-z Server, Training Sets are defined as the product of the spatial cross-matching between a given Reference Redshift Catalog (single survey or compilation) and the photometric data, in this case, the LSST Object Catalog. The Photo-z Server's Training Set Maker pipeline allows users to build customized Training Sets based on the available Reference Redshift Catalogs (see pipeline details here ). train/test subsets Training sets are commonly split into two or more subsets for photo-z validation purposes. If the Training Set owner has previously defined which objects should belong to each subset (training and validation/test sets), this information must be available as an extra column in the table or as clear instructions for reproducing the subset separation in the data product description. For two separate files each one must be uploaded separately and will become an independent data product, both with product type set as \"Training Set\", and their destination can be explicitly informed in the product name and/or description. image-based training sets The product type Training Set only supports catalog-level training sets. Image-based training sets commonly used by deep-learning algorithms are not supported. For this case, use the product type \"Other\" and provide a clear description of the data format in the product description. To ensure flexibility in the observables, the only mandatory column is the redshift ( float ). Other expected columns are: objectId from LSST Objects Catalog - integer Observables (e.g., magnitudes and/or colors, or fluxes) from the LSST Object Catalog - float Observable errors - float Right ascension [degrees] - float Declination [degrees] - float Quality Flag - integer , float , or string Subset Flag - integer , float , or string","title":"Training Set"},{"location":"en/sci-platforms/pz_server.html#training-results","text":"The training results of machine learning-based algorithms can also be hosted in the Photo-z Server to be shared and reused. This product type allows files in free format. When the training results are generated with RAIL's inform method , they are stored as pickle files.","title":"Training Results"},{"location":"en/sci-platforms/pz_server.html#validation-results","text":"The product type Validation Results is intended to identify the results of any photo-z validation procedure. It can be used to store the results of the PZ Validation Cooperative or any other validation tasks. This type of product is quite generic. It might contain photo-z estimates (single estimates and/or PDF) of a test set, photo-z validation metrics, QQ-PIT plots, etc. Users can upload one main file and a list of auxiliary files in any format.","title":"Validation Results"},{"location":"en/sci-platforms/pz_server.html#photo-z-estimates","text":"Photo-z Estimates are the results of any photo-z estimation procedure, usually the output of RAIL's estimate method . If the data is larger than the file upload limit (200MB), the product entry stores only the metadata and instructions on accessing the data should be provided in the description field.","title":"Photo-z Estimates"},{"location":"en/sci-platforms/pz_server.html#other","text":"Any other data product that does not fit in the previous categories can be uploaded as a product of type Other. This is a generic product type that allows users to upload any file format and provide a description of the data product in the description field.","title":"Other"},{"location":"en/sci-platforms/pz_server.html#api-python-library","text":"The Photo-z Server also offers an API and a Python library to facilitate the command-line access of data and metadata. The API contains functions to explore the data products available, retrieve the contents of a given data product to work on memory or download the files of interest. The Python package pzserver is open source available on GitHub and is installable via pip with: pip install pzserver","title":"API &amp; Python library"},{"location":"en/sci-platforms/pz_server.html#tutorial-notebook","text":"A tutorial notebook with examples for all pzserver methods is available on the pzserver library's repository on GitHub . There is also the API documentation page with further details targeted for developers.","title":"Tutorial notebook"},{"location":"en/sci-platforms/pz_server.html#access-token","text":"Once installed and imported in a Python environment, the PzServer class opens the remote connection to the Photo-z Server database. from pzserver import PzServer pz_server = PzServer ( token = \"<paste your access token here>\" ) An access token is required for authentication. Users can generate the token on the Photo-z Server website (top right corner menu on the home page).","title":"Access token"},{"location":"en/sci-platforms/pz_server.html#basic-commands","text":"Basic commands to display data and metadata in a Jupyter notebook cell (if not in a Jupyter notebook, replace display for get to return the results as Python dictionaries): pz_server . display_product_types () pz_server . display_releases () pz_server . display_products_list () pz_server . display_products_list ( filters = { \"release\" : \"DP1\" , \"product_type\" : \"Training Set\" }) search_results = pz_server . get_products_list ( filters = { \"product_type\" : \"results\" }) pz_server . display_product_metadata ( id or \"internal_name\" ) Basic commands to download or retrieve data to memory: pz_server . download_product ( id or \"internal_name\" , save_in = \".\" ) data = pz_server . get_product ( id or \"internal_name\" ) See the tutorial notebook for the complete list of examples, including instructions to upload and modify data products via the pzserver library.","title":"Basic commands"},{"location":"en/sci-platforms/pz_server.html#photo-z-server-pipelines","text":"The Photo-z Server pipelines are a set of tools to help users create and manage data products. The pipelines available now are (click on the links for more details):","title":"Photo-z Server pipelines"},{"location":"en/sci-platforms/pz_server.html#combine-redshift-catalogs","text":"","title":"Combine Redshift Catalogs"},{"location":"en/sci-platforms/pz_server.html#open-source-code","text":"The Photo-z Server is an open-source project. Its source code is available in the following GitHub repositories: pzserver_app : the main application code, including the web interface and API. pzserver : the Python library used to access the Photo-z Server API. pzserver_pipelines : the pipeline code available on the Photo-z Server. orchestration : the application responsible for submitting pipelines to the IDAC HPC cluster and managing their execution. pz-lsst-inkind : code for data management tasks in the Photo-z Server's in-kind program, including data preparation, quality assurance, and pipeline validation notebooks. pz-lsst-inkind-doc : high-level documentation for the Photo-z Server in-kind program, published via GitHub Pages. The code is licensed under the MIT License . Contributions are welcome!","title":"Open Source Code"},{"location":"en/sci-platforms/pz_server.html#acknowledgement","text":"The Photo-z Server uses computational resources of IDAC-Brazil at the Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA) with financial support from INCT do e-Universo (Process no. 465376/2014-2) and FINEP project: LIneA: e-Science Center for exploring the mysteries of the Universe and support Big Data projects (ref no. 0883/24).","title":"Acknowledgement"},{"location":"en/sci-platforms/pz_server_crc.html","text":"Combine Redshift Catalogs \u00b6 The Combine Redshift Catalogs pipeline allows users to generate a single, unified redshift sample by combining multiple individual redshift catalogs. It uses the LSDB library, developed by the LINCC Frameworks , to perform spatial crossmatching between catalogs and identify multiple measurements of the same galaxy. The pipeline offers flexible options to resolve or retain duplicates. This process is especially useful for preparing clean redshift samples that can be used as training sets, validation sets, or calibration inputs for photometric redshift (photo-z) estimation. How to Run the Pipeline \u00b6 Photo-z Server website \u00b6 When running the pipeline from the Photo-z Server GUI, users must provide the following: Combined catalog name A short mnemonic name that will be used to register the result in the system, and to find it in the products list in future searches. There is no need to choose a unique name, as the system will automatically append the ID number (automatically generated) to the product's internal name to ensure uniqueness. Description (optional) Any description or notes about the sample. Select Redshift Catalogs Choose two or more catalogs already available in the system. Resolve duplicates No : Simply concatenate the catalogs, stacking the columns according to the meaning assigned during column association at upload time ( step 3 of the \"Upload a new data product\" section ). Yes, but keep all : Identify duplicates and add tie_result information, but retain all entries. Yes, and remove duplicates : Identify and keep only the best measurement per galaxy (only rows with tie_result == 1 ). Output format Choose one: Parquet, HDF5, CSV, or FITS. Once these fields are filled in, users can click Run to execute the pipeline. Photo-z Server API \u00b6 Warning section under construction Interpreting tie_result \u00b6 When duplicate resolution is enabled, the pipeline assigns a value to the tie_result column: 1 : Best entry (unique object or the winner of a tie-break) 0 : Discarded entry (loser of a tie-break) 2 : Hard tie (unresolved) This information is especially relevant when choosing the \u201cYes, but keep all\u201d or \u201cYes, and remove duplicates\u201d options. Warning To enable duplicate resolution, your input catalogs must contain either : The columns z_flag_homogenized (numeric) and instrument_type_homogenized (string), or A column named survey with survey names supported by the internal translation system ( see the flag_translation.yaml file ). These fields are used in the tie-breaking logic. If neither the homogenized columns nor a valid survey column are present, the pipeline will fail . When a survey column is provided, the pipeline attempts to infer the homogenized columns on a row-by-row basis using internal translation rules. Therefore, different survey names can coexist in the same file \u2014 as long as each individual value is recognized. If at least one row contains a survey name that is not listed in the translation file, the pipeline will raise an error . Standard system for tie-breaking : z_flag_homogenized must be a numeric column using the following values: 0 : No redshift 1 : Low confidence (<70%) 2 : Medium confidence (70\u201390%) 3 : High confidence (90\u201399%) 4 : Very high confidence (>99%) 6 : Star (non-extragalactic) instrument_type_homogenized must be a string column using: s : Spectroscopic g : Grism p : Photometric If these columns exist but are incorrectly typed, the pipeline may crash or behave incorrectly. \ud83d\udca1 Future improvement : In future versions, we plan to support user-defined translation files and tie-breaking rules. This will allow users to: Customize the order and content of tie-breaker columns (via tiebreaking_priority ) Use additional or alternative numeric columns (e.g., quality scores or custom metrics) for resolving ties Add support for new surveys by providing their own translation rules for quality flags and instrument types","title":"Combine Redshift Catalogs"},{"location":"en/sci-platforms/pz_server_crc.html#combine-redshift-catalogs","text":"The Combine Redshift Catalogs pipeline allows users to generate a single, unified redshift sample by combining multiple individual redshift catalogs. It uses the LSDB library, developed by the LINCC Frameworks , to perform spatial crossmatching between catalogs and identify multiple measurements of the same galaxy. The pipeline offers flexible options to resolve or retain duplicates. This process is especially useful for preparing clean redshift samples that can be used as training sets, validation sets, or calibration inputs for photometric redshift (photo-z) estimation.","title":"Combine Redshift Catalogs"},{"location":"en/sci-platforms/pz_server_crc.html#how-to-run-the-pipeline","text":"","title":"How to Run the Pipeline"},{"location":"en/sci-platforms/pz_server_crc.html#photo-z-server-website","text":"When running the pipeline from the Photo-z Server GUI, users must provide the following: Combined catalog name A short mnemonic name that will be used to register the result in the system, and to find it in the products list in future searches. There is no need to choose a unique name, as the system will automatically append the ID number (automatically generated) to the product's internal name to ensure uniqueness. Description (optional) Any description or notes about the sample. Select Redshift Catalogs Choose two or more catalogs already available in the system. Resolve duplicates No : Simply concatenate the catalogs, stacking the columns according to the meaning assigned during column association at upload time ( step 3 of the \"Upload a new data product\" section ). Yes, but keep all : Identify duplicates and add tie_result information, but retain all entries. Yes, and remove duplicates : Identify and keep only the best measurement per galaxy (only rows with tie_result == 1 ). Output format Choose one: Parquet, HDF5, CSV, or FITS. Once these fields are filled in, users can click Run to execute the pipeline.","title":"Photo-z Server website"},{"location":"en/sci-platforms/pz_server_crc.html#photo-z-server-api","text":"Warning section under construction","title":"Photo-z Server API"},{"location":"en/sci-platforms/pz_server_crc.html#interpreting-tie_result","text":"When duplicate resolution is enabled, the pipeline assigns a value to the tie_result column: 1 : Best entry (unique object or the winner of a tie-break) 0 : Discarded entry (loser of a tie-break) 2 : Hard tie (unresolved) This information is especially relevant when choosing the \u201cYes, but keep all\u201d or \u201cYes, and remove duplicates\u201d options. Warning To enable duplicate resolution, your input catalogs must contain either : The columns z_flag_homogenized (numeric) and instrument_type_homogenized (string), or A column named survey with survey names supported by the internal translation system ( see the flag_translation.yaml file ). These fields are used in the tie-breaking logic. If neither the homogenized columns nor a valid survey column are present, the pipeline will fail . When a survey column is provided, the pipeline attempts to infer the homogenized columns on a row-by-row basis using internal translation rules. Therefore, different survey names can coexist in the same file \u2014 as long as each individual value is recognized. If at least one row contains a survey name that is not listed in the translation file, the pipeline will raise an error . Standard system for tie-breaking : z_flag_homogenized must be a numeric column using the following values: 0 : No redshift 1 : Low confidence (<70%) 2 : Medium confidence (70\u201390%) 3 : High confidence (90\u201399%) 4 : Very high confidence (>99%) 6 : Star (non-extragalactic) instrument_type_homogenized must be a string column using: s : Spectroscopic g : Grism p : Photometric If these columns exist but are incorrectly typed, the pipeline may crash or behave incorrectly. \ud83d\udca1 Future improvement : In future versions, we plan to support user-defined translation files and tie-breaking rules. This will allow users to: Customize the order and content of tie-breaker columns (via tiebreaking_priority ) Use additional or alternative numeric columns (e.g., quality scores or custom metrics) for resolving ties Add support for new surveys by providing their own translation rules for quality flags and instrument types","title":"Interpreting tie_result"},{"location":"en/sci-platforms/pz_server_tsm.html","text":"Training Set Maker \u00b6 The Training Set Maker pipeline performs the spatial cross-matching between a pre-registered Redshift Catalog and the LSST Object catalog in order to create training sets for machine-learning based photo-z algorithms. It relies on the LSDB library, developed by the LINCC Frameworks to handle the spatially distributed data efficiently and provide flexibility in the observables included in the training set. Run via Photo-z Server website \u00b6 The pipeline can be run through the Photo-z Server website, which provides a user-friendly interface for configuring the cross-matching parameters, selecting the desired output contents and format. When running the pipeline from the Photo-z Server UI, users must provide the following information: Training set name A short mnemonic name that will be used to register the result in the system, and to find it in the products list in future searches. There is no need to choose a unique name, as the system will automatically append the ID number (automatically generated) to the product's internal name to ensure uniqueness. Description (optional) Any description or notes about the sample. Select the Redshift Catalog for the cross-matching Choose from the available registered Reference Redshift Catalogs listed on the menu. It can be either the result of a previous run of the Combine Redshift Catalogs pipeline or a custom catalog previously uploaded by the user. Select the Object Catalog (photometric data) Choose between the LSST releases available, e.g., DP0.2, DP1, etc. Flux type : Select which flux columns to use during the crossmatch. The options are dynamically populated based on the selected Object Catalog, e.g., cModel , gaap1p0 , psf , etc. Apply dereddening from dustmaps Select which dust map (if any) to use for dereddening the photometric fluxes. Convert fluxes into magnitudes Check this box to convert the fluxes into magnitudes. The conversion is done using the formula: \\[ mag = -2.5 * \\log_{10}(flux) + 31.5 \\] where \\(flux\\) is the value in the selected flux column. Select the cross-matching configuration choices Threshold distance (arcsec) : maximum allowed distance between matched sources. Number of neighbors : number of closest matches to retrieve. Select 1 for a one-to-one match, or a higher number to retrieve multiple matches. Select unique galaxies (coming soon) In case of multiple matchings, apply a given deduplication criteria (this option is not yet implemented). Output format Choose from the list of supported formats: Parquet, CSV, FITS, or HDF5. Example 1: Training Set with simulated galaxies from DP0.2 \u00b6 Warning section under construction Example 2: Training Set with observed galaxies from DP1 \u00b6 Warning section under construction Run via Photo-z Server API \u00b6 Warning section under construction","title":"Training Set Maker"},{"location":"en/sci-platforms/pz_server_tsm.html#training-set-maker","text":"The Training Set Maker pipeline performs the spatial cross-matching between a pre-registered Redshift Catalog and the LSST Object catalog in order to create training sets for machine-learning based photo-z algorithms. It relies on the LSDB library, developed by the LINCC Frameworks to handle the spatially distributed data efficiently and provide flexibility in the observables included in the training set.","title":"Training Set Maker"},{"location":"en/sci-platforms/pz_server_tsm.html#run-via-photo-z-server-website","text":"The pipeline can be run through the Photo-z Server website, which provides a user-friendly interface for configuring the cross-matching parameters, selecting the desired output contents and format. When running the pipeline from the Photo-z Server UI, users must provide the following information: Training set name A short mnemonic name that will be used to register the result in the system, and to find it in the products list in future searches. There is no need to choose a unique name, as the system will automatically append the ID number (automatically generated) to the product's internal name to ensure uniqueness. Description (optional) Any description or notes about the sample. Select the Redshift Catalog for the cross-matching Choose from the available registered Reference Redshift Catalogs listed on the menu. It can be either the result of a previous run of the Combine Redshift Catalogs pipeline or a custom catalog previously uploaded by the user. Select the Object Catalog (photometric data) Choose between the LSST releases available, e.g., DP0.2, DP1, etc. Flux type : Select which flux columns to use during the crossmatch. The options are dynamically populated based on the selected Object Catalog, e.g., cModel , gaap1p0 , psf , etc. Apply dereddening from dustmaps Select which dust map (if any) to use for dereddening the photometric fluxes. Convert fluxes into magnitudes Check this box to convert the fluxes into magnitudes. The conversion is done using the formula: \\[ mag = -2.5 * \\log_{10}(flux) + 31.5 \\] where \\(flux\\) is the value in the selected flux column. Select the cross-matching configuration choices Threshold distance (arcsec) : maximum allowed distance between matched sources. Number of neighbors : number of closest matches to retrieve. Select 1 for a one-to-one match, or a higher number to retrieve multiple matches. Select unique galaxies (coming soon) In case of multiple matchings, apply a given deduplication criteria (this option is not yet implemented). Output format Choose from the list of supported formats: Parquet, CSV, FITS, or HDF5.","title":"Run via Photo-z Server website"},{"location":"en/sci-platforms/pz_server_tsm.html#run-via-photo-z-server-api","text":"Warning section under construction","title":"Run via Photo-z Server API"},{"location":"en/sci-platforms/sci_server.html","text":"The DES Science Server is an image and catalog data visualization service developed as part of LIneA's contribution to the Dark Energy Survey (DES). The DES Science Server consists of independent microservices to meet different demands. For each of these microservices, a newer, more flexible and modern version is being developed to provide access to LSST project private data for members and public data from various surveys for the entire community, all in one place (see details about the new visualization services Sky Viewer and Target Viewer on the Scientific Platforms page). Its initial version is still available and provides access to DES DR2 images and tabular data. Sky Viewer \u00b6 Offers panoramic display of DES images, combined to form a single image limited only by the footprint edges, and maps in HEALPix format. Target Viewer \u00b6 Tool to visualize and manipulate images from a predefined list of objects (targets). Allows ranking images, applying filters based on object properties, and creating mosaics with multiple images. Tile Viewer \u00b6 Provides visualization of DES images displayed based on Tiles, the area unit adopted by the survey. The Tile Viewer was widely used for inspection and validation of data during periods preceding internal and public data releases.","title":"DES Science Server"},{"location":"en/sci-platforms/sci_server.html#sky-viewer","text":"Offers panoramic display of DES images, combined to form a single image limited only by the footprint edges, and maps in HEALPix format.","title":"Sky Viewer"},{"location":"en/sci-platforms/sci_server.html#target-viewer","text":"Tool to visualize and manipulate images from a predefined list of objects (targets). Allows ranking images, applying filters based on object properties, and creating mosaics with multiple images.","title":"Target Viewer"},{"location":"en/sci-platforms/sci_server.html#tile-viewer","text":"Provides visualization of DES images displayed based on Tiles, the area unit adopted by the survey. The Tile Viewer was widely used for inspection and validation of data during periods preceding internal and public data releases.","title":"Tile Viewer"},{"location":"en/sci-platforms/sdss_sky_server.html","text":"The SDSS Sky Server is an online portal developed by the Sloan Digital Sky Survey (SDSS) that provides public access to a vast collection of astronomical data collected by its surveys. It offers interactive tools to view sky images, explore spectra, and query catalogs of celestial objects such as stars, galaxies, and quasars. With a user-friendly interface and resources catering to both enthusiasts and professional researchers, the Sky Server also provides ways to access data through SQL queries, facilitating custom analyses directly on the SDSS database. LIneA hosts an SDSS Sky Server mirror ( skyserver.linea.org.br ), which provides access to DR17 release data.","title":"SDSS Sky Server"},{"location":"en/sci-platforms/sky_viewer.html","text":"The Sky Viewer ( skyviewer.linea.org.br ) is an in-development service based on the Aladin visualization tool that displays HIPS images and catalog overlays from various sources, including publicly available online data and public/private data hosted at LIneA. The list of available datasets depends on each user's access rights. Like the Target Viewer, the Sky Viewer was originally developed for the DES survey, and its initial version is still available on the DES Science Server page, providing access to DES DR2 images and tabular data. The new version, being developed as part of IDAC-Brazil's service portfolio, will be flexible and provide LSST data access for members, plus public data from other surveys for the general community.","title":"Sky Viewer"},{"location":"en/sci-platforms/solar-system-portal.html","text":"The study of small bodies in the Solar System presents considerable challenges, primarily due to their small sizes and vast distances. One method to overcome these obstacles involves indirect studies through the stellar occultation technique. Historically, accurate prediction of these events has been difficult due to the lack of sufficiently precise stellar maps. However, recent advances have transformed this scenario, enabling highly precise predictions of stellar occultations for Solar System bodies. The stellar occultation technique is crucial for studying these bodies, particularly trans-Neptunian objects (TNOs), providing accurate information about sizes and positions, environmental characteristics, etc. This is possible thanks to one of the technique's most notable features: the conversion of high temporal resolution into high angular resolution. With the expected tenfold increase in data volume from the Legacy Survey of Space and Time (LSST), the Solar System Portal emerges as a high-performance computing solution for: Calculating predictions for stellar occultation events; Organizing and distributing data; Making events globally accessible; Reducing the user's computational load; Access to the Solar System Portal and its services \u00b6 Public interface LIneA Occultation Prediction Database ( learn more here ) Documentation Restricted area Intended for members of the Transneptunian Occultation Network (TON) collaboration Learn more about TON here","title":"Solar System Portal"},{"location":"en/sci-platforms/solar-system-portal.html#access-to-the-solar-system-portal-and-its-services","text":"Public interface LIneA Occultation Prediction Database ( learn more here ) Documentation Restricted area Intended for members of the Transneptunian Occultation Network (TON) collaboration Learn more about TON here","title":"Access to the Solar System Portal and its services"},{"location":"en/sci-platforms/target_viewer.html","text":"The Target Viewer ( targetviewer.linea.org.br ) is an in-development service customized for viewing astronomical images based on a predefined target list specified by the user. Step by step: 1. Create a new target list as a table in the user's MyDB space in User Query . For the Target Viewer to locate the target images, the table must contain the columns objectId , ra , and dec . 2. Click the \"NEW CATALOG\" button on the Target Viewer homepage and complete the 3-step form to register the table as a target list. 3. After submitting the form, the target list will appear in the menu on the homepage. Like the Sky Viewer, the Target Viewer was originally developed for the DES survey, and its initial version is still available on the DES Science Server page, providing access to DES DR2 images and tabular data. The new version, being developed as part of IDAC-Brazil's service portfolio, will be flexible and provide LSST data access for members, plus public data from other surveys for the general community.","title":"Target Viewer"},{"location":"en/sci-platforms/user_query.html","text":"The User Query ( userquery.linea.org.br ) is a user-friendly interface for database queries that enables the creation of temporary tables. User-generated tables are managed by MyDB and can be immediately accessed in LIneA's JupyterHub or from anywhere via the TAP Service. Additionally, if a table contains equatorial coordinates and a column with unique identifiers, it automatically becomes a target list, allowing visualization of corresponding object images in the Target Viewer tool (LSST images for members and DES DR2 images for the general public). TAP Service \u00b6 The IVOA TAP Service (Table Access Protocol) is a standard that enables access and querying of astronomical databases through SQL language, in a standardized and interoperable way across different services. In addition to the User Query service, public catalogs hosted at LIneA can also be accessed remotely via the TAP Service using TOPCAT or programmatically through the Python library pyvo . Visit the TAP Service documentation on the User Query website for more details on how to use the service.","title":"User Query"},{"location":"en/sci-platforms/user_query.html#tap-service","text":"The IVOA TAP Service (Table Access Protocol) is a standard that enables access and querying of astronomical databases through SQL language, in a standardized and interoperable way across different services. In addition to the User Query service, public catalogs hosted at LIneA can also be accessed remotely via the TAP Service using TOPCAT or programmatically through the Python library pyvo . Visit the TAP Service documentation on the User Query website for more details on how to use the service.","title":"TAP Service"},{"location":"es/index.html","text":"Bienvenido/a a la p\u00e1gina de Documentaci\u00f3n para Usuarios de LIneA. Este es el lugar central para encontrar gu\u00edas de usuario y documentaci\u00f3n sobre todos los servicios y herramientas proporcionados por LIneA para la comunidad astron\u00f3mica y el p\u00fablico en general. Aqu\u00ed tambi\u00e9n encontrar\u00e1s enlaces a documentaci\u00f3n externa relevante. LIneA - Laboratorio Interinstitucional de e-Astronom\u00eda - es un laboratorio multiusuario, operado por una organizaci\u00f3n sin fines de lucro (Asociaci\u00f3n LIneA) con apoyo financiero proveniente principalmente del Ministerio de Ciencia, Tecnolog\u00eda e Innovaci\u00f3n de Brasil. Nuestra misi\u00f3n es trabajar para apoyar a la comunidad astron\u00f3mica brasile\u00f1a con infraestructura computacional y experiencia en an\u00e1lisis de big data para proporcionar las condiciones t\u00e9cnicas necesarias para la participaci\u00f3n en grandes estudios astron\u00f3micos, como el Sloan Digital Sky Survey (SDSS) , el Dark Energy Survey (DES) y el Legacy Survey of Space and Time (LSST) . Para saber m\u00e1s, consulta nuestros videos institucionales a continuaci\u00f3n, nuestro canal de YouTube o navega por nuestro sitio web institucional . \u00bfComentarios, dudas, sugerencias? Si encuentras algo faltante en esta documentaci\u00f3n, no dudes en abrir un issue en el Repositorio de Documentaci\u00f3n de LIneA en GitHub .","title":"Inicio"},{"location":"es/cursos.html","text":"LIneA ofrece cursos de capacitaci\u00f3n sobre las principales herramientas utilizadas por nuestros proyectos. Las grabaciones y materiales did\u00e1cticos de todos los cursos ofrecidos anteriormente est\u00e1n disponibles en Google Classroom. Visite la p\u00e1gina de cursos en el sitio web de LIneA para m\u00e1s detalles.","title":"Cursos y Tutoriales"},{"location":"es/faq.html","text":"C\u00f3mo crear un par de claves SSH \u00b6 Para acceder a nuestro entorno via ssh necesitas generar un par de claves siguiendo estos pasos: Linux \u00b6 a) Para generar el par de claves usa el siguiente comando en tu terminal: ssh-keygen -t rsa -b 4096 Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): [presiona ENTER] Created directory '/root/.ssh'. Enter passphrase (empty for no passphrase): [ingresa una contrase\u00f1a y presiona ENTER] Enter same passphrase again: [repite la contrase\u00f1a y presiona ENTER] b) Despu\u00e9s de recibir el mensaje de que la clave fue generada. Puedes ver los dos archivos creados listando el contenido del directorio: ls $HOME/.ssh id_rsa id_rsa.pub c) Despu\u00e9s de generar las claves, env\u00eda la clave .pub al equipo de TI por correo helpdesk@linea.org.br . El equipo de TI de LIneA configurar\u00e1 la clave en el servidor y te enviar\u00e1 instrucciones para acceder al cluster Apollo. Espera la confirmaci\u00f3n . Windows \u00b6 Para generar pares de claves en Windows: a) Descarga e instala la aplicaci\u00f3n Putty. b) Accede a la carpeta de instalaci\u00f3n (este ejemplo usa Windows 10) C:\\Program File\\PuTTY (la ruta puede variar seg\u00fan el SO), abre Puttygen. c) Haz clic en Generate (mant\u00e9n el tipo de clave como RSA ). NOTA: Mover el puntero del mouse ayuda a generar la clave m\u00e1s r\u00e1pido al crear bits aleatorios . d) Par de claves generado exitosamente. Copia la clave p\u00fablica para guardarla en el servidor (resaltado en amarillo en la imagen); Establece una contrase\u00f1a para la clave p\u00fablica (resaltado en azul); Despu\u00e9s de copiar, guarda ambas claves (p\u00fablica y privada) en tu computadora (resaltado en rojo) y env\u00eda la clave .pub al equipo de TI por correo helpdesk@linea.org.br . El equipo de TI de LIneA configurar\u00e1 la clave en el servidor. Espera la confirmaci\u00f3n . e) Despu\u00e9s de recibir el correo de confirmaci\u00f3n de que la clave .pub fue registrada en el servidor, configura el programa Putty . Crea un acceso directo en el escritorio, abre PuTTY ; Ingresa Hostname: login.linea.org.br. f) En el lado izquierdo ve a SSH > Auth (resaltado en azul) > haz clic en Browse (resaltado en amarillo) y selecciona el archivo de clave con extensi\u00f3n .ppk . h) Si necesitas usar un t\u00fanel, haz la siguiente configuraci\u00f3n. NOTA: los t\u00faneles se configuran seg\u00fan lo que el usuario necesite acceder Ve a la opci\u00f3n Tunnels (lado izquierdo); En Source port ingresa el n\u00famero de puerto; Destination > ingresa la direcci\u00f3n de destino > Add. Regresa al lado izquierdo y ve a la primera opci\u00f3n del men\u00fa Session (resaltado en rojo) ingresa un nombre para la sesi\u00f3n (resaltado en amarillo) y haz clic en Save (resaltado en azul) , para conectarte haz clic en Open .","title":"Faq"},{"location":"es/faq.html#como-crear-un-par-de-claves-ssh","text":"Para acceder a nuestro entorno via ssh necesitas generar un par de claves siguiendo estos pasos:","title":"C\u00f3mo crear un par de claves SSH"},{"location":"es/faq.html#linux","text":"a) Para generar el par de claves usa el siguiente comando en tu terminal: ssh-keygen -t rsa -b 4096 Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): [presiona ENTER] Created directory '/root/.ssh'. Enter passphrase (empty for no passphrase): [ingresa una contrase\u00f1a y presiona ENTER] Enter same passphrase again: [repite la contrase\u00f1a y presiona ENTER] b) Despu\u00e9s de recibir el mensaje de que la clave fue generada. Puedes ver los dos archivos creados listando el contenido del directorio: ls $HOME/.ssh id_rsa id_rsa.pub c) Despu\u00e9s de generar las claves, env\u00eda la clave .pub al equipo de TI por correo helpdesk@linea.org.br . El equipo de TI de LIneA configurar\u00e1 la clave en el servidor y te enviar\u00e1 instrucciones para acceder al cluster Apollo. Espera la confirmaci\u00f3n .","title":"Linux"},{"location":"es/faq.html#windows","text":"Para generar pares de claves en Windows: a) Descarga e instala la aplicaci\u00f3n Putty. b) Accede a la carpeta de instalaci\u00f3n (este ejemplo usa Windows 10) C:\\Program File\\PuTTY (la ruta puede variar seg\u00fan el SO), abre Puttygen. c) Haz clic en Generate (mant\u00e9n el tipo de clave como RSA ). NOTA: Mover el puntero del mouse ayuda a generar la clave m\u00e1s r\u00e1pido al crear bits aleatorios . d) Par de claves generado exitosamente. Copia la clave p\u00fablica para guardarla en el servidor (resaltado en amarillo en la imagen); Establece una contrase\u00f1a para la clave p\u00fablica (resaltado en azul); Despu\u00e9s de copiar, guarda ambas claves (p\u00fablica y privada) en tu computadora (resaltado en rojo) y env\u00eda la clave .pub al equipo de TI por correo helpdesk@linea.org.br . El equipo de TI de LIneA configurar\u00e1 la clave en el servidor. Espera la confirmaci\u00f3n . e) Despu\u00e9s de recibir el correo de confirmaci\u00f3n de que la clave .pub fue registrada en el servidor, configura el programa Putty . Crea un acceso directo en el escritorio, abre PuTTY ; Ingresa Hostname: login.linea.org.br. f) En el lado izquierdo ve a SSH > Auth (resaltado en azul) > haz clic en Browse (resaltado en amarillo) y selecciona el archivo de clave con extensi\u00f3n .ppk . h) Si necesitas usar un t\u00fanel, haz la siguiente configuraci\u00f3n. NOTA: los t\u00faneles se configuran seg\u00fan lo que el usuario necesite acceder Ve a la opci\u00f3n Tunnels (lado izquierdo); En Source port ingresa el n\u00famero de puerto; Destination > ingresa la direcci\u00f3n de destino > Add. Regresa al lado izquierdo y ve a la primera opci\u00f3n del men\u00fa Session (resaltado en rojo) ingresa un nombre para la sesi\u00f3n (resaltado en amarillo) y haz clic en Save (resaltado en azul) , para conectarte haz clic en Open .","title":"Windows"},{"location":"es/glossario.html","text":"Acceda a la lista de t\u00e9rminos t\u00e9cnicos y acr\u00f3nimos disponible en nuestro sitio web aqu\u00ed .","title":"Glosario"},{"location":"es/politicas.html","text":"Pol\u00edtica de Seguridad de la Informaci\u00f3n (PSI) \u00b6 La Pol\u00edtica de Seguridad de la Informaci\u00f3n (PSI) de LIneA define lineamientos estrat\u00e9gicos sobre c\u00f3mo se abordar\u00e1 la Seguridad de la Informaci\u00f3n en el entorno de la asociaci\u00f3n. Fue elaborada por el Comit\u00e9 de Gesti\u00f3n de Seguridad de la Informaci\u00f3n (CGSI/LIneA) con el apoyo del Centro de Atenci\u00f3n a Incidentes de Seguridad de RNP (CAIS/RNP), y cubre diversos aspectos de la seguridad de la informaci\u00f3n, definiendo roles y responsabilidades tanto para la Asociaci\u00f3n LIneA como para sus colaboradores y usuarios. Para acceder a la PSI haga clic aqu\u00ed Pol\u00edtica de uso del entorno HPC \u00b6 De conformidad con la Pol\u00edtica de Seguridad de la Informaci\u00f3n (PSI), LIneA tambi\u00e9n establece directrices para el uso del entorno de computaci\u00f3n de alto rendimiento. La Pol\u00edtica de Uso del Entorno HPC tiene como objetivo informar a los colaboradores y usuarios sobre la utilizaci\u00f3n de la infraestructura de manera responsable, eficiente y alineada con prop\u00f3sitos cient\u00edficos. Para acceder a la Pol\u00edtica de Uso del Entorno HPC, haga clic aqu\u00ed . Reconocimiento del Uso de Recursos Computacionales de LIneA \u00b6 Comprometerse a reconocer a LIneA en sus publicaciones, utilizando una cotizaci\u00f3n como: En ingl\u00e9s: \u201cThis research was supported by resources supplied by the Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\u201d O En portugu\u00e9s: \u201cEsta pesquisa tornou-se poss\u00edvel gra\u00e7as aos recursos computacionais disponibilizados pelo Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\u201d Incidentes de Seguridad \u00b6 Si sospecha que ha ocurrido alg\u00fan incidente de seguridad con su cuenta, servicio o aplicaci\u00f3n que est\u00e9 utilizando, debe contactar a LIneA inmediatamente en helpdesk@linea.org.br . Recomendamos guardar cualquier evidencia relacionada con el incidente (logs, mensajes, capturas de pantalla, etc.) e incluir tantos detalles como sea posible en su correo electr\u00f3nico. Pol\u00edtica de Privacidad \u00b6 Pr\u00f3ximamente Pol\u00edtica de Tratamiento de Datos \u00b6 Pr\u00f3ximamente","title":"Pol\u00edticas"},{"location":"es/politicas.html#politica-de-seguridad-de-la-informacion-psi","text":"La Pol\u00edtica de Seguridad de la Informaci\u00f3n (PSI) de LIneA define lineamientos estrat\u00e9gicos sobre c\u00f3mo se abordar\u00e1 la Seguridad de la Informaci\u00f3n en el entorno de la asociaci\u00f3n. Fue elaborada por el Comit\u00e9 de Gesti\u00f3n de Seguridad de la Informaci\u00f3n (CGSI/LIneA) con el apoyo del Centro de Atenci\u00f3n a Incidentes de Seguridad de RNP (CAIS/RNP), y cubre diversos aspectos de la seguridad de la informaci\u00f3n, definiendo roles y responsabilidades tanto para la Asociaci\u00f3n LIneA como para sus colaboradores y usuarios. Para acceder a la PSI haga clic aqu\u00ed","title":"Pol\u00edtica de Seguridad de la Informaci\u00f3n (PSI)"},{"location":"es/politicas.html#politica-de-uso-del-entorno-hpc","text":"De conformidad con la Pol\u00edtica de Seguridad de la Informaci\u00f3n (PSI), LIneA tambi\u00e9n establece directrices para el uso del entorno de computaci\u00f3n de alto rendimiento. La Pol\u00edtica de Uso del Entorno HPC tiene como objetivo informar a los colaboradores y usuarios sobre la utilizaci\u00f3n de la infraestructura de manera responsable, eficiente y alineada con prop\u00f3sitos cient\u00edficos. Para acceder a la Pol\u00edtica de Uso del Entorno HPC, haga clic aqu\u00ed .","title":"Pol\u00edtica de uso del entorno HPC"},{"location":"es/politicas.html#reconocimiento-del-uso-de-recursos-computacionales-de-linea","text":"Comprometerse a reconocer a LIneA en sus publicaciones, utilizando una cotizaci\u00f3n como: En ingl\u00e9s: \u201cThis research was supported by resources supplied by the Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\u201d O En portugu\u00e9s: \u201cEsta pesquisa tornou-se poss\u00edvel gra\u00e7as aos recursos computacionais disponibilizados pelo Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\u201d","title":"Reconocimiento del Uso de Recursos Computacionales de LIneA"},{"location":"es/politicas.html#incidentes-de-seguridad","text":"Si sospecha que ha ocurrido alg\u00fan incidente de seguridad con su cuenta, servicio o aplicaci\u00f3n que est\u00e9 utilizando, debe contactar a LIneA inmediatamente en helpdesk@linea.org.br . Recomendamos guardar cualquier evidencia relacionada con el incidente (logs, mensajes, capturas de pantalla, etc.) e incluir tantos detalles como sea posible en su correo electr\u00f3nico.","title":"Incidentes de Seguridad"},{"location":"es/politicas.html#politica-de-privacidad","text":"Pr\u00f3ximamente","title":"Pol\u00edtica de Privacidad"},{"location":"es/politicas.html#politica-de-tratamiento-de-datos","text":"Pr\u00f3ximamente","title":"Pol\u00edtica de Tratamiento de Datos"},{"location":"es/primeiros_passos.html","text":"Para acceder a las plataformas y servicios proporcionados por LIneA, necesitas tener un registro activo en nuestro entorno e iniciar sesi\u00f3n con una de las siguientes opciones: Inicio de sesi\u00f3n con credenciales institucionales - Recomendado para quienes tienen afiliaci\u00f3n institucional (estudiantes de pregrado, posgrado, investigadores, etc.). El inicio de sesi\u00f3n se realiza a trav\u00e9s de CILogon. Inicio de sesi\u00f3n con Google - Recomendado para astr\u00f3nomos aficionados, estudiantes de secundaria, entusiastas de la astronom\u00eda, etc. Accesible para cualquier persona con una cuenta de Gmail. Miembros de Colaboraciones Cient\u00edficas El inicio de sesi\u00f3n con credenciales institucionales es obligatorio para acceder a servicios exclusivos para miembros de las colaboraciones DES, DESI, LSST, SDSS y TON. \u00bfNo tienes una cuenta? \u00b6 Para registrarte como usuario en LIneA, sigue las instrucciones a continuaci\u00f3n: Registro para P\u00fablico General \u00b6 P\u00fablico con afiliaci\u00f3n institucional: Estudiantes de pregrado, posgrado, investigadores, etc., afiliados a una instituci\u00f3n reg\u00edstrate aqu\u00ed . P\u00fablico sin afiliaci\u00f3n institucional: Astr\u00f3nomos aficionados, estudiantes de secundaria, entusiastas de la astronom\u00eda, etc., reg\u00edstrate aqu\u00ed con inicio de sesi\u00f3n de Google . ATENCI\u00d3N: P\u00fablico Sin Afiliaci\u00f3n Institucional Debes buscar y seleccionar Google en el campo \"Select an Identity Provider\" en la p\u00e1gina inicial de CILogon. Registro para Miembros de Colaboraciones \u00b6 Los miembros de las colaboraciones DES, DESI, LSST, SDSS y TON deben completar el formulario de selecci\u00f3n a continuaci\u00f3n, llenando todos los campos requeridos y enviando el formulario al final. Solicitud de registro para miembros de Colaboraciones. Despu\u00e9s de revisar los datos, nos pondremos en contacto contigo por correo electr\u00f3nico. Para cualquier duda o problema, contacta a nuestro Service Desk .","title":"Primeros Pasos"},{"location":"es/primeiros_passos.html#no-tienes-una-cuenta","text":"Para registrarte como usuario en LIneA, sigue las instrucciones a continuaci\u00f3n:","title":"\u00bfNo tienes una cuenta?"},{"location":"es/suporte.html","text":"Para soporte t\u00e9cnico, contacta a nuestro Service Desk al correo helpdesk@linea.org.br . Si es necesario, nuestro equipo puede agendar una videollamada. Si deseas unirte a la comunidad LIneA y conversar con otros usuarios, haz clic aqu\u00ed para recibir una invitaci\u00f3n al espacio de trabajo LIneA Users en Slack.","title":"Soporte"},{"location":"es/armazenamento/index.html","text":"LustreFS (HPC) \u00b6 El entorno del cl\u00faster Apollo cuenta con un sistema de archivos de alto rendimiento Lustre con dos niveles (tiers) de almacenamiento: uno en SSD con ~70 TB (T0) y otro en HDD con ~500 TB (T1), ambos conectados a una red Infiniband EDR de 100 Gb/s. Los dos niveles de almacenamiento est\u00e1n disponibles en /scratch y /data . \u00c1rea de scratch y cuota \u00b6 Los usuarios pueden acceder a su directorio scratch mediante una variable de entorno o accediendo al directorio con la ruta completa. cd $SCRATCH O cd /scratch/users/<username> ATENCI\u00d3N \u00a1Esta \u00e1rea NO tendr\u00e1 respaldo! Los archivos que no se hayan modificado en los \u00faltimos 60 d\u00edas se eliminar\u00e1n autom\u00e1ticamente, lo que har\u00e1 que el almacenamiento en esta \u00e1rea sea temporal. Se recomienda a los usuarios transferir los archivos importantes de $SCRATCH a su directorio personal. Warning El script de limpieza se ejecuta una vez a la semana, siempre los fines de semana. La cuota /scratch predeterminada disponible para los usuarios con derecho a usar el Cluster es: area bsoft bhard isoft ihard grace period /scratch 100 GB 120 GB 100000 120000 7 days Buenas pr\u00e1cticas \u00b6 Los sistemas de archivos distribuidos como Lustre son ideales para entornos HPC y HTC. En estos entornos, la carga de trabajo t\u00edpica consiste en archivos grandes que necesitan ser accedidos desde muchos nodos de computaci\u00f3n con gran ancho de banda y/o baja latencia. Por lo tanto, estos sistemas de archivos son muy diferentes a los usados en computadoras de escritorio o servidores aislados. Aunque son excelentes para manejar archivos grandes, tambi\u00e9n presentan fuertes limitaciones al tratar con archivos peque\u00f1os y patrones de acceso m\u00e1s comunes en entornos corporativos y de escritorio. Las operaciones que pueden ser extremadamente r\u00e1pidas en un disco local de estaci\u00f3n de trabajo pueden ser dolorosamente lentas y costosas en Lustre, afectando tanto a los usuarios que realizan estas operaciones como, eventualmente, a todos los dem\u00e1s usuarios. Estas mejores pr\u00e1cticas tienen como objetivo permitir un uso tranquilo de Lustre, minimizando operaciones innecesarias o muy costosas. Evite acceder a atributos de archivos y directorios Acceder a metadatos como atributos de archivo (tipo, propiedad, protecci\u00f3n, tama\u00f1o, fechas, etc.) en Lustre consume muchos recursos y puede degradar el rendimiento, especialmente cuando se realiza con frecuencia o en directorios con muchos archivos. Minimice el uso de llamadas al sistema que accedan o modifiquen estos atributos, como stat() , statx() , open() , openat() , etc. Lo mismo aplica para comandos como ls -l o ls --color que usan estas llamadas. En su lugar, use un simple ls o ls -l nombrearchivo . Evite comandos que accedan masivamente a metadatos Evite comandos como ls -R , find , locate , du , df y similares. Estos comandos recorren recursivamente el sistema de archivos y/o realizan operaciones pesadas de metadatos. Son muy intensivos en acceso a metadatos y pueden degradar gravemente el rendimiento. Si es absolutamente necesario recorrer recursivamente el sistema, use el comando lfs find de Lustre en lugar de find . Use el comando lfs de Lustre Para minimizar llamadas RPC a Lustre, siempre que sea posible use los comandos lfs en lugar de los del sistema: lfs df => en lugar de df lfs find => en lugar de find Evite usar comodines La expansi\u00f3n de comodines requiere muchos recursos. Ejecutar comandos con comodines en muchos archivos puede tomar mucho tiempo y afectar gravemente el rendimiento. En lugar de usar comodines, cree una lista de archivos objetivo y aplique el comando a cada uno. Acceso de solo lectura Cuando sea posible, abra archivos como solo lectura con O_RDONLY , y si no necesita actualizar el tiempo de acceso, use O_RDONLY | O_NOATIME . Si se necesita informaci\u00f3n de tiempo de acceso durante E/S paralela, haga que el proceso padre abra los archivos como O_RDONLY y los dem\u00e1s como O_RDONLY|O_NOATIME . Evite muchos archivos en un solo directorio Cuando se accede a un archivo, Lustre bloquea su directorio padre. Muchos archivos en el mismo directorio crean contenci\u00f3n. Escribir miles de archivos en un directorio sobrecarga los servidores de metadatos, a menudo resultando en indisponibilidad del sistema. Acceder a un directorio con miles de archivos puede causar gran contenci\u00f3n. La alternativa es organizar los datos en m\u00faltiples subdirectorios. Un enfoque com\u00fan es usar la ra\u00edz cuadrada del n\u00famero de archivos - para 90,000 archivos crear 300 directorios con 300 archivos cada uno. Evite archivos peque\u00f1os Acceder a archivos peque\u00f1os en Lustre es muy ineficiente. El tama\u00f1o recomendado es mayor a 1 GB. Reorganice datos en archivos grandes o use formatos como HDF5 . Alternativamente, si el tama\u00f1o total es peque\u00f1o (pocos GB), copie los archivos a /tmp o a un directorio temporal local al inicio del trabajo (no olvide transferir/eliminar al final). Esto puede combinarse con herramientas como tar para almacenar archivos peque\u00f1os en tarballs grandes. Al leer/escribir archivos, Lustre funciona mucho mejor con buffers grandes (>= 1 MB). Se recomienda agregar peque\u00f1as operaciones de E/S en operaciones mayores. MPI-IO permite E/S agregada. Evite peque\u00f1as operaciones repetitivas Evite realizar peque\u00f1as operaciones de E/S repetitivas como abrir frecuentemente archivos en modo append, escribir poco y cerrar. En su lugar, abra el archivo una vez, realice todas las operaciones y luego cierre. Evite m\u00faltiples procesos abriendo los mismos archivos simult\u00e1neamente Esto puede crear contenci\u00f3n y errores. En su lugar, realice la apertura desde un proceso padre, o abra como solo lectura para evitar bloqueos, o implemente un enfoque de reintento con espera ante errores. Evite m\u00faltiples procesos accediendo a la misma regi\u00f3n de archivo Si m\u00faltiples procesos acceden a la misma regi\u00f3n simult\u00e1neamente, el administrador de bloqueos de Lustre forzar\u00e1 coherencia, lo que puede degradar el rendimiento. En este caso, puede ser preferible: replicar el archivo, dividirlo, realizar E/S desde un solo proceso, o asegurar que no habr\u00e1 acceso simult\u00e1neo. Se recomienda minimizar operaciones paralelas de apertura/bloqueo. Si m\u00faltiples procesos intentan anexar al mismo archivo, esto causar\u00e1 contenci\u00f3n. Idealmente, solo un proceso debe anexar a cada archivo. Operaciones de archivo a trav\u00e9s del proceso padre Al acceder a archivos peque\u00f1os compartidos en tareas paralelas, suele ser m\u00e1s eficiente que el proceso padre realice todas las operaciones necesarias y, si es necesario, transmita los datos a otros procesos. Similarmente, si m\u00faltiples procesos necesitan informaci\u00f3n sobre un archivo, es mejor que el proceso padre realice las llamadas necesarias (ej. stat() , fstat() ) y luego transmita la informaci\u00f3n. Distribuci\u00f3n de archivos (striping) En Lustre, archivos grandes pueden dividirse en segmentos distribuidos autom\u00e1ticamente entre m\u00faltiples dispositivos de almacenamiento. La distribuci\u00f3n es \u00fatil para E/S paralela en archivos grandes. Para que funcione, el punto de montaje debe apuntar a m\u00faltiples dispositivos (OSTs). Use lfs df para verificar esto. Para obtener informaci\u00f3n de distribuci\u00f3n de un archivo: lfs getstripe nombrearchivo La distribuci\u00f3n puede configurarse con lfs setstripe . Si se aplica a un directorio, establece la configuraci\u00f3n por defecto para archivos creados all\u00ed. Un subdirectorio hereda la configuraci\u00f3n de su padre. Si se aplica a un archivo, distribuye ese archivo seg\u00fan la configuraci\u00f3n. lfs setstripe -s 128m -c 8 nombrearchivo => divide el archivo en segmentos de 128 MB distribuidos en 8 OSTs Si un archivo grande es compartido en paralelo por m\u00faltiples procesos, puede ser \u00fatil dividirlo en un n\u00famero de segmentos igual al n\u00famero de procesos. Para m\u00e1ximo rendimiento, las solicitudes de E/S deben alinearse con los segmentos, accediendo en offsets que coincidan con los l\u00edmites de los segmentos. Para archivos peque\u00f1os, desactive la distribuci\u00f3n estableciendo un conteo de 1. Lo mismo aplica si un archivo grande es accedido por un solo proceso. lfs setstripe -s 1m -c 1 midir/archivospequenos/ Evite instalar software en Lustre El software generalmente consiste en muchos archivos peque\u00f1os, y como se mencion\u00f3, acceder a muchos archivos peque\u00f1os puede sobrecargar los servidores de metadatos. Las compilaciones en particular se realizan mejor localmente copiando/descomprimiendo el software en /tmp/$USER/ o en el homedir . Adem\u00e1s, bajo alta carga, el acceso a Lustre puede bloquearse. Si los ejecutables est\u00e1n en Lustre y el acceso falla, pueden colapsar. Por lo tanto, es mejor copiar los ejecutables al /tmp de los nodos. \u00c1rea de scripts \u00b6 Los usuarios podr\u00e1n acceder a su directorio de scripts a trav\u00e9s de la variable de entorno o accediendo al directorio con la ruta completa. cd $SCRIPTS O cd /scripts/<username> Esta \u00e1rea est\u00e1 dise\u00f1ada para almacenar scripts para enviar trabajos al cl\u00faster y otros archivos. Tambi\u00e9n se recomienda usar esta ruta para crear entornos (envs) y kernels de Python. La cuota predeterminada de /scripts disponible para los usuarios es: area bsoft bhard isoft ihard grace period /scripts 10 GB 12 GB 100k 120k 7 days Nota: El directorio /scripts no se ve afectado por el proceso de limpieza autom\u00e1tica. Homedir \u00b6 El directorio home es un \u00e1rea donde los usuarios almacenan sus archivos personales y es accesible a trav\u00e9s de los nodos de inicio de sesi\u00f3n del cl\u00faster y tambi\u00e9n en la plataforma jupyter . La cuota de directorio personal predeterminada para cada usuario, seg\u00fan su perfil, se muestra a continuaci\u00f3n: profile bsoft bhard isoft ihard grace period p\u00fablico geral 5 GB 7 GB 7000 10000 7 days p\u00fablico institucional 25 GB 30 GB 40000 50000 7 days colabora\u00e7\u00e3o 100 GB 120 GB 1000000 1200000 7 days Tip Para comprobar los valores de cuota configurados, simplemente use el comando: quota -s -u <username> /home . Nota: El directorio /home no se ve afectado por el proceso de limpieza autom\u00e1tica. Comandos \u00fatiles \u00b6 a) \u00bfC\u00f3mo consultar mi cuota disponible? show_quota b) \u00bfC\u00f3mo consultar mis archivos creados hace m\u00e1s de 60 d\u00edas? lfs find $SCRATCH --uid $UID -mtime +60 --print c) \u00bfC\u00f3mo consultar mis archivos creados hace menos de 60 d\u00edas? lfs find $SCRATCH --uid $UID -mtime -60 --print d) \u00bfC\u00f3mo listar los OSTs de Lustre? lfs osts $SCRATCH e) \u00bfC\u00f3mo listar archivos mayores a 60 d\u00edas en un OST espec\u00edfico? lfs find $SCRATCH -mtime +60 --print --obd t0-OST0002_UUID f) \u00bfC\u00f3mo configurar striping en un directorio para \"dividir\" archivos y distribuir \"trozos\" en 10 OSTs? lfs setstripe -c 10 $SCRATCH/mis_archivos_grandes g) \u00bfC\u00f3mo consultar el striping de archivos/directorios? lfs getstripe $SCRATCH/mis_archivos_grandes Tip El Lustre de LIneA est\u00e1 dise\u00f1ado para trabajar a 100Gbps - para m\u00e1ximo rendimiento use striping y siempre con archivos grandes (+1GB). NAS (NFS) \u00b6 Los sistemas NAS se usan para almacenamiento a largo plazo y no son accesibles desde los nodos de c\u00f3mputo (HPC). Caracter\u00edsticas actuales: Fabricante Modelo Capacidad Instalado SGI IS5500 [1] 540TB Dic-2011 SGI IS5600 240TB Jul-2014 [1] este equipo fue desactivado en Jun/2023 por problemas f\u00edsicos. Backup \u00b6 \u00e1reas frecuencia tipo retenci\u00f3n /home diario incremental 30 d\u00edas /home semanal diferencial 30 d\u00edas /home mensal completo 90 d\u00edas /archive - - - /scratch - - - /scripts - - - Referencias \u00b6 Estas mejores pr\u00e1cticas fueron compiladas de la experiencia del equipo de LIneA y las siguientes fuentes: https://www.nas.nasa.gov/hecc/support/kb/lustre-best-practices_226.html https://hpcf.umbc.edu/general-productivity/lustre-best-practices/ https://wiki.gsi.de/foswiki/bin/view/Linux/LustreFs https://doc.lustre.org/lustre_manual.pdf","title":"Almacenamiento"},{"location":"es/armazenamento/index.html#lustrefs-hpc","text":"El entorno del cl\u00faster Apollo cuenta con un sistema de archivos de alto rendimiento Lustre con dos niveles (tiers) de almacenamiento: uno en SSD con ~70 TB (T0) y otro en HDD con ~500 TB (T1), ambos conectados a una red Infiniband EDR de 100 Gb/s. Los dos niveles de almacenamiento est\u00e1n disponibles en /scratch y /data .","title":"LustreFS (HPC)"},{"location":"es/armazenamento/index.html#area-de-scratch-y-cuota","text":"Los usuarios pueden acceder a su directorio scratch mediante una variable de entorno o accediendo al directorio con la ruta completa. cd $SCRATCH O cd /scratch/users/<username> ATENCI\u00d3N \u00a1Esta \u00e1rea NO tendr\u00e1 respaldo! Los archivos que no se hayan modificado en los \u00faltimos 60 d\u00edas se eliminar\u00e1n autom\u00e1ticamente, lo que har\u00e1 que el almacenamiento en esta \u00e1rea sea temporal. Se recomienda a los usuarios transferir los archivos importantes de $SCRATCH a su directorio personal. Warning El script de limpieza se ejecuta una vez a la semana, siempre los fines de semana. La cuota /scratch predeterminada disponible para los usuarios con derecho a usar el Cluster es: area bsoft bhard isoft ihard grace period /scratch 100 GB 120 GB 100000 120000 7 days","title":"\u00c1rea de scratch y cuota"},{"location":"es/armazenamento/index.html#buenas-practicas","text":"Los sistemas de archivos distribuidos como Lustre son ideales para entornos HPC y HTC. En estos entornos, la carga de trabajo t\u00edpica consiste en archivos grandes que necesitan ser accedidos desde muchos nodos de computaci\u00f3n con gran ancho de banda y/o baja latencia. Por lo tanto, estos sistemas de archivos son muy diferentes a los usados en computadoras de escritorio o servidores aislados. Aunque son excelentes para manejar archivos grandes, tambi\u00e9n presentan fuertes limitaciones al tratar con archivos peque\u00f1os y patrones de acceso m\u00e1s comunes en entornos corporativos y de escritorio. Las operaciones que pueden ser extremadamente r\u00e1pidas en un disco local de estaci\u00f3n de trabajo pueden ser dolorosamente lentas y costosas en Lustre, afectando tanto a los usuarios que realizan estas operaciones como, eventualmente, a todos los dem\u00e1s usuarios. Estas mejores pr\u00e1cticas tienen como objetivo permitir un uso tranquilo de Lustre, minimizando operaciones innecesarias o muy costosas. Evite acceder a atributos de archivos y directorios Acceder a metadatos como atributos de archivo (tipo, propiedad, protecci\u00f3n, tama\u00f1o, fechas, etc.) en Lustre consume muchos recursos y puede degradar el rendimiento, especialmente cuando se realiza con frecuencia o en directorios con muchos archivos. Minimice el uso de llamadas al sistema que accedan o modifiquen estos atributos, como stat() , statx() , open() , openat() , etc. Lo mismo aplica para comandos como ls -l o ls --color que usan estas llamadas. En su lugar, use un simple ls o ls -l nombrearchivo . Evite comandos que accedan masivamente a metadatos Evite comandos como ls -R , find , locate , du , df y similares. Estos comandos recorren recursivamente el sistema de archivos y/o realizan operaciones pesadas de metadatos. Son muy intensivos en acceso a metadatos y pueden degradar gravemente el rendimiento. Si es absolutamente necesario recorrer recursivamente el sistema, use el comando lfs find de Lustre en lugar de find . Use el comando lfs de Lustre Para minimizar llamadas RPC a Lustre, siempre que sea posible use los comandos lfs en lugar de los del sistema: lfs df => en lugar de df lfs find => en lugar de find Evite usar comodines La expansi\u00f3n de comodines requiere muchos recursos. Ejecutar comandos con comodines en muchos archivos puede tomar mucho tiempo y afectar gravemente el rendimiento. En lugar de usar comodines, cree una lista de archivos objetivo y aplique el comando a cada uno. Acceso de solo lectura Cuando sea posible, abra archivos como solo lectura con O_RDONLY , y si no necesita actualizar el tiempo de acceso, use O_RDONLY | O_NOATIME . Si se necesita informaci\u00f3n de tiempo de acceso durante E/S paralela, haga que el proceso padre abra los archivos como O_RDONLY y los dem\u00e1s como O_RDONLY|O_NOATIME . Evite muchos archivos en un solo directorio Cuando se accede a un archivo, Lustre bloquea su directorio padre. Muchos archivos en el mismo directorio crean contenci\u00f3n. Escribir miles de archivos en un directorio sobrecarga los servidores de metadatos, a menudo resultando en indisponibilidad del sistema. Acceder a un directorio con miles de archivos puede causar gran contenci\u00f3n. La alternativa es organizar los datos en m\u00faltiples subdirectorios. Un enfoque com\u00fan es usar la ra\u00edz cuadrada del n\u00famero de archivos - para 90,000 archivos crear 300 directorios con 300 archivos cada uno. Evite archivos peque\u00f1os Acceder a archivos peque\u00f1os en Lustre es muy ineficiente. El tama\u00f1o recomendado es mayor a 1 GB. Reorganice datos en archivos grandes o use formatos como HDF5 . Alternativamente, si el tama\u00f1o total es peque\u00f1o (pocos GB), copie los archivos a /tmp o a un directorio temporal local al inicio del trabajo (no olvide transferir/eliminar al final). Esto puede combinarse con herramientas como tar para almacenar archivos peque\u00f1os en tarballs grandes. Al leer/escribir archivos, Lustre funciona mucho mejor con buffers grandes (>= 1 MB). Se recomienda agregar peque\u00f1as operaciones de E/S en operaciones mayores. MPI-IO permite E/S agregada. Evite peque\u00f1as operaciones repetitivas Evite realizar peque\u00f1as operaciones de E/S repetitivas como abrir frecuentemente archivos en modo append, escribir poco y cerrar. En su lugar, abra el archivo una vez, realice todas las operaciones y luego cierre. Evite m\u00faltiples procesos abriendo los mismos archivos simult\u00e1neamente Esto puede crear contenci\u00f3n y errores. En su lugar, realice la apertura desde un proceso padre, o abra como solo lectura para evitar bloqueos, o implemente un enfoque de reintento con espera ante errores. Evite m\u00faltiples procesos accediendo a la misma regi\u00f3n de archivo Si m\u00faltiples procesos acceden a la misma regi\u00f3n simult\u00e1neamente, el administrador de bloqueos de Lustre forzar\u00e1 coherencia, lo que puede degradar el rendimiento. En este caso, puede ser preferible: replicar el archivo, dividirlo, realizar E/S desde un solo proceso, o asegurar que no habr\u00e1 acceso simult\u00e1neo. Se recomienda minimizar operaciones paralelas de apertura/bloqueo. Si m\u00faltiples procesos intentan anexar al mismo archivo, esto causar\u00e1 contenci\u00f3n. Idealmente, solo un proceso debe anexar a cada archivo. Operaciones de archivo a trav\u00e9s del proceso padre Al acceder a archivos peque\u00f1os compartidos en tareas paralelas, suele ser m\u00e1s eficiente que el proceso padre realice todas las operaciones necesarias y, si es necesario, transmita los datos a otros procesos. Similarmente, si m\u00faltiples procesos necesitan informaci\u00f3n sobre un archivo, es mejor que el proceso padre realice las llamadas necesarias (ej. stat() , fstat() ) y luego transmita la informaci\u00f3n. Distribuci\u00f3n de archivos (striping) En Lustre, archivos grandes pueden dividirse en segmentos distribuidos autom\u00e1ticamente entre m\u00faltiples dispositivos de almacenamiento. La distribuci\u00f3n es \u00fatil para E/S paralela en archivos grandes. Para que funcione, el punto de montaje debe apuntar a m\u00faltiples dispositivos (OSTs). Use lfs df para verificar esto. Para obtener informaci\u00f3n de distribuci\u00f3n de un archivo: lfs getstripe nombrearchivo La distribuci\u00f3n puede configurarse con lfs setstripe . Si se aplica a un directorio, establece la configuraci\u00f3n por defecto para archivos creados all\u00ed. Un subdirectorio hereda la configuraci\u00f3n de su padre. Si se aplica a un archivo, distribuye ese archivo seg\u00fan la configuraci\u00f3n. lfs setstripe -s 128m -c 8 nombrearchivo => divide el archivo en segmentos de 128 MB distribuidos en 8 OSTs Si un archivo grande es compartido en paralelo por m\u00faltiples procesos, puede ser \u00fatil dividirlo en un n\u00famero de segmentos igual al n\u00famero de procesos. Para m\u00e1ximo rendimiento, las solicitudes de E/S deben alinearse con los segmentos, accediendo en offsets que coincidan con los l\u00edmites de los segmentos. Para archivos peque\u00f1os, desactive la distribuci\u00f3n estableciendo un conteo de 1. Lo mismo aplica si un archivo grande es accedido por un solo proceso. lfs setstripe -s 1m -c 1 midir/archivospequenos/ Evite instalar software en Lustre El software generalmente consiste en muchos archivos peque\u00f1os, y como se mencion\u00f3, acceder a muchos archivos peque\u00f1os puede sobrecargar los servidores de metadatos. Las compilaciones en particular se realizan mejor localmente copiando/descomprimiendo el software en /tmp/$USER/ o en el homedir . Adem\u00e1s, bajo alta carga, el acceso a Lustre puede bloquearse. Si los ejecutables est\u00e1n en Lustre y el acceso falla, pueden colapsar. Por lo tanto, es mejor copiar los ejecutables al /tmp de los nodos.","title":"Buenas pr\u00e1cticas"},{"location":"es/armazenamento/index.html#area-de-scripts","text":"Los usuarios podr\u00e1n acceder a su directorio de scripts a trav\u00e9s de la variable de entorno o accediendo al directorio con la ruta completa. cd $SCRIPTS O cd /scripts/<username> Esta \u00e1rea est\u00e1 dise\u00f1ada para almacenar scripts para enviar trabajos al cl\u00faster y otros archivos. Tambi\u00e9n se recomienda usar esta ruta para crear entornos (envs) y kernels de Python. La cuota predeterminada de /scripts disponible para los usuarios es: area bsoft bhard isoft ihard grace period /scripts 10 GB 12 GB 100k 120k 7 days Nota: El directorio /scripts no se ve afectado por el proceso de limpieza autom\u00e1tica.","title":"\u00c1rea de scripts"},{"location":"es/armazenamento/index.html#homedir","text":"El directorio home es un \u00e1rea donde los usuarios almacenan sus archivos personales y es accesible a trav\u00e9s de los nodos de inicio de sesi\u00f3n del cl\u00faster y tambi\u00e9n en la plataforma jupyter . La cuota de directorio personal predeterminada para cada usuario, seg\u00fan su perfil, se muestra a continuaci\u00f3n: profile bsoft bhard isoft ihard grace period p\u00fablico geral 5 GB 7 GB 7000 10000 7 days p\u00fablico institucional 25 GB 30 GB 40000 50000 7 days colabora\u00e7\u00e3o 100 GB 120 GB 1000000 1200000 7 days Tip Para comprobar los valores de cuota configurados, simplemente use el comando: quota -s -u <username> /home . Nota: El directorio /home no se ve afectado por el proceso de limpieza autom\u00e1tica.","title":"Homedir"},{"location":"es/armazenamento/index.html#comandos-utiles","text":"a) \u00bfC\u00f3mo consultar mi cuota disponible? show_quota b) \u00bfC\u00f3mo consultar mis archivos creados hace m\u00e1s de 60 d\u00edas? lfs find $SCRATCH --uid $UID -mtime +60 --print c) \u00bfC\u00f3mo consultar mis archivos creados hace menos de 60 d\u00edas? lfs find $SCRATCH --uid $UID -mtime -60 --print d) \u00bfC\u00f3mo listar los OSTs de Lustre? lfs osts $SCRATCH e) \u00bfC\u00f3mo listar archivos mayores a 60 d\u00edas en un OST espec\u00edfico? lfs find $SCRATCH -mtime +60 --print --obd t0-OST0002_UUID f) \u00bfC\u00f3mo configurar striping en un directorio para \"dividir\" archivos y distribuir \"trozos\" en 10 OSTs? lfs setstripe -c 10 $SCRATCH/mis_archivos_grandes g) \u00bfC\u00f3mo consultar el striping de archivos/directorios? lfs getstripe $SCRATCH/mis_archivos_grandes Tip El Lustre de LIneA est\u00e1 dise\u00f1ado para trabajar a 100Gbps - para m\u00e1ximo rendimiento use striping y siempre con archivos grandes (+1GB).","title":"Comandos \u00fatiles"},{"location":"es/armazenamento/index.html#nas-nfs","text":"Los sistemas NAS se usan para almacenamiento a largo plazo y no son accesibles desde los nodos de c\u00f3mputo (HPC). Caracter\u00edsticas actuales: Fabricante Modelo Capacidad Instalado SGI IS5500 [1] 540TB Dic-2011 SGI IS5600 240TB Jul-2014 [1] este equipo fue desactivado en Jun/2023 por problemas f\u00edsicos.","title":"NAS (NFS)"},{"location":"es/armazenamento/index.html#backup","text":"\u00e1reas frecuencia tipo retenci\u00f3n /home diario incremental 30 d\u00edas /home semanal diferencial 30 d\u00edas /home mensal completo 90 d\u00edas /archive - - - /scratch - - - /scripts - - -","title":"Backup"},{"location":"es/armazenamento/index.html#referencias","text":"Estas mejores pr\u00e1cticas fueron compiladas de la experiencia del equipo de LIneA y las siguientes fuentes: https://www.nas.nasa.gov/hecc/support/kb/lustre-best-practices_226.html https://hpcf.umbc.edu/general-productivity/lustre-best-practices/ https://wiki.gsi.de/foswiki/bin/view/Linux/LustreFs https://doc.lustre.org/lustre_manual.pdf","title":"Referencias"},{"location":"es/data/index.html","text":"Repositorio de Datos \u00b6 Cat\u00e1logos \u00b6 La informaci\u00f3n relevante sobre cat\u00e1logos astron\u00f3micos p\u00fablicos alojados en LIneA est\u00e1 disponible en las p\u00e1ginas de documentaci\u00f3n del servicio User Query . Para acceder, haga clic en el men\u00fa \"Database Tables\" y seleccione el conjunto de datos deseado. Im\u00e1genes \u00b6 Las im\u00e1genes p\u00fablicas alojadas en LIneA y disponibles en los servicios de visualizaci\u00f3n pertenecen al release DES DR2. Para m\u00e1s informaci\u00f3n, consulte la publicaci\u00f3n The Dark Energy Survey Data Release 2 . Productos de Datos de Photo-z \u00b6 Productos de datos p\u00fablicos de Photo-z \u00b6 (pr\u00f3ximamente) Productos de datos privados de LSST Photo-z \u00b6 Documentaci\u00f3n de los productos de datos del servidor LSST Photo-z","title":"Datos"},{"location":"es/data/index.html#repositorio-de-datos","text":"","title":"Repositorio de Datos"},{"location":"es/data/index.html#catalogos","text":"La informaci\u00f3n relevante sobre cat\u00e1logos astron\u00f3micos p\u00fablicos alojados en LIneA est\u00e1 disponible en las p\u00e1ginas de documentaci\u00f3n del servicio User Query . Para acceder, haga clic en el men\u00fa \"Database Tables\" y seleccione el conjunto de datos deseado.","title":"Cat\u00e1logos"},{"location":"es/data/index.html#imagenes","text":"Las im\u00e1genes p\u00fablicas alojadas en LIneA y disponibles en los servicios de visualizaci\u00f3n pertenecen al release DES DR2. Para m\u00e1s informaci\u00f3n, consulte la publicaci\u00f3n The Dark Energy Survey Data Release 2 .","title":"Im\u00e1genes"},{"location":"es/data/index.html#productos-de-datos-de-photo-z","text":"","title":"Productos de Datos de Photo-z"},{"location":"es/data/index.html#productos-de-datos-publicos-de-photo-z","text":"(pr\u00f3ximamente)","title":"Productos de datos p\u00fablicos de Photo-z"},{"location":"es/data/index.html#productos-de-datos-privados-de-lsst-photo-z","text":"Documentaci\u00f3n de los productos de datos del servidor LSST Photo-z","title":"Productos de datos privados de LSST Photo-z"},{"location":"es/data/pz_server_data.html","text":"Photo-z Server Data \u00b6 Datasets Pr\u00f3ximamente habr\u00e1 traducci\u00f3n al espa\u00f1ol. Consulta la p\u00e1gina original en ingl\u00e9s aqu\u00ed .","title":"Photo-z Server Data"},{"location":"es/data/pz_server_data.html#photo-z-server-data","text":"Datasets Pr\u00f3ximamente habr\u00e1 traducci\u00f3n al espa\u00f1ol. Consulta la p\u00e1gina original en ingl\u00e9s aqu\u00ed .","title":"Photo-z Server Data"},{"location":"es/processamento/index.html","text":"Computaci\u00f3n de Alto Rendimiento \u00b6 LIneA ofrece acceso a recursos computacionales de alto rendimiento para miembros de colaboraciones cient\u00edficas y proyectos apoyados por el laboratorio en los siguientes entornos: El registro en el entorno de LIneA NO otorga acceso autom\u00e1tico a los recursos de HPC. Una vez aprobado su registro, debe abrir un ticket y solicitar acceso, enviando una justificaci\u00f3n de uso que ser\u00e1 enviada al Comit\u00e9 de Gesti\u00f3n para evaluaci\u00f3n. El incumplimiento de nuestras pol\u00edticas de seguridad y uso del entorno puede resultar en el bloqueo de su cuenta sin previo aviso. Las pol\u00edticas se pueden encontrar aqu\u00ed . Cluster HPE Apollo 2000 (LIneA) Open OnDemand JupyterLab sobre HPC","title":"Procesamiento (HPC)"},{"location":"es/processamento/index.html#computacion-de-alto-rendimiento","text":"LIneA ofrece acceso a recursos computacionales de alto rendimiento para miembros de colaboraciones cient\u00edficas y proyectos apoyados por el laboratorio en los siguientes entornos: El registro en el entorno de LIneA NO otorga acceso autom\u00e1tico a los recursos de HPC. Una vez aprobado su registro, debe abrir un ticket y solicitar acceso, enviando una justificaci\u00f3n de uso que ser\u00e1 enviada al Comit\u00e9 de Gesti\u00f3n para evaluaci\u00f3n. El incumplimiento de nuestras pol\u00edticas de seguridad y uso del entorno puede resultar en el bloqueo de su cuenta sin previo aviso. Las pol\u00edticas se pueden encontrar aqu\u00ed . Cluster HPE Apollo 2000 (LIneA) Open OnDemand JupyterLab sobre HPC","title":"Computaci\u00f3n de Alto Rendimiento"},{"location":"es/processamento/sdu.html","text":"LIneA tiene un proyecto \"paraguas\" aprobado en LNCC llamado \" Explorando el Universo mediante big data: desde el sistema solar hasta la energ\u00eda oscura \" (sigla EUBD) que garantiza el derecho a utilizar 2 millones de horas CPU del supercomputador Santos Dumont para apoyar algunos subproyectos que dependen de High-Performance Computing (HPC) en las siguientes \u00e1reas: Sistema Solar V\u00eda L\u00e1ctea/Volumen Local Energ\u00eda Oscura Estructuras a Gran Escala Redshifts Fotom\u00e9tricos (programa de contribuciones in-kind LSST) Info Si tu proyecto tiene demandas de HPC y a\u00fan no forma parte del alcance del proyecto EUBD, cont\u00e1ctanos a trav\u00e9s del correo helpdesk@linea.org.br para recibir orientaci\u00f3n. Registro de usuarios para acceso al Supercomputador - Proyecto EUBD \u00b6 Env\u00eda un correo electr\u00f3nico, siguiendo el modelo abajo indicado a helpdesk@linea.org.br y espera nuestra respuesta (m\u00e1ximo en 72 horas): a.) En el campo asunto escribe: Acceso al supercomputador Santos Dumont - Proyecto EUBD b.) En el cuerpo del correo: 1. Tu nombre completo: 2. Nombre de la instituci\u00f3n (acr\u00f3nimo): 3. Nombre del asesor (si aplica): 4. Correo del asesor/supervisor (si aplica): 5. Justificaci\u00f3n de uso: - Escribe en m\u00e1ximo 5 l\u00edneas cu\u00e1l es el objetivo y la importancia de usar estos recursos computacionales. - Si es posible, informa cu\u00e1ntas horas/CPU pretendes utilizar y tambi\u00e9n el volumen de datos de entrada y salida de tu aplicaci\u00f3n. - Utiliza KiloBytes(KB), MegaBytes(MB) o GigaBytes(GB) para representar el volumen de tus datos. Acceso al supercomputador \u00b6 Despu\u00e9s de obtener tus credenciales, el acceso y las instrucciones de uso son proporcionadas directamente por LNCC. La sumisi\u00f3n de jobs se realizar\u00e1 a trav\u00e9s del gestor de recursos y colas Slurm. El manual de usuario est\u00e1 disponible en la p\u00e1gina de Santos Dumont en el sitio de LNCC .","title":"Sdu"},{"location":"es/processamento/sdu.html#registro-de-usuarios-para-acceso-al-supercomputador-proyecto-eubd","text":"Env\u00eda un correo electr\u00f3nico, siguiendo el modelo abajo indicado a helpdesk@linea.org.br y espera nuestra respuesta (m\u00e1ximo en 72 horas): a.) En el campo asunto escribe: Acceso al supercomputador Santos Dumont - Proyecto EUBD b.) En el cuerpo del correo: 1. Tu nombre completo: 2. Nombre de la instituci\u00f3n (acr\u00f3nimo): 3. Nombre del asesor (si aplica): 4. Correo del asesor/supervisor (si aplica): 5. Justificaci\u00f3n de uso: - Escribe en m\u00e1ximo 5 l\u00edneas cu\u00e1l es el objetivo y la importancia de usar estos recursos computacionales. - Si es posible, informa cu\u00e1ntas horas/CPU pretendes utilizar y tambi\u00e9n el volumen de datos de entrada y salida de tu aplicaci\u00f3n. - Utiliza KiloBytes(KB), MegaBytes(MB) o GigaBytes(GB) para representar el volumen de tus datos.","title":"Registro de usuarios para acceso al Supercomputador - Proyecto EUBD"},{"location":"es/processamento/sdu.html#acceso-al-supercomputador","text":"Despu\u00e9s de obtener tus credenciales, el acceso y las instrucciones de uso son proporcionadas directamente por LNCC. La sumisi\u00f3n de jobs se realizar\u00e1 a trav\u00e9s del gestor de recursos y colas Slurm. El manual de usuario est\u00e1 disponible en la p\u00e1gina de Santos Dumont en el sitio de LNCC .","title":"Acceso al supercomputador"},{"location":"es/processamento/apollo/index.html","text":"HPE Apollo 2000 \u00b6 El Cluster Apollo posee 28 nodos computacionales y ofrece un total de 1072 n\u00facleos f\u00edsicos. Sus nodos est\u00e1n equipados con procesadores Intel Xeon Skylake 5120 2.2GHz (apl01-16) y Intel Xeon Gold 5320 2.20GHz (apl17-28). El conjunto de m\u00e1quinas provee cerca de 85 Tflops de capacidad computacional. Los 28 nodos computacionales del Cluster Apollo pertenecen a la familia de servidores HPE ProLiant, siendo 16 del modelo XL170r y 12 del modelo XL220n. Actualmente, el n\u00famero de n\u00facleos disponibles es de 2144 , ya que el HT est\u00e1 activo en los nodos de computaci\u00f3n. Caracter\u00edsticas de cada servidor \u00b6 Modelo # N\u00facleos (HT) [1] RAM SO Hosts HPE Proliant XL170r 56 128 GB Rocky Linux 9.5 apl[01-16] HPE Proliant XL220n 104 256 GB Rocky Linux 9.5 apl[17-28] [1] La tecnolog\u00eda Hyper-Threading (HT) est\u00e1 habilitada en todos los nodos del cluster. Caracter\u00edsticas del cluster (consolidado) # Nodos # N\u00facleos RAM total Instalado en 16 896 1.5TB Abr-2019 12 624 3TB Jul-2023 El Cluster Apollo es gestionado por Slurm v24.05.5 . Sistema de Archivos \u00b6 El Cluster Apollo cuenta con un sistema de archivos de alto rendimiento Lustre, disponible como \u00e1rea de \"Scratch\". El \"Home\" de los usuarios est\u00e1 accesible \u00fanicamente en el nodo de login y se provee mediante NFS. Estas \u00e1reas de almacenamiento deben utilizarse de la siguiente forma: Scratch: Estructura montada desde /scratch/<usuario> . Utilizada para almacenar todos los archivos que ser\u00e1n usados durante la ejecuci\u00f3n de un job (scripts de env\u00edo, ejecutables, datos de entrada, datos de salida, etc). Variable de entorno $SCRATCH . Home: Estructura montada desde /home/<usuario> . Utilizada especialmente para almacenar resultados que se desee mantener durante toda la vigencia del proyecto. Variable de entorno $HOME . Scriptland: Estructura montada desde /scriptland/<usuario> . Es un \u00e1rea de almacenamiento optimizada para scripts y c\u00f3digos. Variable de entorno $SCRIPTLAND . Haga clic aqu\u00ed para m\u00e1s detalles Atenci\u00f3n No olvide copiar los archivos necesarios (ejecutable, bibliotecas, datos de entrada) al \u00e1rea de SCRATCH, pues el \u00e1rea de HOMEDIR no es accesible por los nodos computacionales. Slurm \u00b6 Slurm es un sistema de gesti\u00f3n de cluster y planificaci\u00f3n de trabajos de c\u00f3digo abierto, tolerante a fallos y altamente escalable para clusters Linux grandes y peque\u00f1os. Slurm no requiere modificaciones en el kernel para su operaci\u00f3n y es relativamente independiente. Como gestor de carga de trabajo de cluster, Slurm tiene tres funciones principales: Asignar acceso exclusivo y/o no exclusivo a recursos (nodos de computaci\u00f3n) a usuarios por un per\u00edodo determinado Ofrecer un marco para iniciar, ejecutar y monitorear trabajos (normalmente trabajos paralelos) en el conjunto de nodos asignados Gestionar la cola de env\u00edo, arbitrando conflictos entre solicitudes de recursos computacionales Particiones disponibles \u00b6 El cluster Apollo est\u00e1 organizado en diferentes particiones (subconjuntos de m\u00e1quinas) para atender diversas necesidades, por ejemplo, garantizar la prioridad m\u00e1xima de los usuarios del proyecto LSST en el uso de las m\u00e1quinas dedicadas a IDAC-Brasil. PARTICI\u00d3N L\u00cdMITE TIEMPO NODES LISTA NODOS cpu_dev 30:00 26 apl[01-28] cpu_small 3-00:00:00 26 apl[01-28] cpu 5-00:00:00 26 apl[01-28] cpu_long 31-00:00:0 26 apl[01-28] lsst_cpu_dev 30:00 12 apl[17-28] lsst_cpu_small 3-00:00:00 12 apl[17-28] lsst_cpu 5-00:00:00 12 apl[17-28] lsst_cpu_long 10-00:00:0 12 apl[17-28] Cuentas disponibles \u00b6 Workflow \u2013 Interrumpe cualquier job en ejecuci\u00f3n: hpc-photoz (photoz) LSST \u2013 Pr\u00f3ximo en la cola: hpc-lsst [solo en nuevas apollos apl[17-28]] (lsst) Grupo A - Prioridad Mayor: hpc-bpglsst (itteam, bpg-lsst) Grupo B - Prioridad Intermedia: hpc-collab (des, desi, sdss, tno) Grupo C - Prioridad Menor: hpc-public (linea-members) Las particiones ( cpu_dev , cpu_small , cpu y cpu_long ) incluyen todas las apollos ( apl[01-28] ), mientras que las particiones del grupo LSST solo incluyen apl[17-28] . Solo la cuenta hpc-lsst puede enviar jobs a particiones con prefijo \"lsst\", que tienen mayor prioridad en los nodos. Atenci\u00f3n Como parte del programa de contribuci\u00f3n in-kind BRA-LIN, IDAC Brasil tiene el compromiso de generar redshifts fotom\u00e9tricos anualmente para el relevamiento LSST, siempre en el per\u00edodo previo a las liberaciones oficiales de datos. En estos per\u00edodos, el Cluster Apollo estar\u00e1 completamente ocupado para este prop\u00f3sito por un tiempo estimado de varias horas, pudiendo extenderse a varios d\u00edas. Los usuarios ser\u00e1n informados con anticipaci\u00f3n por correo sobre la indisponibilidad del cluster. Haga clic aqu\u00ed para conocer m\u00e1s sobre la producci\u00f3n de redshifts fotom\u00e9tricos y el programa de contribuci\u00f3n in-kind BRA-LIN. Anatom\u00eda de un Job \u00b6 Un Job solicita recursos de computaci\u00f3n y especifica las aplicaciones a iniciar en esos recursos, junto con cualquier dato/opci\u00f3n de entrada y directiva de salida. El usuario env\u00eda el trabajo, generalmente como un script de trabajo por lotes, al planificador por lotes. El script de trabajo por lotes consta de cuatro componentes principales: El int\u00e9rprete usado para ejecutar el script Directivas \"#\" que transmiten opciones de env\u00edo predeterminadas Configuraci\u00f3n de variables de entorno y/o script (si es necesario) Las aplicaciones a ejecutar junto con sus argumentos y opciones de entrada Aqu\u00ed un ejemplo de script por lotes que solicita 3 nodos en la partici\u00f3n \"cpu\" e inicia 36 tareas de myApp en los 3 nodos asignados: #!/bin/bash #SBATCH -N 3 #SBATCH -p cpu #SBATCH --ntasks 36 srun myApp Cuando el trabajo est\u00e9 programado para ejecuci\u00f3n, el gestor de recursos ejecutar\u00e1 el script de trabajo por lotes en el primer nodo de la asignaci\u00f3n. Especificando Recursos \u00b6 Slurm tiene su propia sintaxis para solicitar recursos computacionales. A continuaci\u00f3n, una tabla resumen de algunos recursos solicitados frecuentemente y la sintaxis de Slurm para obtenerlos. Para la lista completa de sintaxis, ejecute el comando man sbatch. Sintaxis Significado #SBATCH -p partici\u00f3n Define la partici\u00f3n donde se ejecutar\u00e1 el job #SBATCH -J nombre_job Define el nombre del Job #SBATCH -n cantidad Define el n\u00famero total de tareas de CPU #SBATCH -N cantidad Define el n\u00famero de nodos de computaci\u00f3n solicitados Comandos B\u00e1sicos de Slurm \u00b6 Para aprender sobre todas las opciones disponibles para cada comando, ingrese man mientras est\u00e9 conectado al entorno del Cluster. Comando Definici\u00f3n sbatch Env\u00eda scripts de trabajos a la cola de ejecuci\u00f3n scancel Cancela un job scontrol Usado para mostrar el estado de Slurm (varias opciones disponibles solo para root) sinfo Muestra estado de particiones y nodos squeue Muestra estado de los jobs salloc Env\u00eda un job para ejecuci\u00f3n o inicia un trabajo en tiempo real Entorno de Ejecuci\u00f3n \u00b6 Para cada tipo de trabajo, el usuario puede definir el entorno de ejecuci\u00f3n. Esto incluye configuraciones de variables de entorno as\u00ed como l\u00edmites de shell ( bash ulimit o csh limit ). sbatch y salloc proveen la opci\u00f3n --export para pasar variables de entorno espec\u00edficas al entorno de ejecuci\u00f3n. Tambi\u00e9n tienen la opci\u00f3n --propagate para pasar l\u00edmites espec\u00edficos de shell al entorno de ejecuci\u00f3n. Variables de Entorno \u00b6 La primera categor\u00eda son variables que Slurm inserta en el entorno de ejecuci\u00f3n del trabajo. Transmiten al script del trabajo informaci\u00f3n como ID del job (SLURM_JOB_ID) e ID de la tarea (SLURM_PROCID) . Para la lista completa, consulte la secci\u00f3n \"OUTPUT ENVIRONMENT VARIABLES\" en las p\u00e1ginas sbatch , salloc y srun . La siguiente categor\u00eda son variables que el usuario puede definir en su entorno para pasar opciones predeterminadas a cada job enviado. Esto incluye opciones como el l\u00edmite de tiempo. Para la lista completa, consulte la secci\u00f3n \"INPUT ENVIRONMENT VARIABLES\" en las p\u00e1ginas sbatch , salloc y srun . \u00bfM\u00e1s Informaci\u00f3n? Aprenda a utilizar el Cluster Apollo en C\u00f3mo Utilizar . Para mayor informaci\u00f3n, contacte al Service Desk .","title":"Cluster Apollo"},{"location":"es/processamento/apollo/index.html#hpe-apollo-2000","text":"El Cluster Apollo posee 28 nodos computacionales y ofrece un total de 1072 n\u00facleos f\u00edsicos. Sus nodos est\u00e1n equipados con procesadores Intel Xeon Skylake 5120 2.2GHz (apl01-16) y Intel Xeon Gold 5320 2.20GHz (apl17-28). El conjunto de m\u00e1quinas provee cerca de 85 Tflops de capacidad computacional. Los 28 nodos computacionales del Cluster Apollo pertenecen a la familia de servidores HPE ProLiant, siendo 16 del modelo XL170r y 12 del modelo XL220n. Actualmente, el n\u00famero de n\u00facleos disponibles es de 2144 , ya que el HT est\u00e1 activo en los nodos de computaci\u00f3n.","title":"HPE Apollo 2000"},{"location":"es/processamento/apollo/index.html#sistema-de-archivos","text":"El Cluster Apollo cuenta con un sistema de archivos de alto rendimiento Lustre, disponible como \u00e1rea de \"Scratch\". El \"Home\" de los usuarios est\u00e1 accesible \u00fanicamente en el nodo de login y se provee mediante NFS. Estas \u00e1reas de almacenamiento deben utilizarse de la siguiente forma: Scratch: Estructura montada desde /scratch/<usuario> . Utilizada para almacenar todos los archivos que ser\u00e1n usados durante la ejecuci\u00f3n de un job (scripts de env\u00edo, ejecutables, datos de entrada, datos de salida, etc). Variable de entorno $SCRATCH . Home: Estructura montada desde /home/<usuario> . Utilizada especialmente para almacenar resultados que se desee mantener durante toda la vigencia del proyecto. Variable de entorno $HOME . Scriptland: Estructura montada desde /scriptland/<usuario> . Es un \u00e1rea de almacenamiento optimizada para scripts y c\u00f3digos. Variable de entorno $SCRIPTLAND . Haga clic aqu\u00ed para m\u00e1s detalles Atenci\u00f3n No olvide copiar los archivos necesarios (ejecutable, bibliotecas, datos de entrada) al \u00e1rea de SCRATCH, pues el \u00e1rea de HOMEDIR no es accesible por los nodos computacionales.","title":"Sistema de Archivos"},{"location":"es/processamento/apollo/index.html#slurm","text":"Slurm es un sistema de gesti\u00f3n de cluster y planificaci\u00f3n de trabajos de c\u00f3digo abierto, tolerante a fallos y altamente escalable para clusters Linux grandes y peque\u00f1os. Slurm no requiere modificaciones en el kernel para su operaci\u00f3n y es relativamente independiente. Como gestor de carga de trabajo de cluster, Slurm tiene tres funciones principales: Asignar acceso exclusivo y/o no exclusivo a recursos (nodos de computaci\u00f3n) a usuarios por un per\u00edodo determinado Ofrecer un marco para iniciar, ejecutar y monitorear trabajos (normalmente trabajos paralelos) en el conjunto de nodos asignados Gestionar la cola de env\u00edo, arbitrando conflictos entre solicitudes de recursos computacionales","title":"Slurm"},{"location":"es/processamento/apollo/index.html#particiones-disponibles","text":"El cluster Apollo est\u00e1 organizado en diferentes particiones (subconjuntos de m\u00e1quinas) para atender diversas necesidades, por ejemplo, garantizar la prioridad m\u00e1xima de los usuarios del proyecto LSST en el uso de las m\u00e1quinas dedicadas a IDAC-Brasil. PARTICI\u00d3N L\u00cdMITE TIEMPO NODES LISTA NODOS cpu_dev 30:00 26 apl[01-28] cpu_small 3-00:00:00 26 apl[01-28] cpu 5-00:00:00 26 apl[01-28] cpu_long 31-00:00:0 26 apl[01-28] lsst_cpu_dev 30:00 12 apl[17-28] lsst_cpu_small 3-00:00:00 12 apl[17-28] lsst_cpu 5-00:00:00 12 apl[17-28] lsst_cpu_long 10-00:00:0 12 apl[17-28]","title":"Particiones disponibles"},{"location":"es/processamento/apollo/index.html#cuentas-disponibles","text":"Workflow \u2013 Interrumpe cualquier job en ejecuci\u00f3n: hpc-photoz (photoz) LSST \u2013 Pr\u00f3ximo en la cola: hpc-lsst [solo en nuevas apollos apl[17-28]] (lsst) Grupo A - Prioridad Mayor: hpc-bpglsst (itteam, bpg-lsst) Grupo B - Prioridad Intermedia: hpc-collab (des, desi, sdss, tno) Grupo C - Prioridad Menor: hpc-public (linea-members) Las particiones ( cpu_dev , cpu_small , cpu y cpu_long ) incluyen todas las apollos ( apl[01-28] ), mientras que las particiones del grupo LSST solo incluyen apl[17-28] . Solo la cuenta hpc-lsst puede enviar jobs a particiones con prefijo \"lsst\", que tienen mayor prioridad en los nodos. Atenci\u00f3n Como parte del programa de contribuci\u00f3n in-kind BRA-LIN, IDAC Brasil tiene el compromiso de generar redshifts fotom\u00e9tricos anualmente para el relevamiento LSST, siempre en el per\u00edodo previo a las liberaciones oficiales de datos. En estos per\u00edodos, el Cluster Apollo estar\u00e1 completamente ocupado para este prop\u00f3sito por un tiempo estimado de varias horas, pudiendo extenderse a varios d\u00edas. Los usuarios ser\u00e1n informados con anticipaci\u00f3n por correo sobre la indisponibilidad del cluster. Haga clic aqu\u00ed para conocer m\u00e1s sobre la producci\u00f3n de redshifts fotom\u00e9tricos y el programa de contribuci\u00f3n in-kind BRA-LIN.","title":"Cuentas disponibles"},{"location":"es/processamento/apollo/index.html#anatomia-de-un-job","text":"Un Job solicita recursos de computaci\u00f3n y especifica las aplicaciones a iniciar en esos recursos, junto con cualquier dato/opci\u00f3n de entrada y directiva de salida. El usuario env\u00eda el trabajo, generalmente como un script de trabajo por lotes, al planificador por lotes. El script de trabajo por lotes consta de cuatro componentes principales: El int\u00e9rprete usado para ejecutar el script Directivas \"#\" que transmiten opciones de env\u00edo predeterminadas Configuraci\u00f3n de variables de entorno y/o script (si es necesario) Las aplicaciones a ejecutar junto con sus argumentos y opciones de entrada Aqu\u00ed un ejemplo de script por lotes que solicita 3 nodos en la partici\u00f3n \"cpu\" e inicia 36 tareas de myApp en los 3 nodos asignados: #!/bin/bash #SBATCH -N 3 #SBATCH -p cpu #SBATCH --ntasks 36 srun myApp Cuando el trabajo est\u00e9 programado para ejecuci\u00f3n, el gestor de recursos ejecutar\u00e1 el script de trabajo por lotes en el primer nodo de la asignaci\u00f3n.","title":"Anatom\u00eda de un Job"},{"location":"es/processamento/uso/howtouse-HPC.html","text":"C\u00f3mo Utilizar \u00b6 C\u00f3mo Acceder \u00b6 El acceso a nuestro cl\u00faster puede realizarse a trav\u00e9s de Open OnDemand o mediante el Terminal de JupyterLab (K8S) . En ambas opciones, es imprescindible poseer una cuenta v\u00e1lida en el entorno computacional de LIneA. Si no tiene una cuenta, contacte al Service Desk por correo ( helpdesk@linea.org.br ) para m\u00e1s informaci\u00f3n. Atenci\u00f3n A\u00fan teniendo una cuenta activa en LIneA, el acceso al entorno de procesamiento HPC no es autom\u00e1tico. Para m\u00e1s informaci\u00f3n contacte al Service Desk en helpdesk@linea.org.br . Accediendo por terminal de JupyterLab En la pantalla inicial de su Jupyter Notebook, en la secci\u00f3n \"Other\" , encontrar\u00e1 el bot\u00f3n del terminal. Al hacer clic en \u00e9l, ser\u00e1 redirigido a un terminal Linux, inicialmente ubicado en su directorio home . Para acceder al Cluster Apollo, simplemente ejecute el siguiente comando: ssh loginapl01 La m\u00e1quina loginapl01 es donde podr\u00e1 asignar nodos de computaci\u00f3n para enviar su job. $HOME y $SCRATCH Los nodos de computaci\u00f3n no tienen acceso a su directorio home de usuario. Mueva o copie todos los archivos necesarios para el env\u00edo de su job a su directorio SCRATCH. C\u00f3mo usar el \u00e1rea SCRATCH \u00b6 Su directorio de SCRATCH es el lugar donde puede dirigir los archivos de resultados de su trabajo, as\u00ed como almacenar datos temporalmente que el c\u00f3digo utiliza en el momento del procesamiento. Para acceder a su directorio SCRATCH: cd $SCRATCH Para enviar archivos a su directorio SCRATCH: cp <ARCHIVO> $SCRATCH Limpieza autom\u00e1tica del Scratch \u00b6 Scratch es un \u00e1rea de almacenamiento temporal para los archivos de salida y procesamiento de trabajos realizados en el cl\u00faster. Para mantener el medio ambiente organizado y garantizar el espacio disponible para todos, est\u00e1 vigente un script de limpieza autom\u00e1tico , que funciona una vez por semana . Este proceso elimina los archivos a los que no se ha accedido dentro del per\u00edodo establecido de retenci\u00f3n , actualmente 45 d\u00edas . Los archivos de configuraci\u00f3n esenciales (por ejemplo, .bashrc , .bash_profile , .ssh , etc.) se conservan autom\u00e1ticamente y no ingresan el proceso de exclusi\u00f3n . Atenci\u00f3n El scratch no debe usarse para el almacenamiento permanente. Recomendamos mover datos importantes a su directorio home . C\u00f3mo usar el \u00e1rea de script \u00b6 Su directorio de SCRIPTS es el lugar donde puede almacenar scripts, c\u00f3digos para ejecutarse en el cl\u00faster. Tambi\u00e9n se recomienda utilizar esta \u00e1rea para crear un entorno de conda. Para acceder a su directorio de SCRIPTS: cd $SCRIPTS Ser Consciente El \u00e1rea de script no est\u00e1 incluida en la rutina de backups. Por lo tanto, no debe usarse como almacenamiento de datos permanente. C\u00f3mo Enviar un Job \u00b6 Un Job solicita recursos de computaci\u00f3n y especifica las aplicaciones a iniciar en esos recursos, junto con cualquier dato/opci\u00f3n de entrada y directiva de salida. La gesti\u00f3n y programaci\u00f3n de tareas y recursos del cl\u00faster se realiza a trav\u00e9s de Slurm. Por lo tanto, para enviar un Job necesita usar un script como el siguiente: #!/bin/bash #SBATCH -p PARTICI\u00d3N #Nombre de la Partici\u00f3n a usar #SBATCH --nodelist=NODO #Nombre del Nodo a asignar #SBATCH -J simple-job #Nombre del Job #----------------------------------------------------------------------------# ##ruta al c\u00f3digo ejecutable EXEC = /scripts/USUARIO/CODIGO.EJECUTABLE srun $EXEC En este script debe especificar: el nombre de la cola (Partition) a usar; el nombre del nodo a asignar para la ejecuci\u00f3n del Job; y la ruta al c\u00f3digo/programa a ejecutar. ADVERTENCIA Est\u00e1 estrictamente prohibido enviar jobs directamente a la m\u00e1quina loginapl01 . Cualquier c\u00f3digo ejecut\u00e1ndose en esta m\u00e1quina ser\u00e1 interrumpido inmediatamente sin previo aviso. Para enviar el Job: sbatch script-submit-job.sh Si el script es correcto habr\u00e1 una salida que indica el ID del job . Para verificar el progreso e informaci\u00f3n del Job: scontrol show job <ID> Para cancelar el Job: scancel <ID> Acceso a internet Los nodos de computaci\u00f3n no tienen acceso a internet. Paquetes y bibliotecas deben instalarse desde loginapl01 en su \u00e1rea scratch. Gestor de paquetes EUPS \u00b6 EUPS es un gestor de paquetes alternativo (y oficial de LSST) que permite cargar variables de entorno e incluir la ruta a programas y bibliotecas de forma modular. Para cargar EUPS: Info Actualmente, EUPS se carga autom\u00e1ticamente despu\u00e9s de que el usuario accede a cualquier m\u00e1quina del cl\u00faster Apollo. source /opt/eups/bin/setups.sh Para listar todos los paquetes disponibles: eups list Para listar un paquete espec\u00edfico: eups list | grep <PAQUETE> Para cargar un paquete en la sesi\u00f3n actual: setup <NOMBRE DEL PAQUETE> <VERSI\u00d3N DEL PAQUETE> Para eliminar el paquete cargado: unsetup <NOMBRE DEL PAQUETE> <VERSI\u00d3N DEL PAQUETE> Comandos \u00fatiles de Slurm \u00b6 Para aprender sobre todas las opciones disponibles para cada comando, ingrese man <comando> mientras est\u00e9 conectado al entorno del Cluster. Comando Definici\u00f3n sbatch Env\u00eda scripts de trabajos a la cola de ejecuci\u00f3n squeue Muestra estado de los jobs scontrol Usado para mostrar estado de Slurm (varias opciones disponibles solo para root) sinfo Muestra estado de particiones y nodos salloc Env\u00eda un job para ejecuci\u00f3n o inicia un trabajo en tiempo real Videos tutoriales \u00b6 How to login How to use EUPS How to access the Scratch How to submit a Job","title":"C\u00f3mo Utilizar"},{"location":"es/processamento/uso/howtouse-HPC.html#como-utilizar","text":"","title":"C\u00f3mo Utilizar"},{"location":"es/processamento/uso/howtouse-HPC.html#como-acceder","text":"El acceso a nuestro cl\u00faster puede realizarse a trav\u00e9s de Open OnDemand o mediante el Terminal de JupyterLab (K8S) . En ambas opciones, es imprescindible poseer una cuenta v\u00e1lida en el entorno computacional de LIneA. Si no tiene una cuenta, contacte al Service Desk por correo ( helpdesk@linea.org.br ) para m\u00e1s informaci\u00f3n. Atenci\u00f3n A\u00fan teniendo una cuenta activa en LIneA, el acceso al entorno de procesamiento HPC no es autom\u00e1tico. Para m\u00e1s informaci\u00f3n contacte al Service Desk en helpdesk@linea.org.br . Accediendo por terminal de JupyterLab En la pantalla inicial de su Jupyter Notebook, en la secci\u00f3n \"Other\" , encontrar\u00e1 el bot\u00f3n del terminal. Al hacer clic en \u00e9l, ser\u00e1 redirigido a un terminal Linux, inicialmente ubicado en su directorio home . Para acceder al Cluster Apollo, simplemente ejecute el siguiente comando: ssh loginapl01 La m\u00e1quina loginapl01 es donde podr\u00e1 asignar nodos de computaci\u00f3n para enviar su job. $HOME y $SCRATCH Los nodos de computaci\u00f3n no tienen acceso a su directorio home de usuario. Mueva o copie todos los archivos necesarios para el env\u00edo de su job a su directorio SCRATCH.","title":"C\u00f3mo Acceder"},{"location":"es/processamento/uso/howtouse-HPC.html#como-usar-el-area-scratch","text":"Su directorio de SCRATCH es el lugar donde puede dirigir los archivos de resultados de su trabajo, as\u00ed como almacenar datos temporalmente que el c\u00f3digo utiliza en el momento del procesamiento. Para acceder a su directorio SCRATCH: cd $SCRATCH Para enviar archivos a su directorio SCRATCH: cp <ARCHIVO> $SCRATCH","title":"C\u00f3mo usar el \u00e1rea SCRATCH"},{"location":"es/processamento/uso/howtouse-HPC.html#como-usar-el-area-de-script","text":"Su directorio de SCRIPTS es el lugar donde puede almacenar scripts, c\u00f3digos para ejecutarse en el cl\u00faster. Tambi\u00e9n se recomienda utilizar esta \u00e1rea para crear un entorno de conda. Para acceder a su directorio de SCRIPTS: cd $SCRIPTS Ser Consciente El \u00e1rea de script no est\u00e1 incluida en la rutina de backups. Por lo tanto, no debe usarse como almacenamiento de datos permanente.","title":"C\u00f3mo usar el \u00e1rea de script"},{"location":"es/processamento/uso/howtouse-HPC.html#como-enviar-un-job","text":"Un Job solicita recursos de computaci\u00f3n y especifica las aplicaciones a iniciar en esos recursos, junto con cualquier dato/opci\u00f3n de entrada y directiva de salida. La gesti\u00f3n y programaci\u00f3n de tareas y recursos del cl\u00faster se realiza a trav\u00e9s de Slurm. Por lo tanto, para enviar un Job necesita usar un script como el siguiente: #!/bin/bash #SBATCH -p PARTICI\u00d3N #Nombre de la Partici\u00f3n a usar #SBATCH --nodelist=NODO #Nombre del Nodo a asignar #SBATCH -J simple-job #Nombre del Job #----------------------------------------------------------------------------# ##ruta al c\u00f3digo ejecutable EXEC = /scripts/USUARIO/CODIGO.EJECUTABLE srun $EXEC En este script debe especificar: el nombre de la cola (Partition) a usar; el nombre del nodo a asignar para la ejecuci\u00f3n del Job; y la ruta al c\u00f3digo/programa a ejecutar. ADVERTENCIA Est\u00e1 estrictamente prohibido enviar jobs directamente a la m\u00e1quina loginapl01 . Cualquier c\u00f3digo ejecut\u00e1ndose en esta m\u00e1quina ser\u00e1 interrumpido inmediatamente sin previo aviso. Para enviar el Job: sbatch script-submit-job.sh Si el script es correcto habr\u00e1 una salida que indica el ID del job . Para verificar el progreso e informaci\u00f3n del Job: scontrol show job <ID> Para cancelar el Job: scancel <ID> Acceso a internet Los nodos de computaci\u00f3n no tienen acceso a internet. Paquetes y bibliotecas deben instalarse desde loginapl01 en su \u00e1rea scratch.","title":"C\u00f3mo Enviar un Job"},{"location":"es/processamento/uso/howtouse-HPC.html#gestor-de-paquetes-eups","text":"EUPS es un gestor de paquetes alternativo (y oficial de LSST) que permite cargar variables de entorno e incluir la ruta a programas y bibliotecas de forma modular. Para cargar EUPS: Info Actualmente, EUPS se carga autom\u00e1ticamente despu\u00e9s de que el usuario accede a cualquier m\u00e1quina del cl\u00faster Apollo. source /opt/eups/bin/setups.sh Para listar todos los paquetes disponibles: eups list Para listar un paquete espec\u00edfico: eups list | grep <PAQUETE> Para cargar un paquete en la sesi\u00f3n actual: setup <NOMBRE DEL PAQUETE> <VERSI\u00d3N DEL PAQUETE> Para eliminar el paquete cargado: unsetup <NOMBRE DEL PAQUETE> <VERSI\u00d3N DEL PAQUETE>","title":"Gestor de paquetes EUPS"},{"location":"es/processamento/uso/howtouse-HPC.html#comandos-utiles-de-slurm","text":"Para aprender sobre todas las opciones disponibles para cada comando, ingrese man <comando> mientras est\u00e9 conectado al entorno del Cluster. Comando Definici\u00f3n sbatch Env\u00eda scripts de trabajos a la cola de ejecuci\u00f3n squeue Muestra estado de los jobs scontrol Usado para mostrar estado de Slurm (varias opciones disponibles solo para root) sinfo Muestra estado de particiones y nodos salloc Env\u00eda un job para ejecuci\u00f3n o inicia un trabajo en tiempo real","title":"Comandos \u00fatiles de Slurm"},{"location":"es/processamento/uso/howtouse-HPC.html#videos-tutoriales","text":"How to login How to use EUPS How to access the Scratch How to submit a Job","title":"Videos tutoriales"},{"location":"es/processamento/uso/openondemand.html","text":"Open OnDemand \u00b6 Open OnDemand es una interfaz que facilita el uso de nuestro entorno HPC conformado por el Cluster Apollo. Para acceder es necesario tener una cuenta v\u00e1lida en LIneA ( m\u00e1s informaci\u00f3n ). El acceso a Open OnDemand es a trav\u00e9s de https://ondemand.linea.org.br/ . En la pantalla inicial de la plataforma, en la parte superior, se puede visualizar un men\u00fa con los siguientes elementos: Files - proporciona una interfaz para tu directorio de usuario ( Home Directory ). Jobs - proporciona interfaces para las pantallas \"Active Jobs\" y \"Job Composer\". Clusters - proporciona acceso al terminal en un navegador web. Interactive Apps - proporciona acceso a Jupyter Notebook. Home Directory \u00b6 El Home Directory permite visualizar el directorio de usuario donde se almacenan tus archivos, adem\u00e1s de mostrar varios botones con diferentes funcionalidades. Cambiando de Directorio \u00b6 Al hacer clic en el bot\u00f3n Change Directory puedes navegar por nuestra infraestructura. Solo escribe la ruta en el campo Path y luego presiona \" ok \". Accediendo al Terminal \u00b6 Al hacer clic en el bot\u00f3n Open Terminal acceder\u00e1s al terminal Linux en la m\u00e1quina de login (loginapl01) del Cluster Apollo. En este terminal, te encuentras en tu directorio \" Home \" de usuario y puedes visualizar tus archivos. Tambi\u00e9n puedes ejecutar todas las operaciones habituales de usuario HPC, como asignar nodos de computaci\u00f3n y verificar la cola Slurm usando comandos. Creando, Transfiriendo y Moviendo Archivos \u00b6 En Open OnDemand, crear nuevos archivos y directorios es muy sencillo, as\u00ed como transferirlos. Haz clic en los botones \"New File\" o \"New Directory\" y elige el nombre que deseas para el nuevo archivo o directorio. El bot\u00f3n \"Upload\" te permite transferir archivos desde tu m\u00e1quina local a tu directorio home en nuestro entorno. El bot\u00f3n \"Download\" te permite enviar los archivos seleccionados a tu m\u00e1quina local. Para visualizar, renombrar o editar un archivo creado, haz clic en los \"tres puntos\" que aparecen junto al archivo para ver un men\u00fa con estas opciones. Para mover o copiar archivos sigue estos pasos: Selecciona el archivo deseado Haz clic en \"Copy/Move\" Haz clic en \"Change Directory\" y escribe la ruta del directorio destino Haz clic en \"Copy\" o \"Move\" en el cuadro que aparece en el lado izquierdo de la pantalla. Jobs \u00b6 En la secci\u00f3n Jobs del men\u00fa principal, se encuentran las opciones \"Job Composer\" y \"Active Jobs\". La pantalla \"Job Composer\" facilita el proceso de env\u00edo de jobs y en \"Active Jobs\" ] puedes monitorear la ejecuci\u00f3n de tu Job con detalles. Para enviar un job necesitas usar un script de env\u00edo como este: ( m\u00e1s informaci\u00f3n ) #!/bin/bash #SBATCH -p PARTICI\u00d3N #Nombre de la Partici\u00f3n a usar #SBATCH --nodelist=NODO #Nombre del Nodo a asignar #SBATCH -J simple-job #Nombre del Job #----------------------------------------------------------------------------# ##ruta al c\u00f3digo ejecutable EXEC = /scratch/users/USUARIO/ondemand/projects/CODIGO.EJECUTABLE srun $EXEC Para ver m\u00e1s plantillas de scripts de env\u00edo de Jobs, haz clic aqu\u00ed Job Composer \u00b6 Open OnDemand simplifica todo el proceso de env\u00edo de jobs mediante la herramienta Job Composer . Solo sigue estos pasos: Haz clic en el bot\u00f3n \"New Job\" Elige la opci\u00f3n \"Default Template\" Edita las especificaciones del script de env\u00edo en \"Open Editor\" Haz clic en \"Submit\" para que el Job comience su ejecuci\u00f3n. Aviso Importante Los nodos de computaci\u00f3n del cluster no tienen acceso a tu directorio de usuario (Home Directory). Mueve o copia todos los archivos necesarios para el env\u00edo de tu job a tu directorio SCRATCH. JupyterLab \u00b6 Con Open OnDemand, es posible acceder a Jupyter Notebook en nuestro entorno HPC. A trav\u00e9s de \"Interactive Apps\", Jupyter Notebook iniciar\u00e1 una sesi\u00f3n en uno de los nodos de computaci\u00f3n del cluster, para ello: Haz clic en \"Jupyter Notebook\" Completa los campos \"Account\", \"Partition\" y \"Select node\" Presiona el bot\u00f3n \"Launch\" Con\u00e9ctate a JupyterLab haciendo clic en \"Connect to Jupyter\" . Jupyter en Kubernetes VS Jupyter en HPC Tenemos dos entornos Jupyter en infraestructuras distintas. Uno disponible en https://jupyter.linea.org.br que se ejecuta sobre Kubernetes . El otro, mencionado arriba, se ejecuta de forma interactiva mediante Open OnDemand directamente en los nodos de procesamiento. Al abrir un terminal dentro de JupyterLab v\u00eda Open OnDemand, podr\u00e1s ver que est\u00e1s ubicado en un nodo del Cluster Apollo y tienes acceso a tu \u00e1rea \" scratch \" en el almacenamiento Lustre. Creando Kernels Python \u00b6 Los siguientes comandos deben ejecutarse en el terminal en \"LIneA Shell Access\" . Para acceder: En la p\u00e1gina inicial de Open OnDemand, haz clic en: Clusters -> LIneA Shell Access . Sigue estos comandos: Ve a tu \u00e1rea SCRATCH, instala y carga Miniconda: cd $SCRATCH curl -L -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod +x Miniconda3-latest-Linux-x86_64.sh ./Miniconda3-latest-Linux-x86_64.sh -p $SCRATCH /miniconda source miniconda/bin/activate conda deactivate #Necesario para desactivar el env \"base\" Crea, activa un venv conda e instala ipykernel : conda create -p $SCRATCH /kernelname conda activate kernelname/ conda install -c anaconda ipykernel Configura JUPYTER_PATH (debe ser exactamente esta ruta): JUPYTER_PATH = $SCRATCH /.local echo $JUPYTER_PATH python -m ipykernel install --prefix = $JUPYTER_PATH --name 'kernelname' Abre una sesi\u00f3n de Jupyter Notebook. La salida del \u00faltimo comando debe ser: #[InstallIPythonKernelSpecApp] WARNING | Installing to /lustre/t0/scratch/users/USUARIO/.local/share/jupyter/kernels, which is not in ['/lustre/t0/scratch/users/USUARIO/kernelname/share/jupyter/kernels', '/home/USUARIO/.local/share/jupyter/kernels', '/usr/local/share/jupyter/kernels', '/usr/share/jupyter/kernels', '/home/USUARIO/.ipython/kernels']. The kernelspec may not be found. Installed kernelspec kernelname in /lustre/t0/scratch/users/USUARIO/.local/share/jupyter/kernels/kernelname Al finalizar estos comandos, podr\u00e1s ver el bot\u00f3n del kernel python creado. Videos tutoriales \u00b6 C\u00f3mo enviar un Job C\u00f3mo acceder a Jupyter Notebook Para cualquier duda, contacta al Service Desk .","title":"Open OnDemand"},{"location":"es/processamento/uso/openondemand.html#open-ondemand","text":"Open OnDemand es una interfaz que facilita el uso de nuestro entorno HPC conformado por el Cluster Apollo. Para acceder es necesario tener una cuenta v\u00e1lida en LIneA ( m\u00e1s informaci\u00f3n ). El acceso a Open OnDemand es a trav\u00e9s de https://ondemand.linea.org.br/ . En la pantalla inicial de la plataforma, en la parte superior, se puede visualizar un men\u00fa con los siguientes elementos: Files - proporciona una interfaz para tu directorio de usuario ( Home Directory ). Jobs - proporciona interfaces para las pantallas \"Active Jobs\" y \"Job Composer\". Clusters - proporciona acceso al terminal en un navegador web. Interactive Apps - proporciona acceso a Jupyter Notebook.","title":"Open OnDemand"},{"location":"es/processamento/uso/openondemand.html#home-directory","text":"El Home Directory permite visualizar el directorio de usuario donde se almacenan tus archivos, adem\u00e1s de mostrar varios botones con diferentes funcionalidades.","title":"Home Directory"},{"location":"es/processamento/uso/openondemand.html#jobs","text":"En la secci\u00f3n Jobs del men\u00fa principal, se encuentran las opciones \"Job Composer\" y \"Active Jobs\". La pantalla \"Job Composer\" facilita el proceso de env\u00edo de jobs y en \"Active Jobs\" ] puedes monitorear la ejecuci\u00f3n de tu Job con detalles. Para enviar un job necesitas usar un script de env\u00edo como este: ( m\u00e1s informaci\u00f3n ) #!/bin/bash #SBATCH -p PARTICI\u00d3N #Nombre de la Partici\u00f3n a usar #SBATCH --nodelist=NODO #Nombre del Nodo a asignar #SBATCH -J simple-job #Nombre del Job #----------------------------------------------------------------------------# ##ruta al c\u00f3digo ejecutable EXEC = /scratch/users/USUARIO/ondemand/projects/CODIGO.EJECUTABLE srun $EXEC Para ver m\u00e1s plantillas de scripts de env\u00edo de Jobs, haz clic aqu\u00ed","title":"Jobs"},{"location":"es/processamento/uso/openondemand.html#job-composer","text":"Open OnDemand simplifica todo el proceso de env\u00edo de jobs mediante la herramienta Job Composer . Solo sigue estos pasos: Haz clic en el bot\u00f3n \"New Job\" Elige la opci\u00f3n \"Default Template\" Edita las especificaciones del script de env\u00edo en \"Open Editor\" Haz clic en \"Submit\" para que el Job comience su ejecuci\u00f3n. Aviso Importante Los nodos de computaci\u00f3n del cluster no tienen acceso a tu directorio de usuario (Home Directory). Mueve o copia todos los archivos necesarios para el env\u00edo de tu job a tu directorio SCRATCH.","title":"Job Composer"},{"location":"es/processamento/uso/openondemand.html#jupyterlab","text":"Con Open OnDemand, es posible acceder a Jupyter Notebook en nuestro entorno HPC. A trav\u00e9s de \"Interactive Apps\", Jupyter Notebook iniciar\u00e1 una sesi\u00f3n en uno de los nodos de computaci\u00f3n del cluster, para ello: Haz clic en \"Jupyter Notebook\" Completa los campos \"Account\", \"Partition\" y \"Select node\" Presiona el bot\u00f3n \"Launch\" Con\u00e9ctate a JupyterLab haciendo clic en \"Connect to Jupyter\" . Jupyter en Kubernetes VS Jupyter en HPC Tenemos dos entornos Jupyter en infraestructuras distintas. Uno disponible en https://jupyter.linea.org.br que se ejecuta sobre Kubernetes . El otro, mencionado arriba, se ejecuta de forma interactiva mediante Open OnDemand directamente en los nodos de procesamiento. Al abrir un terminal dentro de JupyterLab v\u00eda Open OnDemand, podr\u00e1s ver que est\u00e1s ubicado en un nodo del Cluster Apollo y tienes acceso a tu \u00e1rea \" scratch \" en el almacenamiento Lustre.","title":"JupyterLab"},{"location":"es/processamento/uso/openondemand.html#creando-kernels-python","text":"Los siguientes comandos deben ejecutarse en el terminal en \"LIneA Shell Access\" . Para acceder: En la p\u00e1gina inicial de Open OnDemand, haz clic en: Clusters -> LIneA Shell Access . Sigue estos comandos: Ve a tu \u00e1rea SCRATCH, instala y carga Miniconda: cd $SCRATCH curl -L -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod +x Miniconda3-latest-Linux-x86_64.sh ./Miniconda3-latest-Linux-x86_64.sh -p $SCRATCH /miniconda source miniconda/bin/activate conda deactivate #Necesario para desactivar el env \"base\" Crea, activa un venv conda e instala ipykernel : conda create -p $SCRATCH /kernelname conda activate kernelname/ conda install -c anaconda ipykernel Configura JUPYTER_PATH (debe ser exactamente esta ruta): JUPYTER_PATH = $SCRATCH /.local echo $JUPYTER_PATH python -m ipykernel install --prefix = $JUPYTER_PATH --name 'kernelname' Abre una sesi\u00f3n de Jupyter Notebook. La salida del \u00faltimo comando debe ser: #[InstallIPythonKernelSpecApp] WARNING | Installing to /lustre/t0/scratch/users/USUARIO/.local/share/jupyter/kernels, which is not in ['/lustre/t0/scratch/users/USUARIO/kernelname/share/jupyter/kernels', '/home/USUARIO/.local/share/jupyter/kernels', '/usr/local/share/jupyter/kernels', '/usr/share/jupyter/kernels', '/home/USUARIO/.ipython/kernels']. The kernelspec may not be found. Installed kernelspec kernelname in /lustre/t0/scratch/users/USUARIO/.local/share/jupyter/kernels/kernelname Al finalizar estos comandos, podr\u00e1s ver el bot\u00f3n del kernel python creado.","title":"Creando Kernels Python"},{"location":"es/processamento/uso/openondemand.html#videos-tutoriales","text":"C\u00f3mo enviar un Job C\u00f3mo acceder a Jupyter Notebook Para cualquier duda, contacta al Service Desk .","title":"Videos tutoriales"},{"location":"es/processamento/uso/politicadeuso.html","text":"Pol\u00edtica de Uso \u00b6 Pol\u00edtica de Uso de la Infraestructura Computacional del Laboratorio Interinstitucional de e-Astronom\u00eda - LIneA. Esta Pol\u00edtica de Uso de la Infraestructura Computacional del Laboratorio Interinstitucional de e-Astronom\u00eda cubre todos los recursos computacionales puestos a disposici\u00f3n para la investigaci\u00f3n cient\u00edfica. Este conjunto de reglas tiene como objetivo: Garantizar un entorno computacional amigable, productivo y confiable; Mantener un entorno que permita el desarrollo y la innovaci\u00f3n; Proteger la reputaci\u00f3n del Laboratorio Interinstitucional de e-Astronom\u00eda (LIneA) y proteger sus sistemas inform\u00e1ticos y los datos que contienen contra ataques externos y uso no autorizado; Garantizar el cumplimiento de todas las pautas legales. Todos los usuarios deben esforzarse por mantener la seguridad del entorno. La cuenta de usuario es de uso exclusivo, personal y no puede compartirse. Los intentos de acceso no autorizados destinados a da\u00f1ar, alterar, falsificar o eliminar datos, falsificar direcciones de correo electr\u00f3nico o causar interrupciones del servicio est\u00e1n prohibidos. Los recursos computacionales de LIneA deben usarse \u00fanica y exclusivamente para la ejecuci\u00f3n de tareas relacionadas con proyectos cient\u00edficos registrados previamente. Se proh\u00edbe expresamente el uso de estos recursos para: Actividades ilegales de cualquier naturaleza; Actividades que ofendan o resulten en verg\u00fcenza p\u00fablica de colaboradores, usuarios o terceros; Actividades que impliquen ganancias financieras personales; Descarga, carga o almacenamiento de material con contenido sexual o ilegal; Ejecuci\u00f3n de tareas que utilicen una cantidad de recursos incompatible con el uso compartido de la infraestructura y que interfieran con el rendimiento general del sistema; Ejecuci\u00f3n de tareas que resulten en la violaci\u00f3n de licencias de aplicaciones computacionales instaladas previamente; Distribuci\u00f3n o almacenamiento de m\u00fasica, pel\u00edculas o cualquier material protegido por derechos de autor sin la licencia adecuada. Las preguntas sobre la idoneidad del uso de los recursos computacionales deben comunicarse directamente a LIneA a trav\u00e9s del Service Desk . Los datos en SCRATCH no tienen copias de seguridad. Los datos cr\u00edticos deben ser guardados por el usuario en su directorio HOME y son de su propia responsabilidad. M\u00e1s detalles sobre los sistemas de almacenamiento . Todo usuario debe informar inmediatamente cualquier sospecha de violaci\u00f3n de las reglas de seguridad enviando un correo electr\u00f3nico a helpdesk@linea.org.br . LIneA valora la privacidad de todos los colaboradores y usuarios. No se realizar\u00e1n consultas ni inspecciones de archivos sin la autorizaci\u00f3n del usuario, excepto en situaciones que involucren compromiso de la seguridad del sistema. Todo usuario de la infraestructura computacional de LIneA se compromete a: * Incluir en todos los art\u00edculos enviados a revistas cient\u00edficas, actas de conferencias, tesis, disertaciones, etc., que hagan uso de los recursos computacionales de LIneA, el siguiente reconocimiento: \"This research was supported by resources supplied by the Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\"; o, \"Esta pesquisa tornou-se poss\u00edvel gra\u00e7as aos recursos computacionais disponibilizados pelo Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\"; Enviar al CDE una copia de la publicaci\u00f3n mencionada que debe incluirse en el sitio web oficial de LIneA.","title":"Pol\u00edtica de Uso"},{"location":"es/processamento/uso/politicadeuso.html#politica-de-uso","text":"Pol\u00edtica de Uso de la Infraestructura Computacional del Laboratorio Interinstitucional de e-Astronom\u00eda - LIneA. Esta Pol\u00edtica de Uso de la Infraestructura Computacional del Laboratorio Interinstitucional de e-Astronom\u00eda cubre todos los recursos computacionales puestos a disposici\u00f3n para la investigaci\u00f3n cient\u00edfica. Este conjunto de reglas tiene como objetivo: Garantizar un entorno computacional amigable, productivo y confiable; Mantener un entorno que permita el desarrollo y la innovaci\u00f3n; Proteger la reputaci\u00f3n del Laboratorio Interinstitucional de e-Astronom\u00eda (LIneA) y proteger sus sistemas inform\u00e1ticos y los datos que contienen contra ataques externos y uso no autorizado; Garantizar el cumplimiento de todas las pautas legales. Todos los usuarios deben esforzarse por mantener la seguridad del entorno. La cuenta de usuario es de uso exclusivo, personal y no puede compartirse. Los intentos de acceso no autorizados destinados a da\u00f1ar, alterar, falsificar o eliminar datos, falsificar direcciones de correo electr\u00f3nico o causar interrupciones del servicio est\u00e1n prohibidos. Los recursos computacionales de LIneA deben usarse \u00fanica y exclusivamente para la ejecuci\u00f3n de tareas relacionadas con proyectos cient\u00edficos registrados previamente. Se proh\u00edbe expresamente el uso de estos recursos para: Actividades ilegales de cualquier naturaleza; Actividades que ofendan o resulten en verg\u00fcenza p\u00fablica de colaboradores, usuarios o terceros; Actividades que impliquen ganancias financieras personales; Descarga, carga o almacenamiento de material con contenido sexual o ilegal; Ejecuci\u00f3n de tareas que utilicen una cantidad de recursos incompatible con el uso compartido de la infraestructura y que interfieran con el rendimiento general del sistema; Ejecuci\u00f3n de tareas que resulten en la violaci\u00f3n de licencias de aplicaciones computacionales instaladas previamente; Distribuci\u00f3n o almacenamiento de m\u00fasica, pel\u00edculas o cualquier material protegido por derechos de autor sin la licencia adecuada. Las preguntas sobre la idoneidad del uso de los recursos computacionales deben comunicarse directamente a LIneA a trav\u00e9s del Service Desk . Los datos en SCRATCH no tienen copias de seguridad. Los datos cr\u00edticos deben ser guardados por el usuario en su directorio HOME y son de su propia responsabilidad. M\u00e1s detalles sobre los sistemas de almacenamiento . Todo usuario debe informar inmediatamente cualquier sospecha de violaci\u00f3n de las reglas de seguridad enviando un correo electr\u00f3nico a helpdesk@linea.org.br . LIneA valora la privacidad de todos los colaboradores y usuarios. No se realizar\u00e1n consultas ni inspecciones de archivos sin la autorizaci\u00f3n del usuario, excepto en situaciones que involucren compromiso de la seguridad del sistema. Todo usuario de la infraestructura computacional de LIneA se compromete a: * Incluir en todos los art\u00edculos enviados a revistas cient\u00edficas, actas de conferencias, tesis, disertaciones, etc., que hagan uso de los recursos computacionales de LIneA, el siguiente reconocimiento: \"This research was supported by resources supplied by the Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\"; o, \"Esta pesquisa tornou-se poss\u00edvel gra\u00e7as aos recursos computacionais disponibilizados pelo Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA).\"; Enviar al CDE una copia de la publicaci\u00f3n mencionada que debe incluirse en el sitio web oficial de LIneA.","title":"Pol\u00edtica de Uso"},{"location":"es/processamento/uso/templates-jobs.html","text":"Enviar Scripts de Trabajo - Plantillas \u00b6 Script Simple de Trabajo \u00b6 #!/bin/bash #SBATCH -p PARTICI\u00d3N #Nombre de la Partici\u00f3n a usar #SBATCH --nodelist=NOMBREDELNODO #Nombre del Nodo a asignar #SBATCH -J NOMBRE-DEL-TRABAJO #Nombre del Trabajo #----------------------------------------------------------------------------# ##ruta al c\u00f3digo ejecutable EXEC = /scripts/USUARIO/CODIGO.EJECUTABLE srun $EXEC En este script debe especificar el nombre de la cola (Partition) a usar, el nombre del nodo a asignar para la ejecuci\u00f3n del Job, y la ruta al c\u00f3digo/programa a ejecutar. Script de Carga EUPS \u00b6 #!/bin/bash #SBATCH -p PARTICI\u00d3N #Nombre de la Partici\u00f3n a usar #SBATCH --nodelist=NOMBREDELNODO #Nombre del Nodo a asignar #SBATCH -J NOMBRE-DEL-TRABAJO #Nombre del Trabajo #----------------------------------------------------------------------------# #Cargar EUPS . /mnt/eups/linea_eups_setup.sh #Cargar paquete setup <PAQUETE> <VERSI\u00d3N> ##ruta al c\u00f3digo ejecutable EXEC = /scripts/USUARIO/CODIGO.EJECUTABLE srun $EXEC Script de Env\u00edo Paralelo \u00b6 OpenMP \u00b6 MPI \u00b6","title":"Scripts de Trabajo (ejemplos)"},{"location":"es/processamento/uso/templates-jobs.html#enviar-scripts-de-trabajo-plantillas","text":"","title":"Enviar Scripts de Trabajo - Plantillas"},{"location":"es/processamento/uso/templates-jobs.html#script-de-envio-paralelo","text":"","title":"Script de Env\u00edo Paralelo"},{"location":"es/sci-platforms/index.html","text":"LIneA Science Platform \u00b6 La Plataforma Cient\u00edfica LIneA ( scienceplatform.linea.org.br ) es una plataforma en l\u00ednea, a\u00fan en fase de desarrollo, que agrupar\u00e1 un conjunto de servicios y herramientas ofrecidos para facilitar el acceso y an\u00e1lisis de datos astron\u00f3micos alojados en LIneA. Actualmente, una versi\u00f3n de prueba de la plataforma est\u00e1 disponible para evaluaci\u00f3n por los usuarios en scienceplatform-dev.linea.org.br/lsp . IDAC Science Platform \u00b6 Durante la operaci\u00f3n del relevamiento LSST , una versi\u00f3n reducida de la Plataforma Cient\u00edfica LIneA, solo con los servicios relacionados al proyecto LSST, ser\u00e1 el principal punto de acceso a los datos alojados en el Centro Brasile\u00f1o de Acceso Independiente a Datos (IDAC-Brasil) para los cient\u00edficos de la comunidad brasile\u00f1a. En los a\u00f1os previos a la operaci\u00f3n del LSST, los servicios del IDAC-Brasil ya proporcionan acceso a datos p\u00fablicos de otros relevamientos, como el Dark Energy Survey , para que la comunidad pueda prepararse y familiarizarse con las diversas herramientas ofrecidas. Servicios \u00b6 Servicios de acceso a datos con diversas herramientas para procesamiento, exploraci\u00f3n, visualizaci\u00f3n y an\u00e1lisis de datos p\u00fablicos y privados. JupyterHub \u00b6 OnDemand \u00b6 User Query \u00b6 TAP Service \u00b6 Sky Viewer \u00b6 Target Viewer \u00b6 DES Science Server \u00b6 Occultation Prediction Database \u00b6 SDSS Sky Server \u00b6 LSST PZ Server \u00b6 Portales Cient\u00edficos \u00b6 Conjuntos de herramientas desarrolladas espec\u00edficamente para cada proyecto y de uso restringido a los miembros de cada colaboraci\u00f3n cient\u00edfica. DES Science Portal \u00b6 Solar System Portal \u00b6 MaNGA Portal \u00b6","title":"Plataformas Cient\u00edficas"},{"location":"es/sci-platforms/index.html#linea-science-platform","text":"La Plataforma Cient\u00edfica LIneA ( scienceplatform.linea.org.br ) es una plataforma en l\u00ednea, a\u00fan en fase de desarrollo, que agrupar\u00e1 un conjunto de servicios y herramientas ofrecidos para facilitar el acceso y an\u00e1lisis de datos astron\u00f3micos alojados en LIneA. Actualmente, una versi\u00f3n de prueba de la plataforma est\u00e1 disponible para evaluaci\u00f3n por los usuarios en scienceplatform-dev.linea.org.br/lsp .","title":"LIneA Science Platform"},{"location":"es/sci-platforms/index.html#idac-science-platform","text":"Durante la operaci\u00f3n del relevamiento LSST , una versi\u00f3n reducida de la Plataforma Cient\u00edfica LIneA, solo con los servicios relacionados al proyecto LSST, ser\u00e1 el principal punto de acceso a los datos alojados en el Centro Brasile\u00f1o de Acceso Independiente a Datos (IDAC-Brasil) para los cient\u00edficos de la comunidad brasile\u00f1a. En los a\u00f1os previos a la operaci\u00f3n del LSST, los servicios del IDAC-Brasil ya proporcionan acceso a datos p\u00fablicos de otros relevamientos, como el Dark Energy Survey , para que la comunidad pueda prepararse y familiarizarse con las diversas herramientas ofrecidas.","title":"IDAC Science Platform"},{"location":"es/sci-platforms/index.html#servicios","text":"Servicios de acceso a datos con diversas herramientas para procesamiento, exploraci\u00f3n, visualizaci\u00f3n y an\u00e1lisis de datos p\u00fablicos y privados.","title":"Servicios"},{"location":"es/sci-platforms/index.html#portales-cientificos","text":"Conjuntos de herramientas desarrolladas espec\u00edficamente para cada proyecto y de uso restringido a los miembros de cada colaboraci\u00f3n cient\u00edfica.","title":"Portales Cient\u00edficos"},{"location":"es/sci-platforms/des.html","text":"El DES Science Portal ( des-portal.linea.org.br ) fue una plataforma desarrollada para el proyecto Dark Energy Survey (DES) que alberg\u00f3 una variedad de pipelines dise\u00f1ados para preparar cat\u00e1logos personalizados para diferentes aplicaciones en astronom\u00eda, as\u00ed como para realizar diversos an\u00e1lisis cient\u00edficos. La parte que involucra el procesamiento de datos del DES Science Portal fue discontinuada debido a la obsolescencia de las tecnolog\u00edas involucradas, pero su sistema de acceso a los resultados generados y de control de procedencia sigue disponible para los miembros de la colaboraci\u00f3n DES. Las funcionalidades del DES Science Portal han sido gradualmente reemplazadas por otros servicios m\u00e1s modernos desarrollados por LIneA, preparados para manejar Big Data en preparaci\u00f3n para el LSST, manteniendo su filosof\u00eda y buenas pr\u00e1cticas. Los pipelines del DES Science Portal fueron agrupados en etapas: Preparaci\u00f3n de Datos - incluye la creaci\u00f3n de mapas de efectos sistem\u00e1ticos, clasificaci\u00f3n de objetos (estrella/galaxia), c\u00e1lculo de redshifts fotom\u00e9tricos y otros valores agregados, dependiendo de la aplicaci\u00f3n. Por ejemplo, estimaci\u00f3n de edad, metalicidad o masa estelar de galaxias. Cat\u00e1logo de \"Valor A\u00f1adido\" - combina los datos y resultados obtenidos en la etapa anterior, selecciona las columnas de inter\u00e9s y aplica cortes de calidad y limpieza de datos, con criterios fuertemente dependientes de la aplicaci\u00f3n cient\u00edfica para la cual ser\u00e1 destinado el cat\u00e1logo. Flujos de Trabajo Cient\u00edficos - agrega diversos pipelines de an\u00e1lisis cient\u00edfico. Toma como entrada los cat\u00e1logos creados en la etapa anterior. Uno de los principales puntos fuertes del DES Science Portal fue su capacidad para proporcionar informaci\u00f3n completa de todo el historial de procesos ejecutados, permitiendo al usuario rastrear las entradas, configuraciones, versiones de los c\u00f3digos utilizados y los resultados obtenidos, en forma de un registro del producto con gr\u00e1ficos y tablas informativas. El portal tambi\u00e9n proporcion\u00f3 acceso a una serie de herramientas dise\u00f1adas para el usuario, el desarrollador y el administrador. Para profundizar en los detalles sobre el DES Science Portal, lea los dos art\u00edculos publicados en la revista Astronomy and Computing : Gschwend et al. 2018 - DES science portal: Computing photometric redshifts ( doi.org/10.1016/j.ascom.2018.08.008 ) ( arXiv:1708.05643 ) Fausti Neto et al. 2018 - DES science portal: Creating science-ready catalogs ( doi.org/10.1016/j.ascom.2018.01.002 ) ( arXiv:1708.05642 )","title":"DES Science Portal"},{"location":"es/sci-platforms/jupyter.html","text":"JupyterHub es un entorno de desarrollo multiusuario basado en iPython Notebooks que ofrece acceso a recursos computacionales compartidos en un servidor remoto, sin necesidad de instalaci\u00f3n ni mantenimiento por parte de los usuarios. Los \u00fanicos requisitos para acceder a JupyterHub son: tener una cuenta de usuario en LIneA (consulte aqu\u00ed c\u00f3mo crear su cuenta) y un navegador con acceso a internet. Los llamados Jupyter Notebooks permiten combinar c\u00f3digo interactivo, resultados de ejecuci\u00f3n, texto explicativo y recursos multimedia en un solo documento. Como parte de la Plataforma Cient\u00edfica LIneA , el JupyterHub de LIneA est\u00e1 integrado con otras herramientas de visualizaci\u00f3n ( Science Server ) y acceso a datos ( User Query ). De esta manera, todo el an\u00e1lisis de datos puede realizarse en l\u00ednea dentro de la plataforma, desde la lectura, visualizaci\u00f3n, procesamiento hasta el an\u00e1lisis de resultados, sin necesidad de descargar los datos. Al hacer clic en la tarjeta \"JupyterHub\" dentro de la Plataforma Cient\u00edfica LIneA, ser\u00e1 dirigido a la p\u00e1gina de inicio de sesi\u00f3n y luego a la p\u00e1gina principal de JupyterHub que mostrar\u00e1 su perfil de usuario. Haga clic en el bot\u00f3n START para comenzar. La instalaci\u00f3n est\u00e1ndar de JupyterHub utiliza la nueva interfaz JupyterLab y se basa en la imagen datascience-notebook , extendida con las bibliotecas Astropy y dblinea (la biblioteca que conecta con la base de datos). Esto significa que varias bibliotecas populares de Python como Numpy y Matplotlib estar\u00e1n autom\u00e1ticamente disponibles. Soporte al usuario \u00b6 Tutoriales en Jupyter Notebooks \u00b6 En el repositorio jupyterhub-tutorial encontrar\u00e1 tutoriales en formato notebook : 1-primeiros-passos.ipynb \u00b6 Instrucciones generales para usar la plataforma JupyterLab, consejos y atajos para escribir notebooks para diferentes tipos de celdas. 2-acesso-a-dados.ipynb \u00b6 Instrucciones para usar la biblioteca dblinea para leer datos directamente desde la base de datos dentro de una celda del notebook con ejemplos de uso (construcci\u00f3n de un diagrama color-magnitud simple para una muestra de estrellas). 3-conda-env.ipynb \u00b6 Instrucciones para crear entornos conda para gestionar bibliotecas que persistan y sobrevivan a la destrucci\u00f3n y recreaci\u00f3n de contenedores, permitiendo a los usuarios volver en una nueva sesi\u00f3n y encontrar el mismo entorno que en la sesi\u00f3n anterior (no disponible para usuarios de perfil p\u00fablico bronce). Para acceder a los notebooks , simplemente abra una Terminal en JupyterLab haciendo clic en el bot\u00f3n \"+\" en la barra superior y luego en el \u00edcono \"Terminal\" en la secci\u00f3n \"Other\" de la pesta\u00f1a \"Launcher\", e ingrese el comando: git clone https://github.com/linea-it/jupyterhub-tutorial.git Recursos Computacionales \u00b6 Configuraciones disponibles para Jupyter over K8S Despu\u00e9s de iniciar sesi\u00f3n en la plataforma, se mostrar\u00e1 un men\u00fa con hasta tres opciones de configuraci\u00f3n. Solo seleccione y haga clic en Start My Server . Tama\u00f1o CPUs RAM Small 1.0 4 GiB Medium 2.0 8 GiB Large 4.0 16 GiB Configuraci\u00f3n de servidores del entorno K8S La plataforma Jupyter se ejecuta sobre Kubernetes (K8S) y cuenta con 12 servidores f\u00edsicos dedicados. Cada m\u00e1quina est\u00e1 equipada con los siguientes recursos computacionales: Kubernetes Node Configuration RAM 64 GB Thread(s) per core 2 Core(s) per socket 6 Socket(s) 2 Jupyter over K8S vs Jupyter over HPC LIneA ofrece dos entornos separados de Jupyter Notebook. El primero se ejecuta en contenedores en la plataforma Kubernetes (K8S). El segundo est\u00e1 disponible en la plataforma Ondemand y accede directamente a la infraestructura HPC.","title":"JupyterHub"},{"location":"es/sci-platforms/jupyter.html#soporte-al-usuario","text":"","title":"Soporte al usuario"},{"location":"es/sci-platforms/jupyter.html#tutoriales-en-jupyter-notebooks","text":"En el repositorio jupyterhub-tutorial encontrar\u00e1 tutoriales en formato notebook :","title":"Tutoriales en Jupyter Notebooks"},{"location":"es/sci-platforms/jupyter.html#recursos-computacionales","text":"Configuraciones disponibles para Jupyter over K8S Despu\u00e9s de iniciar sesi\u00f3n en la plataforma, se mostrar\u00e1 un men\u00fa con hasta tres opciones de configuraci\u00f3n. Solo seleccione y haga clic en Start My Server . Tama\u00f1o CPUs RAM Small 1.0 4 GiB Medium 2.0 8 GiB Large 4.0 16 GiB Configuraci\u00f3n de servidores del entorno K8S La plataforma Jupyter se ejecuta sobre Kubernetes (K8S) y cuenta con 12 servidores f\u00edsicos dedicados. Cada m\u00e1quina est\u00e1 equipada con los siguientes recursos computacionales: Kubernetes Node Configuration RAM 64 GB Thread(s) per core 2 Core(s) per socket 6 Socket(s) 2 Jupyter over K8S vs Jupyter over HPC LIneA ofrece dos entornos separados de Jupyter Notebook. El primero se ejecuta en contenedores en la plataforma Kubernetes (K8S). El segundo est\u00e1 disponible en la plataforma Ondemand y accede directamente a la infraestructura HPC.","title":"Recursos Computacionales"},{"location":"es/sci-platforms/linea-occulation-prediction-database.html","text":"El LIneA Occultation Prediction Database es un servicio cient\u00edfico que proporciona acceso sistem\u00e1tico y actualizado a predicciones de ocultaciones estelares por cuerpos menores del Sistema Solar. Desarrollado por el Portal de Sistema Solar de LIneA, el sistema integra datos orbitales actualizados con el cat\u00e1logo estelar Gaia DR3, utilizando algoritmos especializados para calcular efem\u00e9rides y circunstancias de ocultaci\u00f3n. El servicio est\u00e1 dise\u00f1ado para atender tanto aplicaciones profesionales de investigaci\u00f3n como proyectos de ciencia ciudadana. Principales Funcionalidades \u00b6 La plataforma proporciona acceso program\u00e1tico mediante una API RESTful con endpoints para consulta por identificaci\u00f3n de objetos (nombre o designaci\u00f3n), clases din\u00e1micas (clasificaci\u00f3n Skybot), par\u00e1metros estelares (magnitud), filtros geogr\u00e1ficos, etc. La interfaz web interactiva permite visualizaci\u00f3n tabular de eventos, visualizaci\u00f3n de mapas de predicci\u00f3n din\u00e1micos, y descarga de mapas SORA y archivos KMZ. Adem\u00e1s, el servicio proporciona un paquete Python ( lineaSSP ) para integraci\u00f3n en flujos de trabajo cient\u00edficos y un servicio de suscripci\u00f3n para notificaci\u00f3n de eventos personalizados. Documentaci\u00f3n y Acceso \u00b6 Para informaci\u00f3n detallada sobre par\u00e1metros de la API y ejemplos de uso, consulte la documentaci\u00f3n completa . Acceda al servicio directamente: LIneA Occultation Prediction Database","title":"Occultation Predictions Database"},{"location":"es/sci-platforms/linea-occulation-prediction-database.html#principales-funcionalidades","text":"La plataforma proporciona acceso program\u00e1tico mediante una API RESTful con endpoints para consulta por identificaci\u00f3n de objetos (nombre o designaci\u00f3n), clases din\u00e1micas (clasificaci\u00f3n Skybot), par\u00e1metros estelares (magnitud), filtros geogr\u00e1ficos, etc. La interfaz web interactiva permite visualizaci\u00f3n tabular de eventos, visualizaci\u00f3n de mapas de predicci\u00f3n din\u00e1micos, y descarga de mapas SORA y archivos KMZ. Adem\u00e1s, el servicio proporciona un paquete Python ( lineaSSP ) para integraci\u00f3n en flujos de trabajo cient\u00edficos y un servicio de suscripci\u00f3n para notificaci\u00f3n de eventos personalizados.","title":"Principales Funcionalidades"},{"location":"es/sci-platforms/linea-occulation-prediction-database.html#documentacion-y-acceso","text":"Para informaci\u00f3n detallada sobre par\u00e1metros de la API y ejemplos de uso, consulte la documentaci\u00f3n completa . Acceda al servicio directamente: LIneA Occultation Prediction Database","title":"Documentaci\u00f3n y Acceso"},{"location":"es/sci-platforms/manga.html","text":"El portal MaNGA ( manga.linea.org.br ) fue desarrollado para satisfacer las necesidades de los miembros del Brazilian Participation Group del relevamiento MaNGA del Sloan Digital Sky Survey. El sistema fue dise\u00f1ado para permitir al equipo visualizar no solo los cubos de datos de espectroscop\u00eda IFU (Unidades de Campo Integral) reducidos, sino tambi\u00e9n los resultados del an\u00e1lisis de datos mostrando mapas de varias cantidades f\u00edsicas derivadas de los espectros. Para m\u00e1s informaci\u00f3n sobre c\u00f3mo utilizar el servicio, consulte los tutoriales en el sitio web del portal MaNGA.","title":"MaNGA Portal"},{"location":"es/sci-platforms/ondemand.html","text":"OnDemand ( ondemand.linea.org.br ) es una interfaz que facilita el acceso al entorno de computaci\u00f3n de alto rendimiento a trav\u00e9s de un navegador web. El servicio ofrece: JupyterLab integrado con el cl\u00faster Apollo Herramienta de gesti\u00f3n de archivos Acceso al entorno mediante Terminal Linux Haga clic aqu\u00ed para m\u00e1s detalles sobre c\u00f3mo utilizar el servicio OnDemand.","title":"OnDemand"},{"location":"es/sci-platforms/pz_server.html","text":"Advertencia \u00b6 Advertencia sobre la versi\u00f3n ES Esta es una traducci\u00f3n del documento original disponible en ingl\u00e9s. Elegimos no traducir los nombres de los pipelines y los tipos de productos para mantener los nombres consistentes con los que aparecen en el Photo-z Server. Adem\u00e1s, no traducimos extractos de c\u00f3digo para mantenerlos consistentes con el tutorial disponible como Jupyter notebook en el repositorio de la biblioteca Python pzserver . Introducci\u00f3n \u00b6 Inspirado en el DES Science Portal ( Gschwend et al., 2018 ; Fausti Neto et al., 2018 ), el Photo-z Server es un servicio online complementario a la Rubin Science Platform (RSP) para alojar y producir productos de datos ligeros relacionados con photo-z y ofrecer herramientas de gesti\u00f3n de datos que permiten compartir productos de datos entre usuarios de la RSP, adjuntar y compartir metadatos relevantes y facilitar el rastreo de procedencia. El servicio se aloja en el Brazilian Independent Data Access Center ( IDAC ) y est\u00e1 abierto a toda la comunidad LSST, sin restricciones geogr\u00e1ficas. Su dise\u00f1o es lo m\u00e1s amplio y gen\u00e9rico posible para facilitar la colaboraci\u00f3n cient\u00edfica de LSST con productos de datos de photo-z. Seg\u00fan lo exige el programa de contribuciones in-kind de LSST, el c\u00f3digo fuente est\u00e1 disponible p\u00fablicamente en GitHub . El Photo-z Server se dise\u00f1\u00f3 para ayudar a los usuarios de RSP a participar en el Photo-z (PZ) Validation Cooperative. Esta iniciativa del equipo Data Management (DM) se llevar\u00e1 a cabo durante la fase de commissioning de LSST (consulte la nota t\u00e9cnica dmtn-049 para obtener m\u00e1s informaci\u00f3n). El grupo de coordinaci\u00f3n de PZ recibir\u00e1 credenciales de usuario admin con permisos especiales para agregar productos de datos etiquetados como official data products . Estos incluir\u00e1n conjuntos estandarizados de entrenamiento y validaci\u00f3n utilizados para comparar el rendimiento de los algoritmos, as\u00ed como un medio para recopilar resultados de m\u00faltiples usuarios. M\u00e1s all\u00e1 de el PZ Validation Cooperative, el Photo-z Server seguir\u00e1 siendo un recurso para la Comunidad LSST en los pr\u00f3ximos a\u00f1os. Los usuarios de RSP podr\u00e1n seguir utiliz\u00e1ndolo para organizar, rastrear y compartir archivos ligeros con diversos resultados de pruebas. Datasets Los administradores del Photo-z Server mantienen y actualizan peri\u00f3dicamente una lista seleccionada de recursos de datos para apoyar a la comunidad LSST con productos de datos relacionados con photo-z. Las descripciones detalladas y los enlaces a cada producto de datos est\u00e1n disponibles en una p\u00e1gina aparte . Sitio web del Photo-z Server \u00b6 La interfaz de usuario principal del Photo-z Server es su sitio web en pzserver.linea.org.br . Los tres cards de la p\u00e1gina de inicio conducen a la lista de productos de datos (izquierda y centro) o a los pipelines del Photo-z Server (derecha). En la p\u00e1gina de la lista de productos de datos, los usuarios pueden explorar, buscar y filtrar los productos subidos por otros usuarios o creados con un pipeline del Photo-z Server. Los productos de datos subidos al Photo-z Server se vuelven autom\u00e1ticamente visibles, descargables y compartibles para todos los usuarios registrados. Cargar un nuevo producto de datos \u00b6 Para cargar un nuevo producto de datos, haga clic en el bot\u00f3n NUEVO PRODUCTO en la esquina superior derecha de la User-generated Data Products page y complete el formulario con los metadatos relevantes en cuatro pasos: Paso 1: Ingrese un nombre corto y mnemot\u00e9cnico para su nuevo producto de datos. Seleccione el tipo de producto de datos que va a cargar (p. ej., Reference Redshift Catalog, Training Set, etc.) y el release a que pertenece (si corresponde). Paso 2: Seleccione su archivo principal y todos los archivos auxiliares que desee cargar. El archivo principal contiene el producto de datos, mientras que los archivos auxiliares pueden incluir documentaci\u00f3n, descripci\u00f3n o cualquier otra informaci\u00f3n relevante sobre el producto. Si el producto de datos es tabular, la herramienta de carga podr\u00eda requerir formatos de archivo espec\u00edficos seg\u00fan su tipo. Los formatos actualmente compatibles son: CSV, FITS, HDF5 y Parquet. P\u00f3ngase en contacto con el equipo de desarrollo (mailto: pzserver-admin@linea.org.br ) si su caso cient\u00edfico requiere un formato de archivo diferente o si su archivo supera el l\u00edmite de 200 MB. Paso 3: Si el producto de datos es un Reference Redshift Catalog o un Training Set, algunas columnas son obligatorias. Los nombres de las columnas son libres, pero debe proporcionar la asociaci\u00f3n con su significado y UCD en el est\u00e1ndar IVOA , como se muestra en la figura siguiente. Paso 4: Revise su informaci\u00f3n y vuelva a los pasos anteriores si es necesario. No olvide pulsar el bot\u00f3n FINISH al final de la p\u00e1gina para enviar su producto de datos. Descargar un producto de datos \u00b6 Para descargar un producto de datos, haga clic en el icono en la fila del producto en la p\u00e1gina Productos de datos generados por el usuario . Al hacer clic, se generar\u00e1 un archivo comprimido .zip con todo el contenido del producto, incluyendo los archivos de descripci\u00f3n auxiliares. Tambi\u00e9n hay un bot\u00f3n en la p\u00e1gina de detalles del producto, al que se puede acceder haciendo clic en el nombre del producto en la lista. Compartir productos de datos \u00b6 Para compartir un producto de datos, haga clic en el icono en la fila del producto en la p\u00e1gina de Productos de Datos Generados por el Usuario o en la p\u00e1gina de detalles del producto. Al hacer clic, se abrir\u00e1 una ventana emergente con el internal_name y la URL del producto. Puede copiar la informaci\u00f3n para compartirla con otros usuarios. internal_name Cada producto de datos tiene un nombre \u00fanico (\" internal_name \"), compuesto autom\u00e1ticamente por el sistema como un n\u00famero id \u00fanico seguido del nombre elegido por el usuario, con los espacios en blanco sustituidos por guiones bajos. Este nombre es la URL de la p\u00e1gina de detalles del producto de datos en el sitio web de Photo-z Server: https://pzserver.linea.org.br/product/nombre_interno y es la clave para acceder a los datos mediante la API de Python de Photo-z Server (ver detalles a continuaci\u00f3n). La forma m\u00e1s sencilla de compartir un producto de datos es proporcionar el internal_name o la URL del producto, que lleva a la p\u00e1gina de descarga del producto. Tipos de productos \u00b6 Reference Redshift Catalog \u00b6 En el contexto de Photo-z Server, Reference Redshift Catalogs se definen como cualquier cat\u00e1logo que contenga coordenadas ecuatoriales esf\u00e9ricas y mediciones de redshift (generalmente espectrosc\u00f3picas o redshift verdaderos para simulaciones). Columnas obligatorias: Ascensi\u00f3n recta [grados] - float Declinaci\u00f3n [grados] - float Redshift - float Columna recomendada: Error de redshift - float Un Reference Redshift Catalog puede incluir datos de un \u00fanico proyecto espectrosc\u00f3pico o una combinaci\u00f3n de datos de varias fuentes. Requisitos de pipeline Si se pretende utilizar el Reference Redshift Catalog como datos de entrada para los Combine Redshift Catalogs , aplicando la funci\u00f3n de resoluci\u00f3n de duplicados (consulte detalles de la canalizaci\u00f3n aqu\u00ed ), se recomienda incluir las siguientes columnas: Indicador de calidad (asociar con z_flag en el paso 3 de la carga) - integer , float o string (el indicador de calidad original del cat\u00e1logo fuente, si est\u00e1 disponible). Tipo de medici\u00f3n - string (p. ej., \"s\" para \"spectroscopic\", \"g\" para \"grism/prism\", \"p\" para \"photometric\", seg\u00fan lo adoptado en SITCOMTN-154 ) Nombre del proyecto (asociar con survey en el paso 3 de la carga) - string (p. ej., \"DESI\", \"COSMOS2025\", \"JADES\", etc.) Otras columnas con informaci\u00f3n adicional que desee utilizar para la resoluci\u00f3n de duplicados (p. ej., resoluci\u00f3n del instrumento). Training Set \u00b6 En el contexto del Photo-z Server, los Training Sets se definen como el producto del cruce espacial entre un Reference Redshift Catalog (levantamiento individual o compilaci\u00f3n) y los datos fotom\u00e9tricos, en este caso, el LSST Object Catalog. El pipeline Training Set Maker del Photo-z Server permite a los usuarios crear Training Sets personalizados basados en los Reference Redshift Catalogs disponibles (consulte detalles del pipeline aqu\u00ed ). subconjuntos de train/test Los training sets se suelen dividir en dos o m\u00e1s subconjuntos para la validaci\u00f3n de photo-z. Si el propietario del training set ha definido previamente qu\u00e9 objetos deben pertenecer a cada subconjunto (entrenamiento y validaci\u00f3n/prueba), esta informaci\u00f3n debe estar disponible como una columna adicional en la tabla o como instrucciones claras para reproducir la separaci\u00f3n de los subconjuntos en la descripci\u00f3n del producto de datos. Para dos archivos separados, cada uno debe cargarse por separado y se convertir\u00e1 en un producto de datos independiente, ambos con el tipo de producto \"Training Set\". Su destino puede indicarse expl\u00edcitamente en el nombre y/o descripci\u00f3n del producto. training sets basados en im\u00e1genes El tipo Training Set solo admite datos a nivel de cat\u00e1logo. Los training sets basados en im\u00e1genes, por ejemplo, para algoritmos de deep-learning, no son compatibles. En este caso, utilice el tipo de producto \"Other\" y proporcione una descripci\u00f3n clara del formato de los datos en la descripci\u00f3n del producto. Para garantizar la flexibilidad en los observables, la \u00fanica columna obligatoria es el redshift ( float ). Otras columnas esperadas son: objectId del LSST Objects Catalog - integer Observables (magnitudes y/o colores, o flujos) del LSST Objects Catalog - float Errores de los observables - float Ascensi\u00f3n recta [grados] - float Declinaci\u00f3n [grados] - float Indicador de calidad - integer , float o string Indicador de subconjunto - integer , float o string Training Results \u00b6 Los resultados de entrenamiento de los algoritmos basados en machine learning tambi\u00e9n se pueden alojar en el Photo-z Server para su uso compartido y reutilizaci\u00f3n. Este tipo de producto permite archivos en formato libre. Cuando los resultados de entrenamiento se generan con el m\u00e9todo inform de RAIL , se almacenan como archivos pickle . Validation Results \u00b6 El tipo de producto Validation Results est\u00e1 dise\u00f1ado para identificar los resultados de cualquier procedimiento de validaci\u00f3n de foto-z. Puede utilizarse para almacenar los resultados del PZ Validation Cooperative o cualquier otra tarea de validaci\u00f3n. Este tipo de producto es bastante gen\u00e9rico. Puede contener estimaciones de photo-z (estimaciones individuales o PDF) de un conjunto de prueba, m\u00e9tricas de validaci\u00f3n, gr\u00e1ficos QQ-PIT, etc. Los usuarios pueden cargar un archivo principal y una lista de archivos auxiliares en cualquier formato. Photo-z Estimates \u00b6 Las estimaciones de Photo-z son el resultado de un procedimiento de estimaci\u00f3n de Photo-z, generalmente la salida del m\u00f3dulo m\u00e9todo estimate de RAIL . Si los datos superan el l\u00edmite de carga de archivos (200 MB), la entrada del producto solo almacena los metadatos y se deben proporcionar instrucciones para acceder a ellos en el campo de descripci\u00f3n. Other \u00b6 Cualquier otro producto de datos que no se ajuste a las categor\u00edas anteriores se puede cargar como producto de tipo Other. Este es un tipo de producto gen\u00e9rico que permite a los usuarios cargar cualquier formato de archivo y proporcionar una descripci\u00f3n del producto de datos en el campo de descripci\u00f3n. API & Python library \u00b6 El Photo-z Server tambi\u00e9n ofrece una API y una biblioteca de Python para facilitar el acceso a datos y metadatos mediante la l\u00ednea de comandos. La API contiene funciones para explorar los productos de datos disponibles, recuperar el contenido de un producto de datos determinado para trabajar en la memoria o descargar los archivos de inter\u00e9s. La biblioteca de Python pzserver es de c\u00f3digo abierto y est\u00e1 disponible en GitHub . Se puede instalar mediante pip con: pip install pzserver Tutorial notebook \u00b6 Un tutorial notebook con ejemplos de todos los m\u00e9todos de pzserver est\u00e1 disponible en el repositorio de la biblioteca pzserver en GitHub . Tambi\u00e9n est\u00e1 disponible la p\u00e1gina de documentaci\u00f3n de la API con m\u00e1s detalles para desarrolladores. Token de acceso \u00b6 Una vez instalada e importada en un ambiente Python, la clase PzServer abre la conexi\u00f3n remota a la base de datos de Photo-z Server. from pzserver import PzServer pz_server = PzServer ( token = \"<pegue aqu\u00ed su token de acceso>\" ) Se requiere un token de acceso para la autenticaci\u00f3n. Los usuarios pueden generarlo en el sitio web de Photo-z Server (men\u00fa de la esquina superior derecha de la p\u00e1gina de inicio). Comandos b\u00e1sicos \u00b6 Comandos b\u00e1sicos para mostrar datos y metadatos en una celda de Jupyter Notebook (si no est\u00e1 en una Jupyter Notebook, reemplace display por get para devolver los resultados como diccionarios de Python): pz_server . display_product_types () pz_server . display_releases () pz_server . display_products_list () pz_server . display_products_list ( filters = { \"release\" : \"DP1\" , \"product_type\" : \"Training Set\" }) search_results = pz_server . get_products_list ( filters = { \"product_type\" : \"results\" }) pz_server . display_product_metadata ( id or \"internal_name\" ) Comandos b\u00e1sicos para descargar o devolver datos en memoria: pz_server . download_product ( id or \"internal_name\" , save_in = \".\" ) data = pz_server . get_product ( id or \"internal_name\" ) Consulta el tutorial notebook para obtener la lista completa de ejemplos, incluyendo instrucciones para cargar y modificar metadatos con la biblioteca pzserver . Photo-z Server pipelines \u00b6 Los pipelines de Photo-z Server son un conjunto de herramientas que ayudan a los usuarios a crear y gestionar productos de datos. Los pipelines disponibles actualmente son (clic en los enlaces para m\u00e1s detalles): Combine Redshift Catalog \u00b6 Training Set Maker \u00b6 C\u00f3digo Abierto \u00b6 El Photo-z Server es un proyecto de c\u00f3digo abierto. Su c\u00f3digo fuente est\u00e1 disponible en los siguientes repositorios de GitHub: pzserver_app : el c\u00f3digo principal de la aplicaci\u00f3n, incluyendo la interfaz web y la API. pzserver : la biblioteca de Python utilizada para acceder a la API del Photo-z Server. pzserver_pipelines : el c\u00f3digo de los pipelines disponibles en el Photo-z Server. orchestration : la aplicaci\u00f3n encargada de enviar los pipelines al cl\u00faster HPC del IDAC y gestionar su ejecuci\u00f3n. pz-lsst-inkind : c\u00f3digo para tareas de gesti\u00f3n de datos en el programa in-kind del Photo-z Server, incluyendo preparaci\u00f3n de datos, control de calidad y notebooks de validaci\u00f3n de pipelines. pz-lsst-inkind-doc : documentaci\u00f3n de alto nivel sobre el programa in-kind del Photo-z Server, publicada a trav\u00e9s de GitHub Pages. El c\u00f3digo est\u00e1 licenciado bajo la Licencia MIT . \u00a1Se agradecen las contribuciones! Agradecimientos \u00b6 El Photo-z Server utiliza c\u00e1lculosl recursos del IDAC-Brasil en el Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA) con apoyo financiero del INCT do e-Universo (Proceso n\u00ba 465376/2014-2) y del proyecto FINEP: LIneA: Centro de e-Ciencia para la exploraci\u00f3n de los misterios del Universo y apoyo a proyectos de Big Data (ref n\u00ba 0883/24).","title":"LSST Photo-z Server"},{"location":"es/sci-platforms/pz_server.html#advertencia","text":"Advertencia sobre la versi\u00f3n ES Esta es una traducci\u00f3n del documento original disponible en ingl\u00e9s. Elegimos no traducir los nombres de los pipelines y los tipos de productos para mantener los nombres consistentes con los que aparecen en el Photo-z Server. Adem\u00e1s, no traducimos extractos de c\u00f3digo para mantenerlos consistentes con el tutorial disponible como Jupyter notebook en el repositorio de la biblioteca Python pzserver .","title":"Advertencia"},{"location":"es/sci-platforms/pz_server.html#introduccion","text":"Inspirado en el DES Science Portal ( Gschwend et al., 2018 ; Fausti Neto et al., 2018 ), el Photo-z Server es un servicio online complementario a la Rubin Science Platform (RSP) para alojar y producir productos de datos ligeros relacionados con photo-z y ofrecer herramientas de gesti\u00f3n de datos que permiten compartir productos de datos entre usuarios de la RSP, adjuntar y compartir metadatos relevantes y facilitar el rastreo de procedencia. El servicio se aloja en el Brazilian Independent Data Access Center ( IDAC ) y est\u00e1 abierto a toda la comunidad LSST, sin restricciones geogr\u00e1ficas. Su dise\u00f1o es lo m\u00e1s amplio y gen\u00e9rico posible para facilitar la colaboraci\u00f3n cient\u00edfica de LSST con productos de datos de photo-z. Seg\u00fan lo exige el programa de contribuciones in-kind de LSST, el c\u00f3digo fuente est\u00e1 disponible p\u00fablicamente en GitHub . El Photo-z Server se dise\u00f1\u00f3 para ayudar a los usuarios de RSP a participar en el Photo-z (PZ) Validation Cooperative. Esta iniciativa del equipo Data Management (DM) se llevar\u00e1 a cabo durante la fase de commissioning de LSST (consulte la nota t\u00e9cnica dmtn-049 para obtener m\u00e1s informaci\u00f3n). El grupo de coordinaci\u00f3n de PZ recibir\u00e1 credenciales de usuario admin con permisos especiales para agregar productos de datos etiquetados como official data products . Estos incluir\u00e1n conjuntos estandarizados de entrenamiento y validaci\u00f3n utilizados para comparar el rendimiento de los algoritmos, as\u00ed como un medio para recopilar resultados de m\u00faltiples usuarios. M\u00e1s all\u00e1 de el PZ Validation Cooperative, el Photo-z Server seguir\u00e1 siendo un recurso para la Comunidad LSST en los pr\u00f3ximos a\u00f1os. Los usuarios de RSP podr\u00e1n seguir utiliz\u00e1ndolo para organizar, rastrear y compartir archivos ligeros con diversos resultados de pruebas. Datasets Los administradores del Photo-z Server mantienen y actualizan peri\u00f3dicamente una lista seleccionada de recursos de datos para apoyar a la comunidad LSST con productos de datos relacionados con photo-z. Las descripciones detalladas y los enlaces a cada producto de datos est\u00e1n disponibles en una p\u00e1gina aparte .","title":"Introducci\u00f3n"},{"location":"es/sci-platforms/pz_server.html#sitio-web-del-photo-z-server","text":"La interfaz de usuario principal del Photo-z Server es su sitio web en pzserver.linea.org.br . Los tres cards de la p\u00e1gina de inicio conducen a la lista de productos de datos (izquierda y centro) o a los pipelines del Photo-z Server (derecha). En la p\u00e1gina de la lista de productos de datos, los usuarios pueden explorar, buscar y filtrar los productos subidos por otros usuarios o creados con un pipeline del Photo-z Server. Los productos de datos subidos al Photo-z Server se vuelven autom\u00e1ticamente visibles, descargables y compartibles para todos los usuarios registrados.","title":"Sitio web del Photo-z Server"},{"location":"es/sci-platforms/pz_server.html#cargar-un-nuevo-producto-de-datos","text":"Para cargar un nuevo producto de datos, haga clic en el bot\u00f3n NUEVO PRODUCTO en la esquina superior derecha de la User-generated Data Products page y complete el formulario con los metadatos relevantes en cuatro pasos: Paso 1: Ingrese un nombre corto y mnemot\u00e9cnico para su nuevo producto de datos. Seleccione el tipo de producto de datos que va a cargar (p. ej., Reference Redshift Catalog, Training Set, etc.) y el release a que pertenece (si corresponde). Paso 2: Seleccione su archivo principal y todos los archivos auxiliares que desee cargar. El archivo principal contiene el producto de datos, mientras que los archivos auxiliares pueden incluir documentaci\u00f3n, descripci\u00f3n o cualquier otra informaci\u00f3n relevante sobre el producto. Si el producto de datos es tabular, la herramienta de carga podr\u00eda requerir formatos de archivo espec\u00edficos seg\u00fan su tipo. Los formatos actualmente compatibles son: CSV, FITS, HDF5 y Parquet. P\u00f3ngase en contacto con el equipo de desarrollo (mailto: pzserver-admin@linea.org.br ) si su caso cient\u00edfico requiere un formato de archivo diferente o si su archivo supera el l\u00edmite de 200 MB. Paso 3: Si el producto de datos es un Reference Redshift Catalog o un Training Set, algunas columnas son obligatorias. Los nombres de las columnas son libres, pero debe proporcionar la asociaci\u00f3n con su significado y UCD en el est\u00e1ndar IVOA , como se muestra en la figura siguiente. Paso 4: Revise su informaci\u00f3n y vuelva a los pasos anteriores si es necesario. No olvide pulsar el bot\u00f3n FINISH al final de la p\u00e1gina para enviar su producto de datos.","title":"Cargar un nuevo producto de datos"},{"location":"es/sci-platforms/pz_server.html#descargar-un-producto-de-datos","text":"Para descargar un producto de datos, haga clic en el icono en la fila del producto en la p\u00e1gina Productos de datos generados por el usuario . Al hacer clic, se generar\u00e1 un archivo comprimido .zip con todo el contenido del producto, incluyendo los archivos de descripci\u00f3n auxiliares. Tambi\u00e9n hay un bot\u00f3n en la p\u00e1gina de detalles del producto, al que se puede acceder haciendo clic en el nombre del producto en la lista.","title":"Descargar un producto de datos"},{"location":"es/sci-platforms/pz_server.html#compartir-productos-de-datos","text":"Para compartir un producto de datos, haga clic en el icono en la fila del producto en la p\u00e1gina de Productos de Datos Generados por el Usuario o en la p\u00e1gina de detalles del producto. Al hacer clic, se abrir\u00e1 una ventana emergente con el internal_name y la URL del producto. Puede copiar la informaci\u00f3n para compartirla con otros usuarios. internal_name Cada producto de datos tiene un nombre \u00fanico (\" internal_name \"), compuesto autom\u00e1ticamente por el sistema como un n\u00famero id \u00fanico seguido del nombre elegido por el usuario, con los espacios en blanco sustituidos por guiones bajos. Este nombre es la URL de la p\u00e1gina de detalles del producto de datos en el sitio web de Photo-z Server: https://pzserver.linea.org.br/product/nombre_interno y es la clave para acceder a los datos mediante la API de Python de Photo-z Server (ver detalles a continuaci\u00f3n). La forma m\u00e1s sencilla de compartir un producto de datos es proporcionar el internal_name o la URL del producto, que lleva a la p\u00e1gina de descarga del producto.","title":"Compartir productos de datos"},{"location":"es/sci-platforms/pz_server.html#tipos-de-productos","text":"","title":"Tipos de productos"},{"location":"es/sci-platforms/pz_server.html#reference-redshift-catalog","text":"En el contexto de Photo-z Server, Reference Redshift Catalogs se definen como cualquier cat\u00e1logo que contenga coordenadas ecuatoriales esf\u00e9ricas y mediciones de redshift (generalmente espectrosc\u00f3picas o redshift verdaderos para simulaciones). Columnas obligatorias: Ascensi\u00f3n recta [grados] - float Declinaci\u00f3n [grados] - float Redshift - float Columna recomendada: Error de redshift - float Un Reference Redshift Catalog puede incluir datos de un \u00fanico proyecto espectrosc\u00f3pico o una combinaci\u00f3n de datos de varias fuentes. Requisitos de pipeline Si se pretende utilizar el Reference Redshift Catalog como datos de entrada para los Combine Redshift Catalogs , aplicando la funci\u00f3n de resoluci\u00f3n de duplicados (consulte detalles de la canalizaci\u00f3n aqu\u00ed ), se recomienda incluir las siguientes columnas: Indicador de calidad (asociar con z_flag en el paso 3 de la carga) - integer , float o string (el indicador de calidad original del cat\u00e1logo fuente, si est\u00e1 disponible). Tipo de medici\u00f3n - string (p. ej., \"s\" para \"spectroscopic\", \"g\" para \"grism/prism\", \"p\" para \"photometric\", seg\u00fan lo adoptado en SITCOMTN-154 ) Nombre del proyecto (asociar con survey en el paso 3 de la carga) - string (p. ej., \"DESI\", \"COSMOS2025\", \"JADES\", etc.) Otras columnas con informaci\u00f3n adicional que desee utilizar para la resoluci\u00f3n de duplicados (p. ej., resoluci\u00f3n del instrumento).","title":"Reference Redshift Catalog"},{"location":"es/sci-platforms/pz_server.html#training-set","text":"En el contexto del Photo-z Server, los Training Sets se definen como el producto del cruce espacial entre un Reference Redshift Catalog (levantamiento individual o compilaci\u00f3n) y los datos fotom\u00e9tricos, en este caso, el LSST Object Catalog. El pipeline Training Set Maker del Photo-z Server permite a los usuarios crear Training Sets personalizados basados en los Reference Redshift Catalogs disponibles (consulte detalles del pipeline aqu\u00ed ). subconjuntos de train/test Los training sets se suelen dividir en dos o m\u00e1s subconjuntos para la validaci\u00f3n de photo-z. Si el propietario del training set ha definido previamente qu\u00e9 objetos deben pertenecer a cada subconjunto (entrenamiento y validaci\u00f3n/prueba), esta informaci\u00f3n debe estar disponible como una columna adicional en la tabla o como instrucciones claras para reproducir la separaci\u00f3n de los subconjuntos en la descripci\u00f3n del producto de datos. Para dos archivos separados, cada uno debe cargarse por separado y se convertir\u00e1 en un producto de datos independiente, ambos con el tipo de producto \"Training Set\". Su destino puede indicarse expl\u00edcitamente en el nombre y/o descripci\u00f3n del producto. training sets basados en im\u00e1genes El tipo Training Set solo admite datos a nivel de cat\u00e1logo. Los training sets basados en im\u00e1genes, por ejemplo, para algoritmos de deep-learning, no son compatibles. En este caso, utilice el tipo de producto \"Other\" y proporcione una descripci\u00f3n clara del formato de los datos en la descripci\u00f3n del producto. Para garantizar la flexibilidad en los observables, la \u00fanica columna obligatoria es el redshift ( float ). Otras columnas esperadas son: objectId del LSST Objects Catalog - integer Observables (magnitudes y/o colores, o flujos) del LSST Objects Catalog - float Errores de los observables - float Ascensi\u00f3n recta [grados] - float Declinaci\u00f3n [grados] - float Indicador de calidad - integer , float o string Indicador de subconjunto - integer , float o string","title":"Training Set"},{"location":"es/sci-platforms/pz_server.html#training-results","text":"Los resultados de entrenamiento de los algoritmos basados en machine learning tambi\u00e9n se pueden alojar en el Photo-z Server para su uso compartido y reutilizaci\u00f3n. Este tipo de producto permite archivos en formato libre. Cuando los resultados de entrenamiento se generan con el m\u00e9todo inform de RAIL , se almacenan como archivos pickle .","title":"Training Results"},{"location":"es/sci-platforms/pz_server.html#validation-results","text":"El tipo de producto Validation Results est\u00e1 dise\u00f1ado para identificar los resultados de cualquier procedimiento de validaci\u00f3n de foto-z. Puede utilizarse para almacenar los resultados del PZ Validation Cooperative o cualquier otra tarea de validaci\u00f3n. Este tipo de producto es bastante gen\u00e9rico. Puede contener estimaciones de photo-z (estimaciones individuales o PDF) de un conjunto de prueba, m\u00e9tricas de validaci\u00f3n, gr\u00e1ficos QQ-PIT, etc. Los usuarios pueden cargar un archivo principal y una lista de archivos auxiliares en cualquier formato.","title":"Validation Results"},{"location":"es/sci-platforms/pz_server.html#photo-z-estimates","text":"Las estimaciones de Photo-z son el resultado de un procedimiento de estimaci\u00f3n de Photo-z, generalmente la salida del m\u00f3dulo m\u00e9todo estimate de RAIL . Si los datos superan el l\u00edmite de carga de archivos (200 MB), la entrada del producto solo almacena los metadatos y se deben proporcionar instrucciones para acceder a ellos en el campo de descripci\u00f3n.","title":"Photo-z Estimates"},{"location":"es/sci-platforms/pz_server.html#other","text":"Cualquier otro producto de datos que no se ajuste a las categor\u00edas anteriores se puede cargar como producto de tipo Other. Este es un tipo de producto gen\u00e9rico que permite a los usuarios cargar cualquier formato de archivo y proporcionar una descripci\u00f3n del producto de datos en el campo de descripci\u00f3n.","title":"Other"},{"location":"es/sci-platforms/pz_server.html#api-python-library","text":"El Photo-z Server tambi\u00e9n ofrece una API y una biblioteca de Python para facilitar el acceso a datos y metadatos mediante la l\u00ednea de comandos. La API contiene funciones para explorar los productos de datos disponibles, recuperar el contenido de un producto de datos determinado para trabajar en la memoria o descargar los archivos de inter\u00e9s. La biblioteca de Python pzserver es de c\u00f3digo abierto y est\u00e1 disponible en GitHub . Se puede instalar mediante pip con: pip install pzserver","title":"API &amp; Python library"},{"location":"es/sci-platforms/pz_server.html#tutorial-notebook","text":"Un tutorial notebook con ejemplos de todos los m\u00e9todos de pzserver est\u00e1 disponible en el repositorio de la biblioteca pzserver en GitHub . Tambi\u00e9n est\u00e1 disponible la p\u00e1gina de documentaci\u00f3n de la API con m\u00e1s detalles para desarrolladores.","title":"Tutorial notebook"},{"location":"es/sci-platforms/pz_server.html#token-de-acceso","text":"Una vez instalada e importada en un ambiente Python, la clase PzServer abre la conexi\u00f3n remota a la base de datos de Photo-z Server. from pzserver import PzServer pz_server = PzServer ( token = \"<pegue aqu\u00ed su token de acceso>\" ) Se requiere un token de acceso para la autenticaci\u00f3n. Los usuarios pueden generarlo en el sitio web de Photo-z Server (men\u00fa de la esquina superior derecha de la p\u00e1gina de inicio).","title":"Token de acceso"},{"location":"es/sci-platforms/pz_server.html#comandos-basicos","text":"Comandos b\u00e1sicos para mostrar datos y metadatos en una celda de Jupyter Notebook (si no est\u00e1 en una Jupyter Notebook, reemplace display por get para devolver los resultados como diccionarios de Python): pz_server . display_product_types () pz_server . display_releases () pz_server . display_products_list () pz_server . display_products_list ( filters = { \"release\" : \"DP1\" , \"product_type\" : \"Training Set\" }) search_results = pz_server . get_products_list ( filters = { \"product_type\" : \"results\" }) pz_server . display_product_metadata ( id or \"internal_name\" ) Comandos b\u00e1sicos para descargar o devolver datos en memoria: pz_server . download_product ( id or \"internal_name\" , save_in = \".\" ) data = pz_server . get_product ( id or \"internal_name\" ) Consulta el tutorial notebook para obtener la lista completa de ejemplos, incluyendo instrucciones para cargar y modificar metadatos con la biblioteca pzserver .","title":"Comandos b\u00e1sicos"},{"location":"es/sci-platforms/pz_server.html#photo-z-server-pipelines","text":"Los pipelines de Photo-z Server son un conjunto de herramientas que ayudan a los usuarios a crear y gestionar productos de datos. Los pipelines disponibles actualmente son (clic en los enlaces para m\u00e1s detalles):","title":"Photo-z Server pipelines"},{"location":"es/sci-platforms/pz_server.html#codigo-abierto","text":"El Photo-z Server es un proyecto de c\u00f3digo abierto. Su c\u00f3digo fuente est\u00e1 disponible en los siguientes repositorios de GitHub: pzserver_app : el c\u00f3digo principal de la aplicaci\u00f3n, incluyendo la interfaz web y la API. pzserver : la biblioteca de Python utilizada para acceder a la API del Photo-z Server. pzserver_pipelines : el c\u00f3digo de los pipelines disponibles en el Photo-z Server. orchestration : la aplicaci\u00f3n encargada de enviar los pipelines al cl\u00faster HPC del IDAC y gestionar su ejecuci\u00f3n. pz-lsst-inkind : c\u00f3digo para tareas de gesti\u00f3n de datos en el programa in-kind del Photo-z Server, incluyendo preparaci\u00f3n de datos, control de calidad y notebooks de validaci\u00f3n de pipelines. pz-lsst-inkind-doc : documentaci\u00f3n de alto nivel sobre el programa in-kind del Photo-z Server, publicada a trav\u00e9s de GitHub Pages. El c\u00f3digo est\u00e1 licenciado bajo la Licencia MIT . \u00a1Se agradecen las contribuciones!","title":"C\u00f3digo Abierto"},{"location":"es/sci-platforms/pz_server.html#agradecimientos","text":"El Photo-z Server utiliza c\u00e1lculosl recursos del IDAC-Brasil en el Laborat\u00f3rio Interinstitucional de e-Astronomia (LIneA) con apoyo financiero del INCT do e-Universo (Proceso n\u00ba 465376/2014-2) y del proyecto FINEP: LIneA: Centro de e-Ciencia para la exploraci\u00f3n de los misterios del Universo y apoyo a proyectos de Big Data (ref n\u00ba 0883/24).","title":"Agradecimientos"},{"location":"es/sci-platforms/pz_server_old.html","text":"Inspirado en el DES Science Portal, el PZ Server ( pzserver.linea.org.br ) se est\u00e1 desarrollando como parte de la contribuci\u00f3n in-kind de LIneA para LSST para ser un servicio en l\u00ednea, complementario a la Rubin Science Platform , para almacenar y compartir datos auxiliares relacionados con photo-z (redshifts fotom\u00e9tricos). Permitir\u00e1 la gesti\u00f3n de datos, el intercambio entre usuarios y el control de procedencia. Alojado en IDAC-Brasil, el servicio estar\u00e1 disponible para todos los usuarios con acceso a datos LSST, sin restricciones geogr\u00e1ficas. Su c\u00f3digo ser\u00e1 abierto en GitHub, como lo exige el acuerdo de contribuci\u00f3n in-kind entre LIneA y Rubin Observatory. Los datos alojados en PZ Server tambi\u00e9n pueden accederse mediante la biblioteca Python pzserver (consulte el Jupyter notebook con tutorial de uso de la biblioteca y la p\u00e1gina de documentaci\u00f3n de la API para m\u00e1s detalles). El enfoque inicial es apoyar a la PZ Validation Cooperative durante la fase de pruebas de LSST, permitiendo el intercambio de conjuntos de datos estandarizados para comparaci\u00f3n de algoritmos. Posteriormente, seguir\u00e1 siendo \u00fatil para la comunidad LSST en la gesti\u00f3n de diversos archivos de prueba. Acceda a la p\u00e1gina de descripci\u00f3n del programa BRA-LIN-S4 para conocer m\u00e1s detalles sobre PZ Server y otras contribuciones en el \u00e1rea de photo-zs.","title":"Pz server old"},{"location":"es/sci-platforms/sci_server.html","text":"El DES Science Server es un servicio de visualizaci\u00f3n de im\u00e1genes y datos de cat\u00e1logos desarrollado como parte de la contribuci\u00f3n de LIneA al relevamiento Dark Energy Survey (DES). El DES Science Server consta de microservicios independientes para atender diferentes demandas. Para cada uno de estos microservicios, se est\u00e1 desarrollando una versi\u00f3n m\u00e1s nueva, flexible y moderna para proporcionar acceso a datos privados del proyecto LSST para miembros y datos p\u00fablicos de varios relevamientos para toda la comunidad, todo en un solo lugar (consulte detalles sobre los nuevos servicios de visualizaci\u00f3n Sky Viewer y Target Viewer en la p\u00e1gina Plataformas Cient\u00edficas ). Su versi\u00f3n inicial sigue disponible y proporciona acceso a im\u00e1genes y datos tabulares del DES DR2. Sky Viewer \u00b6 Ofrece visualizaci\u00f3n panor\u00e1mica de im\u00e1genes del DES, combinadas para formar una \u00fanica imagen limitada solo por los bordes del footprint, y mapas en formato HEALPix . Target Viewer \u00b6 Herramienta para visualizar y manipular im\u00e1genes de una lista predefinida de objetos (targets). Permite clasificar im\u00e1genes, aplicar filtros basados en propiedades de los objetos y crear mosaicos con m\u00faltiples im\u00e1genes. Tile Viewer \u00b6 Proporciona visualizaci\u00f3n de im\u00e1genes del DES mostradas seg\u00fan las Tiles, la unidad de \u00e1rea adoptada por el relevamiento. El Tile Viewer fue ampliamente utilizado para inspecci\u00f3n y validaci\u00f3n de datos durante los per\u00edodos previos a los lanzamientos de datos internos y p\u00fablicos.","title":"DES Science Server"},{"location":"es/sci-platforms/sci_server.html#sky-viewer","text":"Ofrece visualizaci\u00f3n panor\u00e1mica de im\u00e1genes del DES, combinadas para formar una \u00fanica imagen limitada solo por los bordes del footprint, y mapas en formato HEALPix .","title":"Sky Viewer"},{"location":"es/sci-platforms/sci_server.html#target-viewer","text":"Herramienta para visualizar y manipular im\u00e1genes de una lista predefinida de objetos (targets). Permite clasificar im\u00e1genes, aplicar filtros basados en propiedades de los objetos y crear mosaicos con m\u00faltiples im\u00e1genes.","title":"Target Viewer"},{"location":"es/sci-platforms/sci_server.html#tile-viewer","text":"Proporciona visualizaci\u00f3n de im\u00e1genes del DES mostradas seg\u00fan las Tiles, la unidad de \u00e1rea adoptada por el relevamiento. El Tile Viewer fue ampliamente utilizado para inspecci\u00f3n y validaci\u00f3n de datos durante los per\u00edodos previos a los lanzamientos de datos internos y p\u00fablicos.","title":"Tile Viewer"},{"location":"es/sci-platforms/sdss_sky_server.html","text":"El SDSS Sky Server es un portal en l\u00ednea desarrollado por el Sloan Digital Sky Survey (SDSS) que brinda acceso p\u00fablico a una vasta colecci\u00f3n de datos astron\u00f3micos recolectados por sus relevamientos. Ofrece herramientas interactivas para visualizar im\u00e1genes del cielo, explorar espectros y consultar cat\u00e1logos de objetos celestes como estrellas, galaxias y cu\u00e1sares. Con una interfaz amigable y recursos que atienden tanto a entusiastas como a investigadores profesionales, el Sky Server tambi\u00e9n proporciona formas de acceder a los datos mediante consultas SQL, facilitando an\u00e1lisis personalizados directamente sobre la base de datos del SDSS. LIneA aloja un espejo del SDSS Sky Server ( skyserver.linea.org.br ), que brinda acceso a los datos del release DR17.","title":"SDSS Sky Sever"},{"location":"es/sci-platforms/sky_viewer.html","text":"El Sky Viewer ( skyviewer.linea.org.br ) es un servicio en desarrollo basado en la herramienta de visualizaci\u00f3n Aladin que muestra im\u00e1genes HIPS y superposiciones de cat\u00e1logos de diversas fuentes, incluyendo datos p\u00fablicos disponibles en l\u00ednea y datos p\u00fablicos/privados alojados en LIneA. La lista de conjuntos de datos disponibles depende de los derechos de acceso de cada usuario. Al igual que el Target Viewer, el Sky Viewer fue desarrollado originalmente para el relevamiento DES, y su versi\u00f3n inicial sigue disponible en la p\u00e1gina del DES Science Server , ofreciendo acceso a im\u00e1genes y datos tabulares del DES DR2. La nueva versi\u00f3n, que se est\u00e1 desarrollando como parte del portafolio de servicios de IDAC-Brasil, ser\u00e1 flexible y proporcionar\u00e1 acceso a datos del LSST para miembros, adem\u00e1s de datos p\u00fablicos de otros relevamientos para el p\u00fablico en general.","title":"Sky Viewer"},{"location":"es/sci-platforms/solar-system-portal.html","text":"El estudio de cuerpos peque\u00f1os en el Sistema Solar presenta desaf\u00edos considerables, principalmente debido a sus peque\u00f1os tama\u00f1os y vastas distancias. Un m\u00e9todo para superar estos obst\u00e1culos involucra estudios indirectos mediante la t\u00e9cnica de ocultaci\u00f3n estelar . Hist\u00f3ricamente, la predicci\u00f3n precisa de estos eventos ha sido dif\u00edcil debido a la falta de mapas estelares suficientemente precisos. Sin embargo, avances recientes han transformado este escenario, permitiendo predicciones altamente precisas de ocultaciones estelares para cuerpos del Sistema Solar. La t\u00e9cnica de ocultaci\u00f3n estelar es crucial para estudiar estos cuerpos, particularmente los objetos transneptunianos (TNOs), proporcionando informaci\u00f3n precisa sobre tama\u00f1os y posiciones, caracter\u00edsticas del entorno, etc. Esto es posible gracias a una de las caracter\u00edsticas m\u00e1s notables de la t\u00e9cnica: la conversi\u00f3n de alta resoluci\u00f3n temporal en alta resoluci\u00f3n angular. Con el esperado incremento de diez veces en el volumen de datos del Legacy Survey of Space and Time (LSST), el Portal del Sistema Solar surge como una soluci\u00f3n de computaci\u00f3n de alto rendimiento para: Calcular predicciones para eventos de ocultaci\u00f3n estelar; Organizar y distribuir datos; Hacer los eventos accesibles globalmente; Reducir la carga computacional del usuario; Acceso al Portal del Sistema Solar y sus servicios \u00b6 Interfaz p\u00fablica LIneA Occultation Prediction Database ( m\u00e1s informaci\u00f3n aqu\u00ed ) Documentaci\u00f3n \u00c1rea restringida Destinada a miembros de la colaboraci\u00f3n Transneptunian Occultation Network (TON) M\u00e1s informaci\u00f3n sobre TON aqu\u00ed","title":"Solar System Portal"},{"location":"es/sci-platforms/solar-system-portal.html#acceso-al-portal-del-sistema-solar-y-sus-servicios","text":"Interfaz p\u00fablica LIneA Occultation Prediction Database ( m\u00e1s informaci\u00f3n aqu\u00ed ) Documentaci\u00f3n \u00c1rea restringida Destinada a miembros de la colaboraci\u00f3n Transneptunian Occultation Network (TON) M\u00e1s informaci\u00f3n sobre TON aqu\u00ed","title":"Acceso al Portal del Sistema Solar y sus servicios"},{"location":"es/sci-platforms/target_viewer.html","text":"El Target Viewer ( targetviewer.linea.org.br ) es un servicio en desarrollo personalizado para visualizar im\u00e1genes astron\u00f3micas basadas en una lista de objetivos predefinida por el usuario. Paso a paso: 1. Cree una nueva lista de objetivos como tabla en el espacio MyDB del usuario en User Query . Para que el Target Viewer pueda localizar las im\u00e1genes de los objetivos, la tabla debe contener las columnas objectId , ra y dec . 2. Haga clic en el bot\u00f3n \"NEW CATALOG\" en la p\u00e1gina principal del Target Viewer y complete el formulario de 3 pasos para registrar la tabla como una lista de objetivos. 3. Despu\u00e9s de enviar el formulario, la lista de objetivos aparecer\u00e1 en el men\u00fa de la p\u00e1gina principal. Al igual que el Sky Viewer, el Target Viewer fue desarrollado originalmente para el relevamiento DES, y su versi\u00f3n inicial sigue disponible en la p\u00e1gina del DES Science Server , ofreciendo acceso a im\u00e1genes y datos tabulares del DES DR2. La nueva versi\u00f3n, que se est\u00e1 desarrollando como parte del portafolio de servicios de IDAC-Brasil, ser\u00e1 flexible y proporcionar\u00e1 acceso a datos del LSST para miembros, adem\u00e1s de datos p\u00fablicos de otros relevamientos para el p\u00fablico en general.","title":"Target Viewer"},{"location":"es/sci-platforms/user_query.html","text":"El User Query ( userquery.linea.org.br ) es una interfaz amigable para consultas a bases de datos que permite la creaci\u00f3n de tablas temporales. Las tablas generadas por los usuarios son gestionadas por MyDB y pueden accederse inmediatamente en el JupyterHub de LIneA o desde cualquier lugar mediante el TAP Service. Adicionalmente, si una tabla contiene coordenadas ecuatoriales y una columna con identificadores \u00fanicos, se convierte autom\u00e1ticamente en una lista de objetivos, lo que permite la visualizaci\u00f3n de las im\u00e1genes correspondientes a los objetos en la herramienta Target Viewer (im\u00e1genes de LSST para miembros e im\u00e1genes de DES DR2 para el p\u00fablico en general). TAP Service \u00b6 El TAP Service (Table Access Protocol) de IVOA es un est\u00e1ndar que permite el acceso y consulta a bases de datos astron\u00f3micas mediante lenguaje SQL, de forma estandarizada e interoperable entre diferentes servicios. Adem\u00e1s del servicio User Query, los cat\u00e1logos p\u00fablicos alojados en LIneA tambi\u00e9n pueden accederse remotamente mediante el TAP Service usando TOPCAT o program\u00e1ticamente mediante la biblioteca Python pyvo . Consulte la documentaci\u00f3n del TAP Service en el sitio web de User Query para m\u00e1s detalles sobre c\u00f3mo utilizar el servicio.","title":"User Query"},{"location":"es/sci-platforms/user_query.html#tap-service","text":"El TAP Service (Table Access Protocol) de IVOA es un est\u00e1ndar que permite el acceso y consulta a bases de datos astron\u00f3micas mediante lenguaje SQL, de forma estandarizada e interoperable entre diferentes servicios. Adem\u00e1s del servicio User Query, los cat\u00e1logos p\u00fablicos alojados en LIneA tambi\u00e9n pueden accederse remotamente mediante el TAP Service usando TOPCAT o program\u00e1ticamente mediante la biblioteca Python pyvo . Consulte la documentaci\u00f3n del TAP Service en el sitio web de User Query para m\u00e1s detalles sobre c\u00f3mo utilizar el servicio.","title":"TAP Service"}]}